Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"67d98ae7-701a-4bb4-818f-88ca79b0713d\" name=\"Changes\" comment=\"new dir\">\r\n      <change afterPath=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_1/events.out.tfevents.1724667749.Trading.52384.0\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/ml_class/toy.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/ml_class/toy.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading.zip\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading.zip\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"simple_trading_algo\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n    <option name=\"ROOT_SYNC\" value=\"DONT_SYNC\" />\r\n  </component>\r\n  <component name=\"GitHubPullRequestSearchHistory\">{\r\n  &quot;lastFilter&quot;: {\r\n    &quot;state&quot;: &quot;OPEN&quot;,\r\n    &quot;assignee&quot;: &quot;Lqz13Th&quot;\r\n  }\r\n}</component>\r\n  <component name=\"GithubPullRequestsUISettings\">\r\n    <option name=\"selectedUrlAndAccountId\">\r\n      <UrlAndAccount>\r\n        <option name=\"accountId\" value=\"77cf43e7-d6cf-422d-a172-003bda14da51\" />\r\n        <option name=\"url\" value=\"https://github.com/Lqz13Th/machine_learning.git\" />\r\n      </UrlAndAccount>\r\n    </option>\r\n  </component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProblemsViewState\">\r\n    <option name=\"selectedTabId\" value=\"DEPENDENCY_CHECKER_PROBLEMS_TAB\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 1\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2gGVbofkneQ8UHrtXJ1iAxMrKHo\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\">\r\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\r\n  </component>\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;ASKED_ADD_EXTERNAL_FILES&quot;: &quot;true&quot;,\r\n    &quot;Python.decision_tree_google.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.dqn.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.dyna_q.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.dynamic_programming.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.filter.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.first_calss.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.main.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.markov_decision_process.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.multi_armed_bandit.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.ppo_algo.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.simple_trading_algo.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.time_difference.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.toy_dqn_model.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.toyppo.executor&quot;: &quot;Run&quot;,\r\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;git-widget-placeholder&quot;: &quot;simple__algo__ppo&quot;,\r\n    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;C:/Users/trade/AppData/Local/Programs/Python/Python312/python.exe&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;\r\n  }\r\n}</component>\r\n  <component name=\"RunManager\" selected=\"Python.ppo_algo\">\r\n    <configuration name=\"dqn\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/reinforcement_learning\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/reinforcement_learning/dqn.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"filter\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$USER_HOME$/anaconda3/envs/machine_learning/Lib/site-packages/ray/rllib/utils\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$USER_HOME$/anaconda3/envs/machine_learning/Lib/site-packages/ray/rllib/utils/filter.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"multi_armed_bandit\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/reinforcement_learning\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/reinforcement_learning/multi_armed_bandit.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"ppo_algo\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"toy_dqn_model\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"machine_learning\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/reinforcement_learning\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/reinforcement_learning/toy_dqn_model.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.ppo_algo\" />\r\n        <item itemvalue=\"Python.toy_dqn_model\" />\r\n        <item itemvalue=\"Python.multi_armed_bandit\" />\r\n        <item itemvalue=\"Python.filter\" />\r\n        <item itemvalue=\"Python.dqn\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-python-sdk-5a2391486177-d3b881c8e49f-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-233.13763.11\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"67d98ae7-701a-4bb4-818f-88ca79b0713d\" name=\"Changes\" comment=\"\" />\r\n      <created>1715323975465</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1715323975465</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00001\" summary=\"Initial commit\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1715398707446</created>\r\n      <option name=\"number\" value=\"00001\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1715398707446</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00002\" summary=\"nah\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1716626051440</created>\r\n      <option name=\"number\" value=\"00002\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1716626051440</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00003\" summary=\"nah\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1716626124107</created>\r\n      <option name=\"number\" value=\"00003\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1716626124107</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00004\" summary=\"learn decision_tree with google example\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1716627452723</created>\r\n      <option name=\"number\" value=\"00004\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1716627452723</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00005\" summary=\"learn dqn\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1717221394986</created>\r\n      <option name=\"number\" value=\"00005\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1717221394986</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00006\" summary=\"learn dqn\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1717233888704</created>\r\n      <option name=\"number\" value=\"00006\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00006\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1717233888704</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00007\" summary=\"finish toy model\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1718881840957</created>\r\n      <option name=\"number\" value=\"00007\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00007\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1718881840957</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00008\" summary=\"new dir\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1719042932008</created>\r\n      <option name=\"number\" value=\"00008\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00008\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1719042932008</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"9\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State>\r\n              <option name=\"FILTERS\">\r\n                <map>\r\n                  <entry key=\"branch\">\r\n                    <value>\r\n                      <list>\r\n                        <option value=\"master\" />\r\n                      </list>\r\n                    </value>\r\n                  </entry>\r\n                </map>\r\n              </option>\r\n            </State>\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <option name=\"ADD_EXTERNAL_FILES_SILENTLY\" value=\"true\" />\r\n    <MESSAGE value=\"Initial commit\" />\r\n    <MESSAGE value=\"nah\" />\r\n    <MESSAGE value=\"learn decision_tree with google example\" />\r\n    <MESSAGE value=\"learn dqn\" />\r\n    <MESSAGE value=\"delete subbranch\" />\r\n    <MESSAGE value=\"finish toy model\" />\r\n    <MESSAGE value=\"new dir\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"new dir\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	
+++ b/.idea/workspace.xml	
@@ -4,10 +4,25 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="67d98ae7-701a-4bb4-818f-88ca79b0713d" name="Changes" comment="new dir">
-      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_1/events.out.tfevents.1724667749.Trading.52384.0" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/ml_class/toy.py" beforeDir="false" afterPath="$PROJECT_DIR$/ml_class/toy.py" afterDir="false" />
+    <list default="true" id="67d98ae7-701a-4bb4-818f-88ca79b0713d" name="Changes" comment="get some pnl from ppo">
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_10/events.out.tfevents.1724751724.Trading.68936.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_11/events.out.tfevents.1724751913.Trading.92976.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_12/events.out.tfevents.1724752100.Trading.107652.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_13/events.out.tfevents.1724752635.Trading.58792.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_14/events.out.tfevents.1724752650.Trading.16248.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_15/events.out.tfevents.1724752667.Trading.106380.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_16/events.out.tfevents.1724752689.Trading.51052.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_17/events.out.tfevents.1724754503.Trading.104472.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_18/events.out.tfevents.1724754769.Trading.100052.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_19/events.out.tfevents.1724755308.Trading.93308.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_2/events.out.tfevents.1724746909.Trading.49200.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_3/events.out.tfevents.1724747499.Trading.101192.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_4/events.out.tfevents.1724748134.Trading.88856.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_5/events.out.tfevents.1724749210.Trading.104092.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_6/events.out.tfevents.1724749590.Trading.100972.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_7/events.out.tfevents.1724751099.Trading.75428.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_8/events.out.tfevents.1724751300.Trading.11060.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_9/events.out.tfevents.1724751535.Trading.16796.0" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py" beforeDir="false" afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading.zip" beforeDir="false" afterPath="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading.zip" afterDir="false" />
     </list>
@@ -63,33 +78,34 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent">{
-  &quot;keyToString&quot;: {
-    &quot;ASKED_ADD_EXTERNAL_FILES&quot;: &quot;true&quot;,
-    &quot;Python.decision_tree_google.executor&quot;: &quot;Run&quot;,
-    &quot;Python.dqn.executor&quot;: &quot;Run&quot;,
-    &quot;Python.dyna_q.executor&quot;: &quot;Run&quot;,
-    &quot;Python.dynamic_programming.executor&quot;: &quot;Run&quot;,
-    &quot;Python.filter.executor&quot;: &quot;Run&quot;,
-    &quot;Python.first_calss.executor&quot;: &quot;Run&quot;,
-    &quot;Python.main.executor&quot;: &quot;Run&quot;,
-    &quot;Python.markov_decision_process.executor&quot;: &quot;Run&quot;,
-    &quot;Python.multi_armed_bandit.executor&quot;: &quot;Run&quot;,
-    &quot;Python.ppo_algo.executor&quot;: &quot;Run&quot;,
-    &quot;Python.simple_trading_algo.executor&quot;: &quot;Run&quot;,
-    &quot;Python.time_difference.executor&quot;: &quot;Run&quot;,
-    &quot;Python.toy_dqn_model.executor&quot;: &quot;Run&quot;,
-    &quot;Python.toyppo.executor&quot;: &quot;Run&quot;,
-    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
-    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
-    &quot;git-widget-placeholder&quot;: &quot;simple__algo__ppo&quot;,
-    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,
-    &quot;last_opened_file_path&quot;: &quot;C:/Users/trade/AppData/Local/Programs/Python/Python312/python.exe&quot;,
-    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;
+  <component name="PropertiesComponent"><![CDATA[{
+  "keyToString": {
+    "ASKED_ADD_EXTERNAL_FILES": "true",
+    "Python.decision_tree_google.executor": "Run",
+    "Python.dqn.executor": "Run",
+    "Python.dyna_q.executor": "Run",
+    "Python.dynamic_programming.executor": "Run",
+    "Python.filter.executor": "Run",
+    "Python.first_calss.executor": "Run",
+    "Python.main.executor": "Run",
+    "Python.markov_decision_process.executor": "Run",
+    "Python.multi_armed_bandit.executor": "Run",
+    "Python.ppo_algo.executor": "Run",
+    "Python.simple_trading_algo.executor": "Run",
+    "Python.time_difference.executor": "Run",
+    "Python.toy.executor": "Run",
+    "Python.toy_dqn_model.executor": "Run",
+    "Python.toyppo.executor": "Run",
+    "RunOnceActivity.OpenProjectViewOnStart": "true",
+    "RunOnceActivity.ShowReadmeOnStart": "true",
+    "git-widget-placeholder": "simple__algo__ppo",
+    "ignore.virus.scanning.warn.message": "true",
+    "last_opened_file_path": "C:/Users/trade/AppData/Local/Programs/Python/Python312/python.exe",
+    "settings.editor.selected.configurable": "com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable"
   }
-}</component>
+}]]></component>
   <component name="RunManager" selected="Python.ppo_algo">
-    <configuration name="dqn" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="first_calss" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -98,11 +114,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/ml_class/class_hello_world" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/dqn.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/ml_class/class_hello_world/first_calss.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -111,7 +127,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="filter" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -120,11 +136,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$USER_HOME$/anaconda3/envs/machine_learning/Lib/site-packages/ray/rllib/utils" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$USER_HOME$/anaconda3/envs/machine_learning/Lib/site-packages/ray/rllib/utils/filter.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -133,7 +149,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
+    <configuration name="ppo_algo" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -142,11 +158,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -155,7 +171,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="multi_armed_bandit" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="simple_trading_algo" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -164,11 +180,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning/trading_algo" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/multi_armed_bandit.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/trading_algo/simple_trading_algo.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -177,7 +193,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="ppo_algo" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="toy" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -186,11 +202,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/ml_class" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/ppo_algo.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/ml_class/toy.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -199,7 +215,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="toy_dqn_model" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="toyppo" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="machine_learning" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -208,11 +224,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/toy_dqn_model.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/reinforcement_learning/ppo_toy_algo/toyppo.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -224,10 +240,10 @@
     <recent_temporary>
       <list>
         <item itemvalue="Python.ppo_algo" />
-        <item itemvalue="Python.toy_dqn_model" />
-        <item itemvalue="Python.multi_armed_bandit" />
-        <item itemvalue="Python.filter" />
-        <item itemvalue="Python.dqn" />
+        <item itemvalue="Python.toyppo" />
+        <item itemvalue="Python.simple_trading_algo" />
+        <item itemvalue="Python.toy" />
+        <item itemvalue="Python.first_calss" />
       </list>
     </recent_temporary>
   </component>
@@ -311,7 +327,15 @@
       <option name="project" value="LOCAL" />
       <updated>1719042932008</updated>
     </task>
-    <option name="localTasksCounter" value="9" />
+    <task id="LOCAL-00009" summary="get some pnl from ppo">
+      <option name="closed" value="true" />
+      <created>1724734883055</created>
+      <option name="number" value="00009" />
+      <option name="presentableId" value="LOCAL-00009" />
+      <option name="project" value="LOCAL" />
+      <updated>1724734883055</updated>
+    </task>
+    <option name="localTasksCounter" value="10" />
     <servers />
   </component>
   <component name="Vcs.Log.Tabs.Properties">
@@ -346,6 +370,7 @@
     <MESSAGE value="delete subbranch" />
     <MESSAGE value="finish toy model" />
     <MESSAGE value="new dir" />
-    <option name="LAST_COMMIT_MESSAGE" value="new dir" />
+    <MESSAGE value="get some pnl from ppo" />
+    <option name="LAST_COMMIT_MESSAGE" value="get some pnl from ppo" />
   </component>
 </project>
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_10/events.out.tfevents.1724751724.Trading.68936.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_10/events.out.tfevents.1724751724.Trading.68936.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_10/events.out.tfevents.1724751724.Trading.68936.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_10/events.out.tfevents.1724751724.Trading.68936.0	
@@ -0,0 +1,676 @@
+H       ��H�	�6�g��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer }׻       QKD	��}�g��A�*
+
+time/fps ��DĤ�#       ��wC	��}�g��A�*
+
+train/reward�'�?oGit       QKD	���g��A� *
+
+time/fps `�D�y�f&       sO� 	���g��A� *
+
+train/approx_kl�0<*g!�*       ����	���g��A� *
+
+train/clip_fraction  =7o��'       ��F	���g��A� *
+
+train/clip_range��L>�F&)       7�_ 	���g��A� *
+
+train/entropy_lossF����WB/       m]P	���g��A� *!
+
+train/explained_variance �M�����*       ����	���g��A� *
+
+train/learning_rateRI�9���!       {��	���g��A� *
+
+
+train/loss/��F	Kо1       ����	���g��A� *#
+!
+train/policy_gradient_loss�ⅻ%�6:#       ��wC	���g��A� *
+
+train/rewardY(��<Gx�'       ��F	���g��A� *
+
+train/value_loss�9^G9��4       QKD	�^��g��A�0*
+
+time/fps ��D�O#&       sO� 	�^��g��A�0*
+
+train/approx_kl�<ˢ0�*       ����	�^��g��A�0*
+
+train/clip_fraction  �=��~N'       ��F	�^��g��A�0*
+
+train/clip_range��L>� ~�)       7�_ 	�^��g��A�0*
+
+train/entropy_lossZ1��r��/       m]P	�^��g��A�0*!
+
+train/explained_variance  �7k(^*       ����	�^��g��A�0*
+
+train/learning_rateRI�9���!       {��	�^��g��A�0*
+
+
+train/loss���E��l1       ����	�^��g��A�0*#
+!
+train/policy_gradient_loss�|���;#       ��wC	�^��g��A�0*
+
+train/reward���L�'       ��F	�^��g��A�0*
+
+train/value_loss#o8F�Q`l       QKD	���g��A�@*
+
+time/fps ��D���&       sO� 	���g��A�@*
+
+train/approx_kl �<8~1�*       ����	���g��A�@*
+
+train/clip_fraction    H%�'       ��F	���g��A�@*
+
+train/clip_range��L>N,�)       7�_ 	���g��A�@*
+
+train/entropy_lossnm��1�$/       m]P	���g��A�@*!
+
+train/explained_variance  f�v\��*       ����	���g��A�@*
+
+train/learning_rateRI�9U��A!       {��	���g��A�@*
+
+
+train/lossH�XHq5��1       ����	���g��A�@*#
+!
+train/policy_gradient_losss)����i#       ��wC	���g��A�@*
+
+train/rewardnG���I�'       ��F	���g��A�@*
+
+train/value_losskA�H8�7�       QKD	����g��A�P*
+
+time/fps  �D��Z�&       sO� 	����g��A�P*
+
+train/approx_klh-.8��E�*       ����	����g��A�P*
+
+train/clip_fraction    ���'       ��F	����g��A�P*
+
+train/clip_range��L>�,G�)       7�_ 	����g��A�P*
+
+train/entropy_lossN��7�/       m]P	����g��A�P*!
+
+train/explained_variance  ��C���*       ����	����g��A�P*
+
+train/learning_rateRI�9&E�!       {��	����g��A�P*
+
+
+train/loss���G�'v{1       ����	����g��A�P*#
+!
+train/policy_gradient_loss�.S����&#       ��wC	����g��A�P*
+
+train/reward�������'       ��F	����g��A�P*
+
+train/value_lossw�SH�� t       QKD	*H,�g��A�`*
+
+time/fps ��D�D��&       sO� 	*H,�g��A�`*
+
+train/approx_klD8}���*       ����	*H,�g��A�`*
+
+train/clip_fraction    ��'       ��F	*H,�g��A�`*
+
+train/clip_range��L>�[_7)       7�_ 	*H,�g��A�`*
+
+train/entropy_loss�B���d<	/       m]P	*H,�g��A�`*!
+
+train/explained_variance  x6��}�*       ����	*H,�g��A�`*
+
+train/learning_rateRI�9�ă�!       {��	*H,�g��A�`*
+
+
+train/loss�8�G3��1       ����	*H,�g��A�`*#
+!
+train/policy_gradient_lossTL>�k��#       ��wC	*H,�g��A�`*
+
+train/reward�8�����.'       ��F	*H,�g��A�`*
+
+train/value_loss$�wHr4�       QKD	̵��g��A�p*
+
+time/fps ��D�
+��&       sO� 	̵��g��A�p*
+
+train/approx_kl�-D91&<�*       ����	̵��g��A�p*
+
+train/clip_fraction    �fu'       ��F	̵��g��A�p*
+
+train/clip_range��L>r�3")       7�_ 	̵��g��A�p*
+
+train/entropy_loss�����d}/       m]P	̵��g��A�p*!
+
+train/explained_variance  ��x��C*       ����	̵��g��A�p*
+
+train/learning_rateRI�9�}a�!       {��	̵��g��A�p*
+
+
+train/lossNN"G,.��1       ����	̵��g��A�p*#
+!
+train/policy_gradient_lossP��z��#       ��wC	̵��g��A�p*
+
+train/reward����H�'       ��F	̵��g��A�p*
+
+train/value_loss[бG��X        )�P	�9�g��A��*
+
+time/fps  D��T~'       ��F	�9�g��A��*
+
+train/approx_kl`��7��7�+       ��K	�9�g��A��*
+
+train/clip_fraction    C=��(       �pJ	�9�g��A��*
+
+train/clip_range��L>>��*       ����	�9�g��A��*
+
+train/entropy_loss&����z>0       ���_	�9�g��A��*!
+
+train/explained_variance  ���0#�+       ��K	�9�g��A��*
+
+train/learning_rateRI�9>�j"       x=�	�9�g��A��*
+
+
+train/loss̜Gb�0e2       $V�	�9�g��A��*#
+!
+train/policy_gradient_lossk������$       B+�M	�9�g��A��*
+
+train/reward�������(       �pJ	�9�g��A��*
+
+train/value_loss�q#H��
+�        )�P	����g��A��*
+
+time/fps �}D���'       ��F	����g��A��*
+
+train/approx_kl(68��CB+       ��K	����g��A��*
+
+train/clip_fraction    �<?�(       �pJ	����g��A��*
+
+train/clip_range��L>p�d�*       ����	����g��A��*
+
+train/entropy_lossUK������0       ���_	����g��A��*!
+
+train/explained_variance  5���+       ��K	����g��A��*
+
+train/learning_rateRI�9�-"       x=�	����g��A��*
+
+
+train/loss^9HHa�2       $V�	����g��A��*#
+!
+train/policy_gradient_loss����:$       B+�M	����g��A��*
+
+train/reward
+ �@.Ҧ�(       �pJ	����g��A��*
+
+train/value_loss-G�Hb��        )�P	SF�g��A��*
+
+time/fps �|Db�"�'       ��F	SF�g��A��*
+
+train/approx_kl }:�$+       ��K	SF�g��A��*
+
+train/clip_fraction    1w�(       �pJ	SF�g��A��*
+
+train/clip_range��L>X�Ǟ*       ����	SF�g��A��*
+
+train/entropy_loss�����,4�0       ���_	SF�g��A��*!
+
+train/explained_variance  �6Ov�5+       ��K	SF�g��A��*
+
+train/learning_rateRI�9,P'#"       x=�	SF�g��A��*
+
+
+train/loss9��F�4/_2       $V�	SF�g��A��*#
+!
+train/policy_gradient_loss��j���Р$       B+�M	SF�g��A��*
+
+train/rewardK���U�fl(       �pJ	SF�g��A��*
+
+train/value_loss
+@G��z�        )�P	mQ��g��A��*
+
+time/fps @{D��'       ��F	mQ��g��A��*
+
+train/approx_klg
+�<���S+       ��K	mQ��g��A��*
+
+train/clip_fraction3�>���(       �pJ	mQ��g��A��*
+
+train/clip_range��L>Ԡ̞*       ����	mQ��g��A��*
+
+train/entropy_loss�����f�<0       ���_	mQ��g��A��*!
+
+train/explained_variance   4�K��+       ��K	mQ��g��A��*
+
+train/learning_rateRI�9��#"       x=�	mQ��g��A��*
+
+
+train/loss�YGZ�2       $V�	mQ��g��A��*#
+!
+train/policy_gradient_loss}R�;g��$       B+�M	mQ��g��A��*
+
+train/reward����h��(       �pJ	mQ��g��A��*
+
+train/value_loss���GAh        )�P	��T�g��A��*
+
+time/fps �zDY\I'       ��F	��T�g��A��*
+
+train/approx_kl9\mA�+       ��K	��T�g��A��*
+
+train/clip_fraction    �
+��(       �pJ	��T�g��A��*
+
+train/clip_range��L>e�O*       ����	��T�g��A��*
+
+train/entropy_loss�-���ܟc0       ���_	��T�g��A��*!
+
+train/explained_variance   ���+       ��K	��T�g��A��*
+
+train/learning_rateRI�9ߴP"       x=�	��T�g��A��*
+
+
+train/lossy~I?��2       $V�	��T�g��A��*#
+!
+train/policy_gradient_loss9�F:����$       B+�M	��T�g��A��*
+
+train/rewardKJ����J(       �pJ	��T�g��A��*
+
+train/value_loss���I�q        )�P	����g��A��*
+
+time/fps @zDr��'       ��F	����g��A��*
+
+train/approx_kl��6���+       ��K	����g��A��*
+
+train/clip_fraction    �v4(       �pJ	����g��A��*
+
+train/clip_range��L>A��*       ����	����g��A��*
+
+train/entropy_loss�(��U=}50       ���_	����g��A��*!
+
+train/explained_variance  86�!�b+       ��K	����g��A��*
+
+train/learning_rateRI�9�M=�"       x=�	����g��A��*
+
+
+train/loss��`I՚@2       $V�	����g��A��*#
+!
+train/policy_gradient_loss;��[��$       B+�M	����g��A��*
+
+train/reward����_���(       �pJ	����g��A��*
+
+train/value_losst��I?st�        )�P	�ec�g��A��*
+
+time/fps �yD~�'       ��F	�ec�g��A��*
+
+train/approx_klx�8����+       ��K	�ec�g��A��*
+
+train/clip_fraction    z1'(       �pJ	�ec�g��A��*
+
+train/clip_range��L>��e*       ����	�ec�g��A��*
+
+train/entropy_lossT���"�0       ���_	�ec�g��A��*!
+
+train/explained_variance  5�>?+       ��K	�ec�g��A��*
+
+train/learning_rateRI�9�P�"       x=�	�ec�g��A��*
+
+
+train/loss��4I\��2       $V�	�ec�g��A��*#
+!
+train/policy_gradient_loss[r��ۘ�$       B+�M	�ec�g��A��*
+
+train/reward�S��B�xw(       �pJ	�ec�g��A��*
+
+train/value_loss}ζI�6�#        )�P	�u��g��A��*
+
+time/fps @yD&l:�'       ��F	�u��g��A��*
+
+train/approx_kl �x6	�+       ��K	�u��g��A��*
+
+train/clip_fraction    톎f(       �pJ	�u��g��A��*
+
+train/clip_range��L>U�i*       ����	�u��g��A��*
+
+train/entropy_loss�����l0       ���_	�u��g��A��*!
+
+train/explained_variance  �4�<$�+       ��K	�u��g��A��*
+
+train/learning_rateRI�9�_��"       x=�	�u��g��A��*
+
+
+train/loss�0NI{s��2       $V�	�u��g��A��*#
+!
+train/policy_gradient_loss�↸��
+�$       B+�M	�u��g��A��*
+
+train/rewardw|��p���(       �pJ	�u��g��A��*
+
+train/value_loss���Ip�DI        )�P	��n�g��A��*
+
+time/fps  yD.ȿ'       ��F	��n�g��A��*
+
+train/approx_kl���6�:1+       ��K	��n�g��A��*
+
+train/clip_fraction    Q,��(       �pJ	��n�g��A��*
+
+train/clip_range��L>|��{*       ����	��n�g��A��*
+
+train/entropy_lossoC��x��:0       ���_	��n�g��A��*!
+
+train/explained_variance   ���*_+       ��K	��n�g��A��*
+
+train/learning_rateRI�9L��R"       x=�	��n�g��A��*
+
+
+train/loss=�H+�9�2       $V�	��n�g��A��*#
+!
+train/policy_gradient_loss9�z���$       B+�M	��n�g��A��*
+
+train/reward�C�����(       �pJ	��n�g��A��*
+
+train/value_loss��(I��:�        )�P	
+���g��A��*
+
+time/fps �xDg'�J'       ��F	
+���g��A��*
+
+train/approx_kl0$�7��m+       ��K	
+���g��A��*
+
+train/clip_fraction    s��(       �pJ	
+���g��A��*
+
+train/clip_range��L>	�Y*       ����	
+���g��A��*
+
+train/entropy_loss�������0       ���_	
+���g��A��*!
+
+train/explained_variance  @�*��i+       ��K	
+���g��A��*
+
+train/learning_rateRI�9�K�,"       x=�	
+���g��A��*
+
+
+train/lossVOI/��Z2       $V�	
+���g��A��*#
+!
+train/policy_gradient_loss��w��ЬE$       B+�M	
+���g��A��*
+
+train/reward�*�Li�(       �pJ	
+���g��A��*
+
+train/value_lossƙ�I}	��        )�P	�Xx�g��A��*
+
+time/fps �xD��_�'       ��F	�Xx�g��A��*
+
+train/approx_kl࢜8���+       ��K	�Xx�g��A��*
+
+train/clip_fraction    �V��(       �pJ	�Xx�g��A��*
+
+train/clip_range��L>�ku�*       ����	�Xx�g��A��*
+
+train/entropy_lossmӇ���)�0       ���_	�Xx�g��A��*!
+
+train/explained_variance  7��K+       ��K	�Xx�g��A��*
+
+train/learning_rateRI�9�m�"       x=�	�Xx�g��A��*
+
+
+train/loss�I}@�2       $V�	�Xx�g��A��*#
+!
+train/policy_gradient_loss����k��$       B+�M	�Xx�g��A��*
+
+train/reward���+*"(       �pJ	�Xx�g��A��*
+
+train/value_losssG�I�[��        )�P	sb��g��A��*
+
+time/fps �xDl��P'       ��F	sb��g��A��*
+
+train/approx_kl �O7��/J+       ��K	�j��g��A��*
+
+train/clip_fraction    WF��(       �pJ	�j��g��A��*
+
+train/clip_range��L>kbT�*       ����	�j��g��A��*
+
+train/entropy_loss����^&1�0       ���_	�j��g��A��*!
+
+train/explained_variance  ��مpR+       ��K	�j��g��A��*
+
+train/learning_rateRI�9J��S"       x=�	�j��g��A��*
+
+
+train/lossvI��2       $V�	�j��g��A��*#
+!
+train/policy_gradient_loss0޸�S�n$       B+�M	�j��g��A��*
+
+train/rewarda��B�VGj(       �pJ	�j��g��A��*
+
+train/value_lossQ��I��R        )�P	����g��A��*
+
+time/fps @xD{��`'       ��F	����g��A��*
+
+train/approx_kl���:o1�+       ��K	����g��A��*
+
+train/clip_fraction���:
+��2(       �pJ	����g��A��*
+
+train/clip_range��L>�>�*       ����	����g��A��*
+
+train/entropy_loss􂅿�=T�0       ���_	����g��A��*!
+
+train/explained_variance    �1Xy+       ��K	����g��A��*
+
+train/learning_rateRI�9G7��"       x=�	����g��A��*
+
+
+train/loss6�H�bLK2       $V�	����g��A��*#
+!
+train/policy_gradient_loss��G9,�P*$       B+�M	����g��A��*
+
+train/rewardq�BRa(       �pJ	����g��A��*
+
+train/value_loss���H���        )�P	ʇ�g��A��*
+
+time/fps  xDjz��'       ��F	ʇ�g��A��*
+
+train/approx_kl:g5;/8�+       ��K	ʇ�g��A��*
+
+train/clip_fraction33=E��_(       �pJ	ʇ�g��A��*
+
+train/clip_range��L>���R*       ����	ʇ�g��A��*
+
+train/entropy_loss�|y���4�0       ���_	ʇ�g��A��*!
+
+train/explained_variance    z��!+       ��K	ʇ�g��A��*
+
+train/learning_rateRI�9F�
+�"       x=�	ʇ�g��A��*
+
+
+train/loss�9I�/�2       $V�	ʇ�g��A��*#
+!
+train/policy_gradient_loss��:t;%c$       B+�M	ʇ�g��A��*
+
+train/reward�}�B��J(       �pJ	ʇ�g��A��*
+
+train/value_loss�ӒI�/I        )�P	s)��g��A��*
+
+time/fps �wD�_��'       ��F	s)��g��A��*
+
+train/approx_kl���7��\^+       ��K	s)��g��A��*
+
+train/clip_fraction    �u�(       �pJ	s)��g��A��*
+
+train/clip_range��L>�K��*       ����	s)��g��A��*
+
+train/entropy_loss��y�qPw�0       ���_	s)��g��A��*!
+
+train/explained_variance    t>Ĩ+       ��K	s)��g��A��*
+
+train/learning_rateRI�9#a,"       x=�	s)��g��A��*
+
+
+train/loss�,QI<AX32       $V�	s)��g��A��*#
+!
+train/policy_gradient_lossu��Q��$       B+�M	s)��g��A��*
+
+train/reward��B�HѢ(       �pJ	s)��g��A��*
+
+train/value_loss���IR��$        )�P	���g��A��*
+
+time/fps �wDʎ��'       ��F	���g��A��*
+
+train/approx_klLN�9�b�+       ��K	���g��A��*
+
+train/clip_fraction    c'��(       �pJ	���g��A��*
+
+train/clip_range��L>S��*       ����	���g��A��*
+
+train/entropy_lossXy�I��0       ���_	���g��A��*!
+
+train/explained_variance   ��!8[+       ��K	���g��A��*
+
+train/learning_rateRI�9^��7"       x=�	���g��A��*
+
+
+train/loss0�IpČ02       $V�	���g��A��*#
+!
+train/policy_gradient_loss�?�	��$       B+�M	���g��A��*
+
+train/reward��ByK4�(       �pJ	���g��A��*
+
+train/value_losso�&Jا��        )�P	m��g��A��*
+
+time/fps @wD����'       ��F	m��g��A��*
+
+train/approx_klp�v8}���+       ��K	m��g��A��*
+
+train/clip_fraction    VƬ�(       �pJ	m��g��A��*
+
+train/clip_range��L>���Q*       ����	m��g��A��*
+
+train/entropy_loss�k{�s;3<0       ���_	m��g��A��*!
+
+train/explained_variance   ���c�+       ��K	m��g��A��*
+
+train/learning_rateRI�9K�^"       x=�	m��g��A��*
+
+
+train/loss���I�φ�2       $V�	m��g��A��*#
+!
+train/policy_gradient_loss���Ɵ�<$       B+�M	m��g��A��*
+
+train/reward�ՉBr8�(       �pJ	m��g��A��*
+
+train/value_loss��J�Є8        )�P	�E&�g��A��*
+
+time/fps @wDV��d'       ��F	�E&�g��A��*
+
+train/approx_kl��8ӷ�v+       ��K	�E&�g��A��*
+
+train/clip_fraction    ���<(       �pJ	�E&�g��A��*
+
+train/clip_range��L>�j��*       ����	�E&�g��A��*
+
+train/entropy_loss�+�����0       ���_	�E&�g��A��*!
+
+train/explained_variance  ��$9��+       ��K	�E&�g��A��*
+
+train/learning_rateRI�9J��"       x=�	�E&�g��A��*
+
+
+train/lossÉ6I�[2       $V�	�E&�g��A��*#
+!
+train/policy_gradient_loss�ɕ��$ek$       B+�M	�E&�g��A��*
+
+train/reward��YB�r,�(       �pJ	�E&�g��A��*
+
+train/value_loss:��I�:D        )�P	�E��g��A��*
+
+time/fps @vD^P��'       ��F	�E��g��A��*
+
+train/approx_kl��8!tQ�+       ��K	�E��g��A��*
+
+train/clip_fraction    *]�(       �pJ	�E��g��A��*
+
+train/clip_range��L>u��*       ����	�E��g��A��*
+
+train/entropy_lossM����70       ���_	�E��g��A��*!
+
+train/explained_variance  p���}+       ��K	�E��g��A��*
+
+train/learning_rateRI�97�yN"       x=�	�E��g��A��*
+
+
+train/loss��I�#��2       $V�	�E��g��A��*#
+!
+train/policy_gradient_losst��N�~�$       B+�M	�E��g��A��*
+
+train/reward[�:B81��(       �pJ	�E��g��A��*
+
+train/value_loss��I����        )�P	�9?�g��A��*
+
+time/fps  vD�
+'       ��F	�9?�g��A��*
+
+train/approx_klVG:U�U�+       ��K	�9?�g��A��*
+
+train/clip_fraction    �|�(       �pJ	�9?�g��A��*
+
+train/clip_range��L>��I*       ����	�9?�g��A��*
+
+train/entropy_loss�@��e<��0       ���_	�9?�g��A��*!
+
+train/explained_variance   7�+d�+       ��K	�9?�g��A��*
+
+train/learning_rateRI�9�c9D"       x=�	�9?�g��A��*
+
+
+train/loss�5�HwT��2       $V�	�9?�g��A��*#
+!
+train/policy_gradient_lossqr`�����$       B+�M	�9?�g��A��*
+
+train/reward6I�@�9D(       �pJ	�9?�g��A��*
+
+train/value_loss�H	Io��H        )�P	m��g��A��*
+
+time/fps @uD͋k'       ��F	m��g��A��*
+
+train/approx_kll��:�3,+       ��K	m��g��A��*
+
+train/clip_fraction    ���(       �pJ	m��g��A��*
+
+train/clip_range��L>�H5*       ����	m��g��A��*
+
+train/entropy_loss|x�
+�!60       ���_	m��g��A��*!
+
+train/explained_variance  е4
+�5+       ��K	m��g��A��*
+
+train/learning_rateRI�9\�,"       x=�	m��g��A��*
+
+
+train/loss��G���2       $V�	m��g��A��*#
+!
+train/policy_gradient_loss٪ͺX��k$       B+�M	m��g��A��*
+
+train/reward�!B�^5(       �pJ	m��g��A��*
+
+train/value_lossζ�Ho��}        )�P	�XW�g��A��*
+
+time/fps @uD��)['       ��F	�XW�g��A��*
+
+train/approx_kl
+�;�NP+       ��K	�XW�g��A��*
+
+train/clip_fraction    Y��K(       �pJ	�XW�g��A��*
+
+train/clip_range��L>�G��*       ����	�XW�g��A��*
+
+train/entropy_loss�xs�Ύ�0       ���_	�XW�g��A��*!
+
+train/explained_variance  6��S�+       ��K	�XW�g��A��*
+
+train/learning_rateRI�9d���"       x=�	�XW�g��A��*
+
+
+train/losss2H�y"2       $V�	�XW�g��A��*#
+!
+train/policy_gradient_loss�������O$       B+�M	�XW�g��A��*
+
+train/reward'�A�� n(       �pJ	�XW�g��A��*
+
+train/value_losst
+�H�"�
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_9/events.out.tfevents.1724751535.Trading.16796.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_9/events.out.tfevents.1724751535.Trading.16796.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_9/events.out.tfevents.1724751535.Trading.16796.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_9/events.out.tfevents.1724751535.Trading.16796.0	
@@ -0,0 +1,671 @@
+H       ��H�	�	ѫg��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer�F�       QKD	�t%�g��A�*
+
+time/fps ��D&��#       ��wC	�t%�g��A�*
+
+train/reward�'�?ҫG�       QKD	V殬g��A� *
+
+time/fps ��D��&       sO� 	V殬g��A� *
+
+train/approx_kl�s/<�*�l*       ����	V殬g��A� *
+
+train/clip_fractionff�<��G'       ��F	V殬g��A� *
+
+train/clip_range��L>&�3�)       7�_ 	V殬g��A� *
+
+train/entropy_loss풋�J"�|/       m]P	V殬g��A� *!
+
+train/explained_variance �M�6�4\*       ����	V殬g��A� *
+
+train/learning_rateRI�9��v�!       {��	V殬g��A� *
+
+
+train/loss*��Fc�D1       ����	V殬g��A� *#
+!
+train/policy_gradient_loss}d���Ǚ�#       ��wC	V殬g��A� *
+
+train/reward*����a�x'       ��F	V殬g��A� *
+
+train/value_loss�9^G�9��       QKD	�{9�g��A�0*
+
+time/fps @�Dy��&       sO� 	�{9�g��A�0*
+
+train/approx_klJ��;�k;�*       ����	�9�g��A�0*
+
+train/clip_fraction33}=�.x'       ��F	�9�g��A�0*
+
+train/clip_range��L>��u�)       7�_ 	�9�g��A�0*
+
+train/entropy_loss�0��N��/       m]P	�9�g��A�0*!
+
+train/explained_variance  �7֕t}*       ����	�9�g��A�0*
+
+train/learning_rateRI�9�1�!       {��	�9�g��A�0*
+
+
+train/loss���E���1       ����	�9�g��A�0*#
+!
+train/policy_gradient_loss�/���sѼ#       ��wC	�9�g��A�0*
+
+train/reward�im8�'       ��F	�9�g��A�0*
+
+train/value_loss^4F�;��       QKD	�Qĭg��A�@*
+
+time/fps  �D�Ϙ�&       sO� 	�Qĭg��A�@*
+
+train/approx_kl��K8�R?/*       ����	�Qĭg��A�@*
+
+train/clip_fraction    ���H'       ��F	�Qĭg��A�@*
+
+train/clip_range��L>���x)       7�_ 	�Qĭg��A�@*
+
+train/entropy_loss"	��AF�M/       m]P	�Qĭg��A�@*!
+
+train/explained_variance  x�/��"*       ����	�Qĭg��A�@*
+
+train/learning_rateRI�9����!       {��	�Qĭg��A�@*
+
+
+train/loss*�IH�}1       ����	�Qĭg��A�@*#
+!
+train/policy_gradient_loss� �����#       ��wC	�Qĭg��A�@*
+
+train/reward�&��t`P�'       ��F	�Qĭg��A�@*
+
+train/value_loss���H�t5I       QKD	aJ�g��A�P*
+
+time/fps `�D ���&       sO� 	aJ�g��A�P*
+
+train/approx_kl�Rr8>��*       ����	aJ�g��A�P*
+
+train/clip_fraction    ��(�'       ��F	aJ�g��A�P*
+
+train/clip_range��L>V�Dm)       7�_ 	�&J�g��A�P*
+
+train/entropy_loss�����/       m]P	�&J�g��A�P*!
+
+train/explained_variance  ����N*       ����	�&J�g��A�P*
+
+train/learning_rateRI�9�
+�!       {��	�&J�g��A�P*
+
+
+train/loss�e�G�{6�1       ����	�&J�g��A�P*#
+!
+train/policy_gradient_loss5�r���yJ#       ��wC	�&J�g��A�P*
+
+train/rewardj���nS'       ��F	�&J�g��A�P*
+
+train/value_loss�6IHVIP�       QKD	��Ѯg��A�`*
+
+time/fps �D�'�&       sO� 	�Ѯg��A�`*
+
+train/approx_kl8����*       ����	�Ѯg��A�`*
+
+train/clip_fraction    �v�'       ��F	�Ѯg��A�`*
+
+train/clip_range��L>b$;)       7�_ 	�Ѯg��A�`*
+
+train/entropy_loss 爿�A0/       m]P	�Ѯg��A�`*!
+
+train/explained_variance  `6�q�*       ����	�Ѯg��A�`*
+
+train/learning_rateRI�9�Ib.!       {��	�Ѯg��A�`*
+
+
+train/lossS��G_G1       ����	�Ѯg��A�`*#
+!
+train/policy_gradient_loss�j���l$�#       ��wC	�Ѯg��A�`*
+
+train/rewardB}����av'       ��F	�Ѯg��A�`*
+
+train/value_loss��_H����       QKD	f�V�g��A�p*
+
+time/fps @~D+���&       sO� 	f�V�g��A�p*
+
+train/approx_kl�K:)�*       ����	f�V�g��A�p*
+
+train/clip_fraction    �lC^'       ��F	f�V�g��A�p*
+
+train/clip_range��L>��M)       7�_ 	f�V�g��A�p*
+
+train/entropy_lossx���;N�/       m]P	f�V�g��A�p*!
+
+train/explained_variance  `�q���*       ����	f�V�g��A�p*
+
+train/learning_rateRI�9�So�!       {��	��V�g��A�p*
+
+
+train/loss��F�~t1       ����	��V�g��A�p*#
+!
+train/policy_gradient_loss�y�X��#       ��wC	��V�g��A�p*
+
+train/reward���<bp�'       ��F	��V�g��A�p*
+
+train/value_loss��zG�L��        )�P	)�ݯg��A��*
+
+time/fps �|D_=�e'       ��F	)�ݯg��A��*
+
+train/approx_kl`��9o��+       ��K	)�ݯg��A��*
+
+train/clip_fraction    �X6(       �pJ	)�ݯg��A��*
+
+train/clip_range��L>��*       ����	)�ݯg��A��*
+
+train/entropy_loss2j���,8�0       ���_	)�ݯg��A��*!
+
+train/explained_variance  ��&�u+       ��K	)�ݯg��A��*
+
+train/learning_rateRI�9�o8�"       x=�	)�ݯg��A��*
+
+
+train/loss�?�G��D�2       $V�	)�ݯg��A��*#
+!
+train/policy_gradient_loss;zǹ�ۤ�$       B+�M	)�ݯg��A��*
+
+train/reward�� ¬�C�(       �pJ	)�ݯg��A��*
+
+train/value_losstW1H+�P        )�P	dEd�g��A��*
+
+time/fps �{D�נk'       ��F	dEd�g��A��*
+
+train/approx_kl k�7`"��+       ��K	dEd�g��A��*
+
+train/clip_fraction    ���(       �pJ	dEd�g��A��*
+
+train/clip_range��L>E|`[*       ����	dEd�g��A��*
+
+train/entropy_loss鐉��x��0       ���_	dEd�g��A��*!
+
+train/explained_variance  05Vm��+       ��K	dEd�g��A��*
+
+train/learning_rateRI�9�9�""       x=�	dEd�g��A��*
+
+
+train/lossG�H �9�2       $V�	dEd�g��A��*#
+!
+train/policy_gradient_lossI���@@$       B+�M	dEd�g��A��*
+
+train/reward�0)��$�(       �pJ	dEd�g��A��*
+
+train/value_lossI�-l~        )�P	$l�g��A��*
+
+time/fps  {DBr�-'       ��F	$l�g��A��*
+
+train/approx_kl�m:Ӱ�+       ��K	$l�g��A��*
+
+train/clip_fraction    bi�@(       �pJ	$l�g��A��*
+
+train/clip_range��L>j�Z*       ����	$l�g��A��*
+
+train/entropy_loss�ˉ�/���0       ���_	$l�g��A��*!
+
+train/explained_variance  �6�yO+       ��K	$l�g��A��*
+
+train/learning_rateRI�9_���"       x=�	$l�g��A��*
+
+
+train/loss�W�G�FX2       $V�	$l�g��A��*#
+!
+train/policy_gradient_lossR\2�ħv�$       B+�M	$l�g��A��*
+
+train/rewardz#����O(       �pJ	$l�g��A��*
+
+train/value_lossn�H�lc�        )�P	'-t�g��A��*
+
+time/fps �yD�d��'       ��F	'-t�g��A��*
+
+train/approx_kl�� <�Zm�+       ��K	'-t�g��A��*
+
+train/clip_fractionfff=��ȩ(       �pJ	'-t�g��A��*
+
+train/clip_range��L>�;	I*       ����	'-t�g��A��*
+
+train/entropy_loss���#��0       ���_	'-t�g��A��*!
+
+train/explained_variance    �@��+       ��K	'-t�g��A��*
+
+train/learning_rateRI�9:�j"       x=�	'-t�g��A��*
+
+
+train/loss���F����2       $V�	'-t�g��A��*#
+!
+train/policy_gradient_lossT,Y8nK�$       B+�M	'-t�g��A��*
+
+train/reward˨����`(       �pJ	'-t�g��A��*
+
+train/value_loss�J0G��m�        )�P	����g��A��*
+
+time/fps  yDw`TC'       ��F	G���g��A��*
+
+train/approx_kl"-:} �a+       ��K	G���g��A��*
+
+train/clip_fraction    ���(       �pJ	G���g��A��*
+
+train/clip_range��L>$4�*       ����	G���g��A��*
+
+train/entropy_loss΀��4z3~0       ���_	G���g��A��*!
+
+train/explained_variance   ��y�4+       ��K	G���g��A��*
+
+train/learning_rateRI�9�D��"       x=�	G���g��A��*
+
+
+train/loss��1I�TbH2       $V�	G���g��A��*#
+!
+train/policy_gradient_loss�͹!x�g$       B+�M	G���g��A��*
+
+train/reward9V�¢�W(       �pJ	G���g��A��*
+
+train/value_lossm�IJ�R         )�P	����g��A��*
+
+time/fps �xD=��G'       ��F	� ��g��A��*
+
+train/approx_kl �D7[Qr�+       ��K	� ��g��A��*
+
+train/clip_fraction    xA�(       �pJ	� ��g��A��*
+
+train/clip_range��L>S
+fU*       ����	� ��g��A��*
+
+train/entropy_lossh���mb[�0       ���_	� ��g��A��*!
+
+train/explained_variance  �*ZJ+       ��K	� ��g��A��*
+
+train/learning_rateRI�9��'"       x=�	� ��g��A��*
+
+
+train/loss��`I��>2       $V�	� ��g��A��*#
+!
+train/policy_gradient_loss�C%�
+7��$       B+�M	� ��g��A��*
+
+train/reward����Gr��(       �pJ	� ��g��A��*
+
+train/value_lossx��Iؐ�:        )�P	M��g��A��*
+
+time/fps @xDf{�'       ��F	M��g��A��*
+
+train/approx_kl��/7LbF+       ��K	M��g��A��*
+
+train/clip_fraction    �(       �pJ	M��g��A��*
+
+train/clip_range��L>��
+�*       ����	M��g��A��*
+
+train/entropy_loss���|͖0       ���_	M��g��A��*!
+
+train/explained_variance  �5��++       ��K	M��g��A��*
+
+train/learning_rateRI�9�I��"       x=�	M��g��A��*
+
+
+train/loss΂I{�]2       $V�	M��g��A��*#
+!
+train/policy_gradient_loss���A��F$       B+�M	M��g��A��*
+
+train/rewardҳ��ւ(       �pJ	M��g��A��*
+
+train/value_loss.J鐶        )�P	֌��g��A��*
+
+time/fps �wD+��'       ��F	֌��g��A��*
+
+train/approx_kl J{54�+       ��K	֌��g��A��*
+
+train/clip_fraction    �c4(       �pJ	֌��g��A��*
+
+train/clip_range��L>�I�%*       ����	֌��g��A��*
+
+train/entropy_loss ��M �0       ���_	֌��g��A��*!
+
+train/explained_variance  @4ĥ�+       ��K	֌��g��A��*
+
+train/learning_rateRI�98O�"       x=�	֌��g��A��*
+
+
+train/loss8��I��>2       $V�	֌��g��A��*#
+!
+train/policy_gradient_loss
+��*v+�$       B+�M	֌��g��A��*
+
+train/rewardd_�¾���(       �pJ	֌��g��A��*
+
+train/value_loss1JKY�        )�P	� �g��A��*
+
+time/fps �wD�r��'       ��F	� �g��A��*
+
+train/approx_kl�Ԋ74�+�+       ��K	� �g��A��*
+
+train/clip_fraction    l��F(       �pJ	� �g��A��*
+
+train/clip_range��L>ҧFJ*       ����	� �g��A��*
+
+train/entropy_loss�5��BV�:0       ���_	� �g��A��*!
+
+train/explained_variance   ����]+       ��K	� �g��A��*
+
+train/learning_rateRI�9q7��"       x=�	� �g��A��*
+
+
+train/loss�ŷH���A2       $V�	� �g��A��*#
+!
+train/policy_gradient_lossT������q$       B+�M	� �g��A��*
+
+train/reward�����(       �pJ	� �g��A��*
+
+train/value_loss&-Il��        )�P	����g��A��*
+
+time/fps  wD����'       ��F	����g��A��*
+
+train/approx_kl 9T6��N+       ��K	����g��A��*
+
+train/clip_fraction    �[o�(       �pJ	����g��A��*
+
+train/clip_range��L>��)�*       ����	����g��A��*
+
+train/entropy_lossU������0       ���_	����g��A��*!
+
+train/explained_variance  `����+       ��K	����g��A��*
+
+train/learning_rateRI�9�CN"       x=�	����g��A��*
+
+
+train/loss�BRIҽ�2       $V�	����g��A��*#
+!
+train/policy_gradient_loss������-c$       B+�M	����g��A��*
+
+train/reward�D��:$��(       �pJ	����g��A��*
+
+train/value_loss���I~�7        )�P	*�'�g��A��*
+
+time/fps �vD8W�'       ��F	*�'�g��A��*
+
+train/approx_klp��7��˖+       ��K	*�'�g��A��*
+
+train/clip_fraction    %�)P(       �pJ	*�'�g��A��*
+
+train/clip_range��L>;(^�*       ����	*�'�g��A��*
+
+train/entropy_loss�n�����v0       ���_	*�'�g��A��*!
+
+train/explained_variance  �6��22+       ��K	*�'�g��A��*
+
+train/learning_rateRI�9��"       x=�	*�'�g��A��*
+
+
+train/loss�%I<�2       $V�	*�'�g��A��*#
+!
+train/policy_gradient_loss.'<��H��$       B+�M	*�'�g��A��*
+
+train/reward��Q°:�@(       �pJ	*�'�g��A��*
+
+train/value_loss�R�I���        )�P	�a��g��A��*
+
+time/fps @vDɬ�%'       ��F	�a��g��A��*
+
+train/approx_kl�<�64^h_+       ��K	�a��g��A��*
+
+train/clip_fraction    A�|o(       �pJ	�a��g��A��*
+
+train/clip_range��L>��L�*       ����	�a��g��A��*
+
+train/entropy_loss�щ���0       ���_	�a��g��A��*!
+
+train/explained_variance   ��XH+       ��K	�a��g��A��*
+
+train/learning_rateRI�9<'X�"       x=�	�a��g��A��*
+
+
+train/losspgaI��|m2       $V�	�a��g��A��*#
+!
+train/policy_gradient_loss�#��QGc$       B+�M	�a��g��A��*
+
+train/rewardh}*Bj�LL(       �pJ	�a��g��A��*
+
+train/value_loss�;�I�Q�        )�P	�A�g��A��*
+
+time/fps  uD��'       ��F	�A�g��A��*
+
+train/approx_kl��8.��p+       ��K	�A�g��A��*
+
+train/clip_fraction    ߆��(       �pJ	�A�g��A��*
+
+train/clip_range��L>��**       ����	�A�g��A��*
+
+train/entropy_loss�]����\Z0       ���_	�A�g��A��*!
+
+train/explained_variance    +9x+       ��K	�A�g��A��*
+
+train/learning_rateRI�9�_�z"       x=�	�A�g��A��*
+
+
+train/loss��Hn�O�2       $V�	�A�g��A��*#
+!
+train/policy_gradient_loss�u�V�~$       B+�M	�A�g��A��*
+
+train/rewardډGB���N(       �pJ	�A�g��A��*
+
+train/value_lossİ6I��*�        )�P	*9ɶg��A��*
+
+time/fps  uDn2�['       ��F	*9ɶg��A��*
+
+train/approx_kl`�(8a�+       ��K	*9ɶg��A��*
+
+train/clip_fraction    ��Z�(       �pJ	*9ɶg��A��*
+
+train/clip_range��L>D�ZU*       ����	*9ɶg��A��*
+
+train/entropy_loss�?���r�0       ���_	*9ɶg��A��*!
+
+train/explained_variance   ��Oe�+       ��K	*9ɶg��A��*
+
+train/learning_rateRI�9�\~?"       x=�	*9ɶg��A��*
+
+
+train/loss]��H�r�;2       $V�	*9ɶg��A��*#
+!
+train/policy_gradient_lossDs'�����$       B+�M	*9ɶg��A��*
+
+train/reward樊B��}�(       �pJ	*9ɶg��A��*
+
+train/value_loss>/IlQ��        )�P	�PR�g��A��*
+
+time/fps �tDjN
+�'       ��F	�PR�g��A��*
+
+train/approx_kl �7"��>+       ��K	�PR�g��A��*
+
+train/clip_fraction    HtH�(       �pJ	�PR�g��A��*
+
+train/clip_range��L>�&|�*       ����	�PR�g��A��*
+
+train/entropy_loss���ա�50       ���_	�PR�g��A��*!
+
+train/explained_variance    z�í+       ��K	�PR�g��A��*
+
+train/learning_rateRI�9UKl<"       x=�	�PR�g��A��*
+
+
+train/loss��IǨ2       $V�	�PR�g��A��*#
+!
+train/policy_gradient_loss;�߸�[��$       B+�M	�PR�g��A��*
+
+train/reward��C�16�(       �pJ	�PR�g��A��*
+
+train/value_lossq1�IƯZU        )�P	
+?۷g��A��*
+
+time/fps �tD�FOj'       ��F	
+?۷g��A��*
+
+train/approx_kl �5~�'+       ��K	
+?۷g��A��*
+
+train/clip_fraction    �p�1(       �pJ	
+?۷g��A��*
+
+train/clip_range��L>,K*       ����	
+?۷g��A��*
+
+train/entropy_loss�(����r0       ���_	
+?۷g��A��*!
+
+train/explained_variance  @4h6C)+       ��K	
+?۷g��A��*
+
+train/learning_rateRI�9�Y��"       x=�	
+?۷g��A��*
+
+
+train/loss^J�_�2       $V�	
+?۷g��A��*#
+!
+train/policy_gradient_lossA��%դ�$       B+�M	
+?۷g��A��*
+
+train/reward;C/�n(       �pJ	
+?۷g��A��*
+
+train/value_lossi�JN��0        )�P	��c�g��A��*
+
+time/fps @tD��$'       ��F	�c�g��A��*
+
+train/approx_kl�N,7�w=u+       ��K	�c�g��A��*
+
+train/clip_fraction    ~�](       �pJ	�c�g��A��*
+
+train/clip_range��L>�ʴ�*       ����	�c�g��A��*
+
+train/entropy_loss ���U��0       ���_	�c�g��A��*!
+
+train/explained_variance  @4�Mf�+       ��K	�c�g��A��*
+
+train/learning_rateRI�9��V�"       x=�	�c�g��A��*
+
+
+train/loss�gJp�_�2       $V�	�c�g��A��*#
+!
+train/policy_gradient_loss*I���$       B+�M	�c�g��A��*
+
+train/reward�;C�˺�(       �pJ	�c�g��A��*
+
+train/value_lossO�J'j��        )�P	�9�g��A��*
+
+time/fps  tD 
+�.'       ��F	�B�g��A��*
+
+train/approx_kl�8O6�m�a+       ��K	�B�g��A��*
+
+train/clip_fraction    \�&0(       �pJ	�B�g��A��*
+
+train/clip_range��L>be��*       ����	�B�g��A��*
+
+train/entropy_loss�2��Y�T�0       ���_	�B�g��A��*!
+
+train/explained_variance  ?8�_+       ��K	�B�g��A��*
+
+train/learning_rateRI�9a��"       x=�	�B�g��A��*
+
+
+train/lossq�Ji�1%2       $V�	�B�g��A��*#
+!
+train/policy_gradient_loss�H�`��$       B+�M	�B�g��A��*
+
+train/reward���B�6�(       �pJ	�B�g��A��*
+
+train/value_losss��J��X        )�P	�Bv�g��A��*
+
+time/fps �sD#��'       ��F	�Bv�g��A��*
+
+train/approx_kl _�7{X7o+       ��K	�Bv�g��A��*
+
+train/clip_fraction    ο �(       �pJ	�Bv�g��A��*
+
+train/clip_range��L>��I*       ����	�Bv�g��A��*
+
+train/entropy_lossXo���0       ���_	�Bv�g��A��*!
+
+train/explained_variance  �6��7G+       ��K	�Bv�g��A��*
+
+train/learning_rateRI�9�(�-"       x=�	�Bv�g��A��*
+
+
+train/lossN��I*��2       $V�	�Bv�g��A��*#
+!
+train/policy_gradient_lossvy���?N$       B+�M	�Bv�g��A��*
+
+train/reward�>�B�+Fd(       �pJ	�Bv�g��A��*
+
+train/value_loss��iJ۹�        )�P	1���g��A��*
+
+time/fps �sD�|�*'       ��F	1���g��A��*
+
+train/approx_klLu�8�2��+       ��K	1���g��A��*
+
+train/clip_fraction    ��rs(       �pJ	1���g��A��*
+
+train/clip_range��L>$��*       ����	1���g��A��*
+
+train/entropy_lossbm����*;0       ���_	1���g��A��*!
+
+train/explained_variance  h�/�Є+       ��K	1���g��A��*
+
+train/learning_rateRI�9��_�"       x=�	1���g��A��*
+
+
+train/loss�-�I�=l�2       $V�	1���g��A��*#
+!
+train/policy_gradient_loss�w����,$       B+�M	1���g��A��*
+
+train/rewardc����GD!(       �pJ	1���g��A��*
+
+train/value_lossLJM�pH        )�P	/��g��A��*
+
+time/fps @rDA�e
+'       ��F	/��g��A��*
+
+train/approx_kl�=�9�y��+       ��K	/��g��A��*
+
+train/clip_fraction    �|O(       �pJ	/��g��A��*
+
+train/clip_range��L>{�*       ����	/��g��A��*
+
+train/entropy_loss%`��5#
+�0       ���_	/��g��A��*!
+
+train/explained_variance  @��C��+       ��K	/��g��A��*
+
+train/learning_rateRI�9��X�"       x=�	/��g��A��*
+
+
+train/loss{H>��\2       $V�	/��g��A��*#
+!
+train/policy_gradient_loss�X�q��<$       B+�M	/��g��A��*
+
+train/rewardJ��A�z�t(       �pJ	/��g��A��*
+
+train/value_loss�sH��0        )�P	:#�g��A��*
+
+time/fps @rD��F�'       ��F	:#�g��A��*
+
+train/approx_kl��7;[�Р+       ��K	:#�g��A��*
+
+train/clip_fraction    ��)�(       �pJ	:#�g��A��*
+
+train/clip_range��L>��h*       ����	:#�g��A��*
+
+train/entropy_lossJ��	��W0       ���_	:#�g��A��*!
+
+train/explained_variance  P5O`�+       ��K	:#�g��A��*
+
+train/learning_rateRI�9��/"       x=�	:#�g��A��*
+
+
+train/loss�U�G���U2       $V�	:#�g��A��*#
+!
+train/policy_gradient_loss�ú`��X$       B+�M	:#�g��A��*
+
+train/rewardY+�Bߜ2(       �pJ	:#�g��A��*
+
+train/value_loss�/=H���
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_8/events.out.tfevents.1724751300.Trading.11060.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_8/events.out.tfevents.1724751300.Trading.11060.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_8/events.out.tfevents.1724751300.Trading.11060.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_8/events.out.tfevents.1724751300.Trading.11060.0	
@@ -0,0 +1,666 @@
+H       ��H�	3+qg��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer����       QKD	��gqg��A�*
+
+time/fps  �D����#       ��wC	��gqg��A�*
+
+train/reward�'�?׾s�       QKD	;�2rg��A� *
+
+time/fps  mD����&       sO� 	��2rg��A� *
+
+train/approx_kl��3<��y*       ����	��2rg��A� *
+
+train/clip_fraction��3=��g&'       ��F	��2rg��A� *
+
+train/clip_range��L>�7�#)       7�_ 	��2rg��A� *
+
+train/entropy_lossUh��o&U�/       m]P	��2rg��A� *!
+
+train/explained_variance �M�D#�%*       ����	��2rg��A� *
+
+train/learning_rateRI�9��!!       {��	��2rg��A� *
+
+
+train/loss���F�_|1       ����	��2rg��A� *#
+!
+train/policy_gradient_loss�˚�� 8�#       ��wC	��2rg��A� *
+
+train/reward ��~��:'       ��F	��2rg��A� *
+
+train/value_loss(ZWG�St'       QKD	���rg��A�0*
+
+time/fps �QD�\Y&       sO� 	���rg��A�0*
+
+train/approx_kl
+W<���*       ����	���rg��A�0*
+
+train/clip_fraction ��=�j@'       ��F	���rg��A�0*
+
+train/clip_range��L>�(zh)       7�_ 	���rg��A�0*
+
+train/entropy_lossl؊�>��/       m]P	���rg��A�0*!
+
+train/explained_variance  d6��M*       ����	���rg��A�0*
+
+train/learning_rateRI�9����!       {��	���rg��A�0*
+
+
+train/loss�ġE�
+Y=1       ����	���rg��A�0*#
+!
+train/policy_gradient_loss����/��#       ��wC	���rg��A�0*
+
+train/reward�b�٥'       ��F	���rg��A�0*
+
+train/value_loss.t)F�ڡ�       QKD	WQ�sg��A�@*
+
+time/fps @EDw�g&       sO� 	WQ�sg��A�@*
+
+train/approx_kl,+9��7X*       ����	WQ�sg��A�@*
+
+train/clip_fraction    Ŗ��'       ��F	WQ�sg��A�@*
+
+train/clip_range��L>�(_�)       7�_ 	WQ�sg��A�@*
+
+train/entropy_loss�<���C�F/       m]P	WQ�sg��A�@*!
+
+train/explained_variance  �����*       ����	WQ�sg��A�@*
+
+train/learning_rateRI�9�-�o!       {��	WQ�sg��A�@*
+
+
+train/loss��GH��N1       ����	WQ�sg��A�@*#
+!
+train/policy_gradient_loss^��8��4�#       ��wC	WQ�sg��A�@*
+
+train/reward5�x���l{'       ��F	WQ�sg��A�@*
+
+train/value_loss���H���       QKD	��~tg��A�P*
+
+time/fps �=D���N&       sO� 	��~tg��A�P*
+
+train/approx_klJ-	9��{*       ����	��~tg��A�P*
+
+train/clip_fraction    t���'       ��F	��~tg��A�P*
+
+train/clip_range��L>6w)       7�_ 	��~tg��A�P*
+
+train/entropy_loss&)����P~/       m]P	��~tg��A�P*!
+
+train/explained_variance   49�׈*       ����	��~tg��A�P*
+
+train/learning_rateRI�9iѤ!       {��	��~tg��A�P*
+
+
+train/loss٦iG��1       ����	��~tg��A�P*#
+!
+train/policy_gradient_loss��ѹ�+�?#       ��wC	��~tg��A�P*
+
+train/reward�|���s|"'       ��F	��~tg��A�P*
+
+train/value_loss �G7#H       QKD	LAug��A�`*
+
+time/fps �9DwN�|&       sO� 	LAug��A�`*
+
+train/approx_kl�{M:lV�*       ����	LAug��A�`*
+
+train/clip_fraction    ז��'       ��F	LAug��A�`*
+
+train/clip_range��L>��	�)       7�_ 	LAug��A�`*
+
+train/entropy_loss�%����W/       m]P	LAug��A�`*!
+
+train/explained_variance  �5G��*       ����	LAug��A�`*
+
+train/learning_rateRI�9�-E!       {��	LAug��A�`*
+
+
+train/loss�Y�F�W{�1       ����	LAug��A�`*#
+!
+train/policy_gradient_lossA�����a1#       ��wC	LAug��A�`*
+
+train/reward���K���'       ��F	LAug��A�`*
+
+train/value_loss�?rGLI�       QKD	��vg��A�p*
+
+time/fps �6D���7&       sO� 	��vg��A�p*
+
+train/approx_klM�<��X*       ����	��vg��A�p*
+
+train/clip_fraction  �<L5�'       ��F	��vg��A�p*
+
+train/clip_range��L>�Y��)       7�_ 	��vg��A�p*
+
+train/entropy_loss������/       m]P	��vg��A�p*!
+
+train/explained_variance  �����*       ����	��vg��A�p*
+
+train/learning_rateRI�9jH˕!       {��	��vg��A�p*
+
+
+train/lossk5�FqXQ�1       ����	��vg��A�p*#
+!
+train/policy_gradient_loss�g<�C	=[#       ��wC	��vg��A�p*
+
+train/reward ďA�]��'       ��F	��vg��A�p*
+
+train/value_loss�G"�a.        )�P		�vg��A��*
+
+time/fps @4D��'       ��F		�vg��A��*
+
+train/approx_klL�<�a+       ��K		�vg��A��*
+
+train/clip_fraction�Y�=�8�(       �pJ		�vg��A��*
+
+train/clip_range��L>MQ�*       ����		�vg��A��*
+
+train/entropy_lossSi���D0       ���_		�vg��A��*!
+
+train/explained_variance  �3��	+       ��K		�vg��A��*
+
+train/learning_rateRI�9�i�"       x=�		�vg��A��*
+
+
+train/loss��F���|2       $V�		�vg��A��*#
+!
+train/policy_gradient_loss�I�%��	$       B+�M		�vg��A��*
+
+train/rewardX�@��;�(       �pJ		�vg��A��*
+
+train/value_loss
+IGe���        )�P	7�wg��A��*
+
+time/fps  2Dm�d'       ��F	��wg��A��*
+
+train/approx_klV��:��fe+       ��K	��wg��A��*
+
+train/clip_fraction  <����(       �pJ	��wg��A��*
+
+train/clip_range��L>Q�.D*       ����	��wg��A��*
+
+train/entropy_loss`E��݈��0       ���_	��wg��A��*!
+
+train/explained_variance ��7aO�+       ��K	��wg��A��*
+
+train/learning_rateRI�9X�e�"       x=�	��wg��A��*
+
+
+train/loss��Fm�{82       $V�	��wg��A��*#
+!
+train/policy_gradient_loss�� ;S[$       B+�M	��wg��A��*
+
+train/reward�CSA�i-�(       �pJ	��wg��A��*
+
+train/value_lossm֖F���[        )�P	Ԁ\xg��A��*
+
+time/fps �0D#CH'       ��F	Ԁ\xg��A��*
+
+train/approx_kl,�(<|�f^+       ��K	Ԁ\xg��A��*
+
+train/clip_fraction  <�}J�(       �pJ	Ԁ\xg��A��*
+
+train/clip_range��L>��x�*       ����	Ԁ\xg��A��*
+
+train/entropy_loss|P~����0       ���_	Ԁ\xg��A��*!
+
+train/explained_variance ~�;�
+%�+       ��K	Ԁ\xg��A��*
+
+train/learning_rateRI�9!T��"       x=�	Ԁ\xg��A��*
+
+
+train/lossK�.F\��2       $V�	Ԁ\xg��A��*#
+!
+train/policy_gradient_lossn������$       B+�M	Ԁ\xg��A��*
+
+train/rewardr�r��{}�(       �pJ	Ԁ\xg��A��*
+
+train/value_loss2�F���f        )�P	��'yg��A��*
+
+time/fps  /D;��,'       ��F	��'yg��A��*
+
+train/approx_kl��8�Y�y+       ��K	��'yg��A��*
+
+train/clip_fraction    ��T*(       �pJ	��'yg��A��*
+
+train/clip_range��L>��x�*       ����	��'yg��A��*
+
+train/entropy_lossJ�k���w�0       ���_	��'yg��A��*!
+
+train/explained_variance  �7s?�+       ��K	��'yg��A��*
+
+train/learning_rateRI�9�B��"       x=�	��'yg��A��*
+
+
+train/loss�F�F�};�2       $V�	��'yg��A��*#
+!
+train/policy_gradient_lossb��Y��$       B+�M	��'yg��A��*
+
+train/rewardH���ؖ�m(       �pJ	��'yg��A��*
+
+train/value_loss&�[G5�q�        )�P	���yg��A��*
+
+time/fps �.D�|U'       ��F	���yg��A��*
+
+train/approx_kl���6Z�+       ��K	���yg��A��*
+
+train/clip_fraction    h�wQ(       �pJ	���yg��A��*
+
+train/clip_range��L>�y3�*       ����	���yg��A��*
+
+train/entropy_loss�ou�*���0       ���_	���yg��A��*!
+
+train/explained_variance  l6���+       ��K	���yg��A��*
+
+train/learning_rateRI�9u
+[�"       x=�	���yg��A��*
+
+
+train/loss	�II�s�d2       $V�	���yg��A��*#
+!
+train/policy_gradient_loss�7���k��$       B+�M	���yg��A��*
+
+train/reward�T�E��(       �pJ	
+��yg��A��*
+
+train/value_loss��I��x&        )�P	� �zg��A��*
+
+time/fps �-D�¼�'       ��F	� �zg��A��*
+
+train/approx_klT��8��}+       ��K	� �zg��A��*
+
+train/clip_fraction    4G�(       �pJ	� �zg��A��*
+
+train/clip_range��L>2F�*       ����	� �zg��A��*
+
+train/entropy_loss��{�z��)0       ���_	� �zg��A��*!
+
+train/explained_variance �8�P�+       ��K	� �zg��A��*
+
+train/learning_rateRI�9!�kR"       x=�	� �zg��A��*
+
+
+train/loss�vkIC;l2       $V�	� �zg��A��*#
+!
+train/policy_gradient_loss�����$       B+�M	� �zg��A��*
+
+train/rewardv���HGP(       �pJ	� �zg��A��*
+
+train/value_loss-e�I
+��S        )�P	��y{g��A��*
+
+time/fps �,D��{'       ��F	��y{g��A��*
+
+train/approx_kl�{9ܞj�+       ��K	��y{g��A��*
+
+train/clip_fraction    ���}(       �pJ	��y{g��A��*
+
+train/clip_range��L>uA��*       ����	��y{g��A��*
+
+train/entropy_loss�|�T/|-0       ���_	��y{g��A��*!
+
+train/explained_variance  �6�I�+       ��K	��y{g��A��*
+
+train/learning_rateRI�9},�"       x=�	��y{g��A��*
+
+
+train/loss�pIf�ʩ2       $V�	��y{g��A��*#
+!
+train/policy_gradient_loss�����$       B+�M	��y{g��A��*
+
+train/reward+���{ 
+(       �pJ	��y{g��A��*
+
+train/value_loss
+��I��        )�P	w?|g��A��*
+
+time/fps �,D��nb'       ��F	w?|g��A��*
+
+train/approx_kl�ڶ8rƫ�+       ��K	w?|g��A��*
+
+train/clip_fraction    �� �(       �pJ	w?|g��A��*
+
+train/clip_range��L>A��@*       ����	w?|g��A��*
+
+train/entropy_lossz�u�G���0       ���_	w?|g��A��*!
+
+train/explained_variance  t6/��+       ��K	w?|g��A��*
+
+train/learning_rateRI�9��3H"       x=�	w?|g��A��*
+
+
+train/lossE|;I����2       $V�	w?|g��A��*#
+!
+train/policy_gradient_loss숹��v�$       B+�M	w?|g��A��*
+
+train/reward i��MU�(       �pJ	w?|g��A��*
+
+train/value_loss���IR���        )�P	~�}g��A��*
+
+time/fps �+D�.$�'       ��F	~�}g��A��*
+
+train/approx_kl�O8__>%+       ��K	~�}g��A��*
+
+train/clip_fraction    ��z(       �pJ	~�}g��A��*
+
+train/clip_range��L>�3_�*       ����	~�}g��A��*
+
+train/entropy_loss2q�7�.0       ���_	~�}g��A��*!
+
+train/explained_variance  @�x:f�+       ��K	~�}g��A��*
+
+train/learning_rateRI�9��W�"       x=�	~�}g��A��*
+
+
+train/loss���H9.8j2       $V�	~�}g��A��*#
+!
+train/policy_gradient_loss�xk�>��$       B+�M	~�}g��A��*
+
+train/reward����5(       �pJ	~�}g��A��*
+
+train/value_loss)pWI(�I�        )�P	yn�}g��A��*
+
+time/fps �+D����'       ��F	yn�}g��A��*
+
+train/approx_kl^�E9���w+       ��K	yn�}g��A��*
+
+train/clip_fraction    2{(       �pJ	yn�}g��A��*
+
+train/clip_range��L>�U�*       ����	yn�}g��A��*
+
+train/entropy_loss���2�k0       ���_	yn�}g��A��*!
+
+train/explained_variance  ��x_[�+       ��K	yn�}g��A��*
+
+train/learning_rateRI�9�W�"       x=�	yn�}g��A��*
+
+
+train/loss�;I���2       $V�	yn�}g��A��*#
+!
+train/policy_gradient_loss	S/�e��O$       B+�M	yn�}g��A��*
+
+train/reward~-��
+&�(       �pJ	yn�}g��A��*
+
+train/value_loss�T�I�̥        )�P	 �~g��A��*
+
+time/fps @+D����'       ��F	 �~g��A��*
+
+train/approx_kl��:~��+       ��K	 �~g��A��*
+
+train/clip_fraction    ��O(       �pJ	 �~g��A��*
+
+train/clip_range��L>�qMC*       ����	 �~g��A��*
+
+train/entropy_loss,O{���~�0       ���_	 �~g��A��*!
+
+train/explained_variance  .7��+       ��K	 �~g��A��*
+
+train/learning_rateRI�9�
+c�"       x=�	 �~g��A��*
+
+
+train/loss�}�H~_N�2       $V�	 �~g��A��*#
+!
+train/policy_gradient_loss~�s�����$       B+�M	 �~g��A��*
+
+train/reward�i%�b���(       �pJ	 �~g��A��*
+
+train/value_lossc�jI����        )�P	��Wg��A��*
+
+time/fps �*DUO�+'       ��F	��Wg��A��*
+
+train/approx_kl�ˌ:]���+       ��K	��Wg��A��*
+
+train/clip_fraction    *X�(       �pJ	��Wg��A��*
+
+train/clip_range��L>
+C|�*       ����	��Wg��A��*
+
+train/entropy_loss�v��ˎ0       ���_	��Wg��A��*!
+
+train/explained_variance   �C�J�+       ��K	��Wg��A��*
+
+train/learning_rateRI�9��|�"       x=�	��Wg��A��*
+
+
+train/loss��I����2       $V�	��Wg��A��*#
+!
+train/policy_gradient_lossnk�ʉ��$       B+�M	��Wg��A��*
+
+train/reward�kBM��(       �pJ	��Wg��A��*
+
+train/value_lossa:�I!��B        )�P	F� �g��A��*
+
+time/fps �*D>ȤG'       ��F	F� �g��A��*
+
+train/approx_kl,�I<��'�+       ��K	F� �g��A��*
+
+train/clip_fraction�̍=`q(       �pJ	F� �g��A��*
+
+train/clip_range��L>L4�*       ����	F� �g��A��*
+
+train/entropy_loss�'c���?�0       ���_	F� �g��A��*!
+
+train/explained_variance   ���o+       ��K	F� �g��A��*
+
+train/learning_rateRI�96 "       x=�	F� �g��A��*
+
+
+train/loss�U	Hѩ�2       $V�	F� �g��A��*#
+!
+train/policy_gradient_lossO����1l$       B+�M	F� �g��A��*
+
+train/reward)s�B�2�(       �pJ	F� �g��A��*
+
+train/value_lossui�H�.a        )�P	�y�g��A��*
+
+time/fps  *Dq��'       ��F	�y�g��A��*
+
+train/approx_kl��:��?+       ��K	�y�g��A��*
+
+train/clip_fraction    �tlk(       �pJ	�y�g��A��*
+
+train/clip_range��L>�cl�*       ����	�y�g��A��*
+
+train/entropy_lossbd���0       ���_	�y�g��A��*!
+
+train/explained_variance  �4ɣ.�+       ��K	�y�g��A��*
+
+train/learning_rateRI�9�KH�"       x=�	�y�g��A��*
+
+
+train/loss-�H��2       $V�	�y�g��A��*#
+!
+train/policy_gradient_loss#Ѡ�Ѝ_�$       B+�M	�y�g��A��*
+
+train/reward%B!F��(       �pJ	�y�g��A��*
+
+train/value_loss?%jI*��d        )�P	>���g��A��*
+
+time/fps �)D��_�'       ��F	>���g��A��*
+
+train/approx_klG6;�!z+       ��K	>���g��A��*
+
+train/clip_fraction33�9>�<�(       �pJ	>���g��A��*
+
+train/clip_range��L>S�G�*       ����	>���g��A��*
+
+train/entropy_lossb�_�=��I0       ���_	>���g��A��*!
+
+train/explained_variance  @4�c�6+       ��K	>���g��A��*
+
+train/learning_rateRI�9"��P"       x=�	>���g��A��*
+
+
+train/loss�£Hc_�2       $V�	>���g��A��*#
+!
+train/policy_gradient_loss�+�W��$       B+�M	>���g��A��*
+
+train/reward��YB�f\(       �pJ	>���g��A��*
+
+train/value_loss�\4I@Z��        )�P	)�~�g��A��*
+
+time/fps @)Dds��'       ��F	)�~�g��A��*
+
+train/approx_klk
+;.�+       ��K	)�~�g��A��*
+
+train/clip_fractionfff:x���(       �pJ	)�~�g��A��*
+
+train/clip_range��L>e�DJ*       ����	)�~�g��A��*
+
+train/entropy_lossY�ϝ�w0       ���_	)�~�g��A��*!
+
+train/explained_variance ��8�s��+       ��K	)�~�g��A��*
+
+train/learning_rateRI�9|�?�"       x=�	)�~�g��A��*
+
+
+train/loss�χH8$��2       $V�	)�~�g��A��*#
+!
+train/policy_gradient_lossMN��5�<$       B+�M	)�~�g��A��*
+
+train/rewardu9�B[c`N(       �pJ	)�~�g��A��*
+
+train/value_loss?�6Ia��        )�P	�UO�g��A��*
+
+time/fps �(D���'       ��F	�UO�g��A��*
+
+train/approx_kl���:�5C	+       ��K	�UO�g��A��*
+
+train/clip_fraction    ���b(       �pJ	�UO�g��A��*
+
+train/clip_range��L>ǅ��*       ����	�UO�g��A��*
+
+train/entropy_loss�m]���.�0       ���_	�UO�g��A��*!
+
+train/explained_variance  P��{�+       ��K	�UO�g��A��*
+
+train/learning_rateRI�9���"       x=�	�UO�g��A��*
+
+
+train/loss��9I�:2       $V�	�UO�g��A��*#
+!
+train/policy_gradient_loss��6���$       B+�M	�UO�g��A��*
+
+train/reward�t�B8��(       �pJ	�UO�g��A��*
+
+train/value_loss-�I����        )�P	���g��A��*
+
+time/fps �(D�+��'       ��F	���g��A��*
+
+train/approx_kl���:�Yi�+       ��K	���g��A��*
+
+train/clip_fractionff�:�]��(       �pJ	���g��A��*
+
+train/clip_range��L>�:2,*       ����	���g��A��*
+
+train/entropy_loss_^l�*m�0       ���_	���g��A��*!
+
+train/explained_variance  ���	k�+       ��K	���g��A��*
+
+train/learning_rateRI�9���"       x=�	���g��A��*
+
+
+train/loss�I�I��5�2       $V�	���g��A��*#
+!
+train/policy_gradient_loss�޺v�r$       B+�M	���g��A��*
+
+train/reward�\�BH�k�(       �pJ	���g��A��*
+
+train/value_loss�SJ����        )�P	7��g��A��*
+
+time/fps @(D-�'       ��F	7��g��A��*
+
+train/approx_klR�f:,�++       ��K	7��g��A��*
+
+train/clip_fraction    7'�8(       �pJ	7��g��A��*
+
+train/clip_range��L>L�*       ����	7��g��A��*
+
+train/entropy_loss%�r�?�iv0       ���_	7��g��A��*!
+
+train/explained_variance  ���^l+       ��K	7��g��A��*
+
+train/learning_rateRI�9*���"       x=�	7��g��A��*
+
+
+train/loss��J�$�N2       $V�	7��g��A��*#
+!
+train/policy_gradient_loss�	�+L�+$       B+�M	7��g��A��*
+
+train/reward�/�BZR8(       �pJ	7��g��A��*
+
+train/value_losscόJF�N        )�P	T���g��A��*
+
+time/fps @(D��5B'       ��F	T���g��A��*
+
+train/approx_kl��;ܪ��+       ��K	T���g��A��*
+
+train/clip_fraction���8�0�q(       �pJ	T���g��A��*
+
+train/clip_range��L>��9/*       ����	T���g��A��*
+
+train/entropy_lossQ�s���0       ���_	T���g��A��*!
+
+train/explained_variance   ���x@+       ��K	T���g��A��*
+
+train/learning_rateRI�9��M�"       x=�	T���g��A��*
+
+
+train/loss U�ITL/2       $V�	T���g��A��*#
+!
+train/policy_gradient_loss����$       B+�M	T���g��A��*
+
+train/reward���BÓ]�(       �pJ	T���g��A��*
+
+train/value_loss��rJʋ~q        )�P	s�m�g��A��*
+
+time/fps  (D
+'       ��F	s�m�g��A��*
+
+train/approx_klv;�;G�e+       ��K	s�m�g��A��*
+
+train/clip_fraction���;r-�A(       �pJ	s�m�g��A��*
+
+train/clip_range��L>be�U*       ����	s�m�g��A��*
+
+train/entropy_loss`]����"0       ���_	s�m�g��A��*!
+
+train/explained_variance   ��v�+       ��K	s�m�g��A��*
+
+train/learning_rateRI�9�7t�"       x=�	s�m�g��A��*
+
+
+train/loss^��I;��2       $V�	s�m�g��A��*#
+!
+train/policy_gradient_loss�=y�
+��$       B+�M	s�m�g��A��*
+
+train/reward�A#C�d ,(       �pJ	s�m�g��A��*
+
+train/value_lossH�kJ�.ۋ        )�P	|�/�g��A��*
+
+time/fps  (DW��'       ��F	|�/�g��A��*
+
+train/approx_kl�*;LZ�m+       ��K	|�/�g��A��*
+
+train/clip_fraction��;�ӘA(       �pJ	|�/�g��A��*
+
+train/clip_range��L>�I7*       ����	|�/�g��A��*
+
+train/entropy_loss�QY���c0       ���_	|�/�g��A��*!
+
+train/explained_variance    �k�+       ��K	|�/�g��A��*
+
+train/learning_rateRI�9�^?"       x=�	|�/�g��A��*
+
+
+train/loss޶uJ�pa�2       $V�	|�/�g��A��*#
+!
+train/policy_gradient_loss9�źy���$       B+�M	|�/�g��A��*
+
+train/reward��CA�U(       �pJ	|�/�g��A��*
+
+train/value_loss+��J�I[
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_15/events.out.tfevents.1724752667.Trading.106380.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_15/events.out.tfevents.1724752667.Trading.106380.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_15/events.out.tfevents.1724752667.Trading.106380.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_15/events.out.tfevents.1724752667.Trading.106380.0	
@@ -0,0 +1,7 @@
+H       ��H�	(���h��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer0�5       QKD	����h��A�c*
+
+time/fps @�D�<ʉ#       ��wC	����h��A�c*
+
+train/reward����: jN
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_14/events.out.tfevents.1724752650.Trading.16248.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_14/events.out.tfevents.1724752650.Trading.16248.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_14/events.out.tfevents.1724752650.Trading.16248.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_14/events.out.tfevents.1724752650.Trading.16248.0	
@@ -0,0 +1,3 @@
+H       ��H�	���h��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer���
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_13/events.out.tfevents.1724752635.Trading.58792.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_13/events.out.tfevents.1724752635.Trading.58792.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_13/events.out.tfevents.1724752635.Trading.58792.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_13/events.out.tfevents.1724752635.Trading.58792.0	
@@ -0,0 +1,7 @@
+H       ��H�	�i׾h��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer�&��       QKD	5n��h��A�c*
+
+time/fps ��D3J:j#       ��wC	�~��h��A�c*
+
+train/reward������V�
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_12/events.out.tfevents.1724752100.Trading.107652.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_12/events.out.tfevents.1724752100.Trading.107652.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_12/events.out.tfevents.1724752100.Trading.107652.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_12/events.out.tfevents.1724752100.Trading.107652.0	
@@ -0,0 +1,137 @@
+H       ��H�	�
+9h��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer�6�L       QKD	��q:h��A�N*
+
+time/fps ��DO��#       ��wC	��q:h��A�N*
+
+train/reward�E����n�        )�P	a@>h��A��*
+
+time/fps @rDwQz�'       ��F	a@>h��A��*
+
+train/approx_kl.��;S
+�/+       ��K	a@>h��A��*
+
+train/clip_fractionJ6�;��R(       �pJ	a@>h��A��*
+
+train/clip_range��L> *��*       ����	a@>h��A��*
+
+train/entropy_loss�=���4�20       ���_	a@>h��A��*!
+
+train/explained_variance ��8;O3`+       ��K	a@>h��A��*
+
+train/learning_rateRI�9�2�"       x=�	a@>h��A��*
+
+
+train/loss���G��H�2       $V�	a@>h��A��*#
+!
+train/policy_gradient_loss�z������$       B+�M	a@>h��A��*
+
+train/reward�˨B�21T(       �pJ	a@>h��A��*
+
+train/value_loss�H i�}        )�P	j�Bh��A��*
+
+time/fps �RD�yz�'       ��F	j�Bh��A��*
+
+train/approx_kl��<v�`O+       ��K	j�Bh��A��*
+
+train/clip_fraction�߲<��}X(       �pJ	j�Bh��A��*
+
+train/clip_range��L>�o�$*       ����	j�Bh��A��*
+
+train/entropy_loss�4��z�nb0       ���_	j�Bh��A��*!
+
+train/explained_variance  ���H�+       ��K	j�Bh��A��*
+
+train/learning_rateRI�9��@"       x=�	j�Bh��A��*
+
+
+train/loss���G$X�2       $V�	j�Bh��A��*#
+!
+train/policy_gradient_loss1�U�o;IS$       B+�M	j�Bh��A��*
+
+train/rewardΣ��o�#(       �pJ	j�Bh��A��*
+
+train/value_lossy,�H�7q        )�P	L�
+Fh��A��*
+
+time/fps �AD�tM�'       ��F	L�
+Fh��A��*
+
+train/approx_kl���9Ǟ��+       ��K	L�
+Fh��A��*
+
+train/clip_fraction    ��˄(       �pJ	L�
+Fh��A��*
+
+train/clip_range��L>�E
+*       ����	L�
+Fh��A��*
+
+train/entropy_loss^Ȋ���o�0       ���_	L�
+Fh��A��*!
+
+train/explained_variance 1�<m
+�5+       ��K	L�
+Fh��A��*
+
+train/learning_rateRI�9���"       x=�	L�
+Fh��A��*
+
+
+train/loss*�G����2       $V�	L�
+Fh��A��*#
+!
+train/policy_gradient_loss�<��o0��$       B+�M	L�
+Fh��A��*
+
+train/rewardE�C4�U�(       �pJ	L�
+Fh��A��*
+
+train/value_loss���H�=,        )�P	k��Ih��A��*
+
+time/fps �:Dw��'       ��F	k��Ih��A��*
+
+train/approx_kl6E:��o++       ��K	k��Ih��A��*
+
+train/clip_fraction    :�zY(       �pJ	k��Ih��A��*
+
+train/clip_range��L>��*       ����	k��Ih��A��*
+
+train/entropy_loss������0       ���_	k��Ih��A��*!
+
+train/explained_variance _<��T�+       ��K	k��Ih��A��*
+
+train/learning_rateRI�9G�W�"       x=�	k��Ih��A��*
+
+
+train/loss @H�d]2       $V�	k��Ih��A��*#
+!
+train/policy_gradient_loss%(M��?�^$       B+�M	ڳ�Ih��A��*
+
+train/reward��IC�(       �pJ	ڳ�Ih��A��*
+
+train/value_loss�<#I��'        )�P	z��Mh��A��*
+
+time/fps  7DR��'       ��F	z��Mh��A��*
+
+train/approx_kl ��;�;�+       ��K	z��Mh��A��*
+
+train/clip_fractionF��:L�Ӗ(       �pJ	z��Mh��A��*
+
+train/clip_range��L>|�`5*       ����	z��Mh��A��*
+
+train/entropy_loss�v��� 0       ���_	z��Mh��A��*!
+
+train/explained_variance@(��8�RL+       ��K	z��Mh��A��*
+
+train/learning_rateRI�9��"       x=�	z��Mh��A��*
+
+
+train/loss�^BJ��~2       $V�	z��Mh��A��*#
+!
+train/policy_gradient_losssڈ���G\$       B+�M	z��Mh��A��*
+
+train/reward? �C�n�(       �pJ	z��Mh��A��*
+
+train/value_loss���JK��
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_3/events.out.tfevents.1724747499.Trading.101192.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_3/events.out.tfevents.1724747499.Trading.101192.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_3/events.out.tfevents.1724747499.Trading.101192.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_3/events.out.tfevents.1724747499.Trading.101192.0	
@@ -0,0 +1,3044 @@
+H       ��H�	���c��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writerAdٽ       QKD	��c��A�*
+
+time/fps ��D���       QKD	�-�c��A�
+*
+
+time/fps ��D���\&       sO� 	�-�c��A�
+*
+
+train/approx_klقH;�9��*       ����	�-�c��A�
+*
+
+train/clip_fraction    "6:�'       ��F	�-�c��A�
+*
+
+train/clip_range��L>�=)       7�_ 	�-�c��A�
+*
+
+train/entropy_loss�z��74/I/       m]P	�-�c��A�
+*!
+
+train/explained_variance ����� �*       ����	�-�c��A�
+*
+
+train/learning_rateRI�9�!       {��	�-�c��A�
+*
+
+
+train/lossyJH���1       ����	�-�c��A�
+*#
+!
+train/policy_gradient_lossc���^�'       ��F	�-�c��A�
+*
+
+train/value_loss��H{1K       QKD	@CV�c��A�*
+
+time/fps ��D����&       sO� 	@CV�c��A�*
+
+train/approx_kl��9m���*       ����	@CV�c��A�*
+
+train/clip_fraction    |��z'       ��F	@CV�c��A�*
+
+train/clip_range��L>5��9)       7�_ 	@CV�c��A�*
+
+train/entropy_loss��]w=�/       m]P	@CV�c��A�*!
+
+train/explained_variance pX:��d�*       ����	@CV�c��A�*
+
+train/learning_rateRI�9����!       {��	@CV�c��A�*
+
+
+train/loss���H�%�1       ����	@CV�c��A�*#
+!
+train/policy_gradient_loss�#�4�_�'       ��F	@CV�c��A�*
+
+train/value_lossN2I���       QKD	��|�c��A�*
+
+time/fps `�DO�.~&       sO� 	��|�c��A�*
+
+train/approx_kl���8�q��*       ����	��|�c��A�*
+
+train/clip_fraction    �#3�'       ��F	��|�c��A�*
+
+train/clip_range��L>�j��)       7�_ 	��|�c��A�*
+
+train/entropy_loss���3x�y/       m]P	��|�c��A�*!
+
+train/explained_variance `���x*       ����	��|�c��A�*
+
+train/learning_rateRI�9̼ !       {��	:�|�c��A�*
+
+
+train/loss�]�H�P�S1       ����	:�|�c��A�*#
+!
+train/policy_gradient_lossಙ����'       ��F	:�|�c��A�*
+
+train/value_loss��PI7��       QKD	kݧ�c��A�!*
+
+time/fps ��D&'\�&       sO� 	kݧ�c��A�!*
+
+train/approx_kl*��8Z��l*       ����	kݧ�c��A�!*
+
+train/clip_fraction    9�B�'       ��F	kݧ�c��A�!*
+
+train/clip_range��L>��X)       7�_ 	kݧ�c��A�!*
+
+train/entropy_loss������(/       m]P	kݧ�c��A�!*!
+
+train/explained_variance  K7��g*       ����	kݧ�c��A�!*
+
+train/learning_rateRI�9D�n!       {��	kݧ�c��A�!*
+
+
+train/loss�R�I9��1       ����	kݧ�c��A�!*#
+!
+train/policy_gradient_lossC�k���h�'       ��F	kݧ�c��A�!*
+
+train/value_loss�	J�1��       QKD	ڗѻc��A�'*
+
+time/fps ��D5�e&       sO� 	ڗѻc��A�'*
+
+train/approx_kl�{�9����*       ����	ڗѻc��A�'*
+
+train/clip_fraction    ���'       ��F	ڗѻc��A�'*
+
+train/clip_range��L>�pv�)       7�_ 	ڗѻc��A�'*
+
+train/entropy_loss���%�%�/       m]P	>�ѻc��A�'*!
+
+train/explained_variance `
+9��� *       ����	>�ѻc��A�'*
+
+train/learning_rateRI�9[�!       {��	>�ѻc��A�'*
+
+
+train/loss�~I�km�1       ����	>�ѻc��A�'*#
+!
+train/policy_gradient_loss^����h#'       ��F	>�ѻc��A�'*
+
+train/value_loss&x�Ib͑�       QKD	ڸ��c��A�.*
+
+time/fps ��D�ħ�&       sO� 	ڸ��c��A�.*
+
+train/approx_kl9�9s���*       ����	ڸ��c��A�.*
+
+train/clip_fraction    ف'       ��F	ڸ��c��A�.*
+
+train/clip_range��L>���)       7�_ 	ڸ��c��A�.*
+
+train/entropy_loss�ʋ��Bh/       m]P	ڸ��c��A�.*!
+
+train/explained_variance ��8��o�*       ����	ڸ��c��A�.*
+
+train/learning_rateRI�9$���!       {��	ڸ��c��A�.*
+
+
+train/loss�^�H��(1       ����	ڸ��c��A�.*#
+!
+train/policy_gradient_loss-ㇺ��4'       ��F	ڸ��c��A�.*
+
+train/value_loss�/cI���       QKD	�[#�c��A�5*
+
+time/fps ��D�x�&       sO� 	�[#�c��A�5*
+
+train/approx_kl$�9�ɓ�*       ����	�[#�c��A�5*
+
+train/clip_fraction    (N`�'       ��F	�[#�c��A�5*
+
+train/clip_range��L>{��)       7�_ 	�[#�c��A�5*
+
+train/entropy_lossV؋��ܩ/       m]P	�[#�c��A�5*!
+
+train/explained_variance  ��0��&*       ����	�[#�c��A�5*
+
+train/learning_rateRI�95uta!       {��	�[#�c��A�5*
+
+
+train/lossd I}��1       ����	�[#�c��A�5*#
+!
+train/policy_gradient_loss�+�����'       ��F	�[#�c��A�5*
+
+train/value_loss��I��b�       QKD	�NK�c��A�;*
+
+time/fps ��D�u��&       sO� 	�NK�c��A�;*
+
+train/approx_klf9�8ywD?*       ����	�NK�c��A�;*
+
+train/clip_fraction    �%��'       ��F	�NK�c��A�;*
+
+train/clip_range��L>����)       7�_ 	�NK�c��A�;*
+
+train/entropy_loss����	�M/       m]P	�NK�c��A�;*!
+
+train/explained_variance  Q��߭*       ����	�NK�c��A�;*
+
+train/learning_rateRI�9Ծ�U!       {��	�NK�c��A�;*
+
+
+train/loss-��J���1       ����	�NK�c��A�;*#
+!
+train/policy_gradient_loss��ӹ{�!'       ��F	�NK�c��A�;*
+
+train/value_loss|k4K���       QKD	Iu�c��A�B*
+
+time/fps ��D�,A&       sO� 	Iu�c��A�B*
+
+train/approx_kl��7nw7�*       ����	Iu�c��A�B*
+
+train/clip_fraction    �'       ��F	Iu�c��A�B*
+
+train/clip_range��L>[��)       7�_ 	Iu�c��A�B*
+
+train/entropy_loss����B�r/       m]P	Iu�c��A�B*!
+
+train/explained_variance �M9�u0*       ����	Iu�c��A�B*
+
+train/learning_rateRI�9$̓�!       {��	Iu�c��A�B*
+
+
+train/loss�j9K�u2�1       ����	Iu�c��A�B*#
+!
+train/policy_gradient_loss��0�O��'       ��F	Iu�c��A�B*
+
+train/value_loss��K��e�       QKD	�Ȟ�c��A�H*
+
+time/fps  �D^��&       sO� 	�Ȟ�c��A�H*
+
+train/approx_kls��7�F*       ����	�Ȟ�c��A�H*
+
+train/clip_fraction    �s'       ��F	�Ȟ�c��A�H*
+
+train/clip_range��L>�X�)       7�_ 	�Ȟ�c��A�H*
+
+train/entropy_loss�鋿����/       m]P	�Ȟ�c��A�H*!
+
+train/explained_variance  ,�C=��*       ����	�Ȟ�c��A�H*
+
+train/learning_rateRI�9$
+��!       {��	�Ȟ�c��A�H*
+
+
+train/loss��@K؃!L1       ����	�Ȟ�c��A�H*#
+!
+train/policy_gradient_loss��T�2�g{'       ��F	�Ȟ�c��A�H*
+
+train/value_loss&��K��       QKD	7'ʼc��A�O*
+
+time/fps  �D��G�&       sO� 	7'ʼc��A�O*
+
+train/approx_kl�RL7Zp�}*       ����	7'ʼc��A�O*
+
+train/clip_fraction    
+gu�'       ��F	7'ʼc��A�O*
+
+train/clip_range��L>V��@)       7�_ 	7'ʼc��A�O*
+
+train/entropy_loss�ۋ��J�/       m]P	7'ʼc��A�O*!
+
+train/explained_variance  67��R*       ����	7'ʼc��A�O*
+
+train/learning_rateRI�9�a!       {��	7'ʼc��A�O*
+
+
+train/losseR!K�p#�1       ����	7'ʼc��A�O*#
+!
+train/policy_gradient_loss�&ݸ�`g'       ��F	7'ʼc��A�O*
+
+train/value_loss�*�K<�;�       QKD	G%�c��A�V*
+
+time/fps  �D`ҫ�&       sO� 	G%�c��A�V*
+
+train/approx_kl-I�7
+��*       ����	G%�c��A�V*
+
+train/clip_fraction    L�'       ��F	G%�c��A�V*
+
+train/clip_range��L>M�f�)       7�_ 	G%�c��A�V*
+
+train/entropy_lossy؋�Y�{�/       m]P	G%�c��A�V*!
+
+train/explained_variance  �\�
+�*       ����	G%�c��A�V*
+
+train/learning_rateRI�9Q�#!       {��	G%�c��A�V*
+
+
+train/loss��(K�?\e1       ����	G%�c��A�V*#
+!
+train/policy_gradient_loss�\J���e'       ��F	G%�c��A�V*
+
+train/value_loss> �K�7       QKD	�@�c��A�\*
+
+time/fps ��D�v&       sO� 	�@�c��A�\*
+
+train/approx_klM#�6a�*       ����	�@�c��A�\*
+
+train/clip_fraction    ��T'       ��F	�@�c��A�\*
+
+train/clip_range��L>9�>)       7�_ 	�@�c��A�\*
+
+train/entropy_loss�ً�L=�/       m]P	�@�c��A�\*!
+
+train/explained_variance  �7i˞*       ����	�@�c��A�\*
+
+train/learning_rateRI�9�8��!       {��	�@�c��A�\*
+
+
+train/lossLB-Kb�g1       ����	�@�c��A�\*#
+!
+train/policy_gradient_lossN��ɒ�9'       ��F	�@�c��A�\*
+
+train/value_loss���KI?��       QKD	��E�c��A�c*
+
+time/fps @�DPNG�&       sO� 	��E�c��A�c*
+
+train/approx_kl�(�6`)��*       ����	��E�c��A�c*
+
+train/clip_fraction    f�G<'       ��F	��E�c��A�c*
+
+train/clip_range��L>����)       7�_ 	��E�c��A�c*
+
+train/entropy_loss�㋿7�/       m]P	��E�c��A�c*!
+
+train/explained_variance   5R��*       ����	��E�c��A�c*
+
+train/learning_rateRI�9[Fk�!       {��	��E�c��A�c*
+
+
+train/loss��TKW��
+1       ����	��E�c��A�c*#
+!
+train/policy_gradient_loss�s��V�'       ��F	��E�c��A�c*
+
+train/value_loss��K���       QKD	Zso�c��A�j*
+
+time/fps  �D��&       sO� 	Zso�c��A�j*
+
+train/approx_kl���6Ƴ9�*       ����	Zso�c��A�j*
+
+train/clip_fraction    ��2^'       ��F	Zso�c��A�j*
+
+train/clip_range��L>��)       7�_ 	Zso�c��A�j*
+
+train/entropy_loss��Y9\/       m]P	Zso�c��A�j*!
+
+train/explained_variance  �6
+:E
+*       ����	Zso�c��A�j*
+
+train/learning_rateRI�9���!       {��	Zso�c��A�j*
+
+
+train/loss�/CK,�{�1       ����	Zso�c��A�j*#
+!
+train/policy_gradient_loss��ɸ�	a�'       ��F	Zso�c��A�j*
+
+train/value_loss���K]���       QKD	$���c��A�p*
+
+time/fps �D�t�I&       sO� 	$���c��A�p*
+
+train/approx_kl3]�5�s *       ����	$���c��A�p*
+
+train/clip_fraction    ��h�'       ��F	$���c��A�p*
+
+train/clip_range��L>���{)       7�_ 	$���c��A�p*
+
+train/entropy_loss����Q1/       m]P	$���c��A�p*!
+
+train/explained_variance @�l��*       ����	$���c��A�p*
+
+train/learning_rateRI�9�]L!       {��	$���c��A�p*
+
+
+train/loss;MK���1       ����	$���c��A�p*#
+!
+train/policy_gradient_loss{U��9Ý'       ��F	$���c��A�p*
+
+train/value_loss"Z�K�&�       QKD	� ½c��A�w*
+
+time/fps ��D�6oI&       sO� 	� ½c��A�w*
+
+train/approx_klfD7�Ɣ4*       ����	� ½c��A�w*
+
+train/clip_fraction    FP�''       ��F	� ½c��A�w*
+
+train/clip_range��L>�5�)       7�_ 	� ½c��A�w*
+
+train/entropy_loss��������/       m]P	� ½c��A�w*!
+
+train/explained_variance  x��O��*       ����	� ½c��A�w*
+
+train/learning_rateRI�9;w��!       {��	� ½c��A�w*
+
+
+train/losss�KK�ƨ�1       ����	� ½c��A�w*#
+!
+train/policy_gradient_loss�Z�J��'       ��F	� ½c��A�w*
+
+train/value_loss���KG��       QKD	���c��A�}*
+
+time/fps �D��o>&       sO� 	���c��A�}*
+
+train/approx_kl�Z�6�堄*       ����	���c��A�}*
+
+train/clip_fraction    f~�`'       ��F	���c��A�}*
+
+train/clip_range��L>��r�)       7�_ 	���c��A�}*
+
+train/entropy_loss����B�/       m]P	���c��A�}*!
+
+train/explained_variance  G7��Z�*       ����	���c��A�}*
+
+train/learning_rateRI�9[eT�!       {��	���c��A�}*
+
+
+train/loss�YGK��[1       ����	���c��A�}*#
+!
+train/policy_gradient_loss�F���$�'       ��F	���c��A�}*
+
+train/value_loss�G�K�x        )�P	`��c��A��*
+
+time/fps ��D�4�'       ��F	`��c��A��*
+
+train/approx_kl�#7���+       ��K	`��c��A��*
+
+train/clip_fraction    .L�C(       �pJ	`��c��A��*
+
+train/clip_range��L>d'�*       ����	`��c��A��*
+
+train/entropy_lossH���Cw�0       ���_	`��c��A��*!
+
+train/explained_variance   �Q���+       ��K	`��c��A��*
+
+train/learning_rateRI�93s��"       x=�	`��c��A��*
+
+
+train/loss*�OK��2       $V�	`��c��A��*#
+!
+train/policy_gradient_lossax�l;E%(       �pJ	`��c��A��*
+
+train/value_loss3��K.��        )�P	"<�c��A��*
+
+time/fps ��D���4'       ��F	"<�c��A��*
+
+train/approx_klZ�97t��u+       ��K	"<�c��A��*
+
+train/clip_fraction    ����(       �pJ	"<�c��A��*
+
+train/clip_range��L>��̽*       ����	"<�c��A��*
+
+train/entropy_loss����_пc0       ���_	"<�c��A��*!
+
+train/explained_variance   4(;
+�+       ��K	"<�c��A��*
+
+train/learning_rateRI�9�-�g"       x=�	"<�c��A��*
+
+
+train/loss~[�K�`��2       $V�	"<�c��A��*#
+!
+train/policy_gradient_lossӉ����|(       �pJ	"<�c��A��*
+
+train/value_loss�L�gL�        )�P	&�e�c��A��*
+
+time/fps ��D�s�/'       ��F	&�e�c��A��*
+
+train/approx_kl ��4K��+       ��K	&�e�c��A��*
+
+train/clip_fraction    �#��(       �pJ	&�e�c��A��*
+
+train/clip_range��L>e�*       ����	&�e�c��A��*
+
+train/entropy_lossD���Il[0       ���_	&�e�c��A��*!
+
+train/explained_variance  P5�JŬ+       ��K	&�e�c��A��*
+
+train/learning_rateRI�9ϼ�"       x=�	&�e�c��A��*
+
+
+train/loss�ͳKy�Z�2       $V�	&�e�c��A��*#
+!
+train/policy_gradient_loss.����m��(       �pJ	&�e�c��A��*
+
+train/value_loss��1L�K        )�P	q\��c��A��*
+
+time/fps ��D9��'       ��F	q\��c��A��*
+
+train/approx_kl�� 6�++       ��K	q\��c��A��*
+
+train/clip_fraction    �� (       �pJ	q\��c��A��*
+
+train/clip_range��L>��Uv*       ����	q\��c��A��*
+
+train/entropy_loss���xD2k0       ���_	q\��c��A��*!
+
+train/explained_variance  ��(�'*+       ��K	q\��c��A��*
+
+train/learning_rateRI�9�,ԕ"       x=�	q\��c��A��*
+
+
+train/loss��K�k>2       $V�	q\��c��A��*#
+!
+train/policy_gradient_loss2з����(       �pJ	q\��c��A��*
+
+train/value_lossghL�н.        )�P	ha��c��A��*
+
+time/fps ��D�p�'       ��F	ha��c��A��*
+
+train/approx_kl3�7���$+       ��K	ha��c��A��*
+
+train/clip_fraction    #��(       �pJ	ha��c��A��*
+
+train/clip_range��L>2�P�*       ����	ha��c��A��*
+
+train/entropy_loss~��ڑ�}0       ���_	ha��c��A��*!
+
+train/explained_variance  ���~��+       ��K	ha��c��A��*
+
+train/learning_rateRI�9.;!"       x=�	ha��c��A��*
+
+
+train/loss'�KE��$2       $V�	ha��c��A��*#
+!
+train/policy_gradient_loss=(��	(       �pJ	ha��c��A��*
+
+train/value_loss>�L$Z�        )�P	K]߾c��AХ*
+
+time/fps ��D����'       ��F	K]߾c��AХ*
+
+train/approx_kl� �6�C=+       ��K	K]߾c��AХ*
+
+train/clip_fraction    ��j(       �pJ	K]߾c��AХ*
+
+train/clip_range��L>ʇz�*       ����	K]߾c��AХ*
+
+train/entropy_lossh��H���0       ���_	K]߾c��AХ*!
+
+train/explained_variance  �6u}+       ��K	K]߾c��AХ*
+
+train/learning_rateRI�98U�"       x=�	K]߾c��AХ*
+
+
+train/loss�4�K���2       $V�	K]߾c��AХ*#
+!
+train/policy_gradient_loss�¸^+{�(       �pJ	K]߾c��AХ*
+
+train/value_lossc�SL�D�o        )�P	�	�c��A��*
+
+time/fps `�D7��'       ��F	�	�c��A��*
+
+train/approx_kl  �3O�	&+       ��K	�	�c��A��*
+
+train/clip_fraction    $,��(       �pJ	�	�c��A��*
+
+train/clip_range��L>��b*       ����	�	�c��A��*
+
+train/entropy_loss�����v 0       ���_	�	�c��A��*!
+
+train/explained_variance  `�OK"�+       ��K	�	�c��A��*
+
+train/learning_rateRI�9��<"       x=�	�	�c��A��*
+
+
+train/loss�	L�^��2       $V�	�	�c��A��*#
+!
+train/policy_gradient_loss1��5]_�(       �pJ	�	�c��A��*
+
+train/value_loss�ΈL�`�(        )�P	ڻ2�c��A�*
+
+time/fps @�D=(Q�'       ��F	ڻ2�c��A�*
+
+train/approx_kl 6�y(�+       ��K	ڻ2�c��A�*
+
+train/clip_fraction    �9�(       �pJ	ڻ2�c��A�*
+
+train/clip_range��L><Dg*       ����	ڻ2�c��A�*
+
+train/entropy_loss���Йv0       ���_	ڻ2�c��A�*!
+
+train/explained_variance   ��qu+       ��K	ڻ2�c��A�*
+
+train/learning_rateRI�9����"       x=�	ڻ2�c��A�*
+
+
+train/lossޡL���2       $V�	ڻ2�c��A�*#
+!
+train/policy_gradient_loss�~�".o(       �pJ	ڻ2�c��A�*
+
+train/value_lossl4�LI�        )�P	μ\�c��A��*
+
+time/fps  �D��n�'       ��F	μ\�c��A��*
+
+train/approx_kl �P6�+W�+       ��K	μ\�c��A��*
+
+train/clip_fraction    z� �(       �pJ	μ\�c��A��*
+
+train/clip_range��L>��R�*       ����	μ\�c��A��*
+
+train/entropy_loss{��ɔ<[0       ���_	μ\�c��A��*!
+
+train/explained_variance   �qx�A+       ��K	μ\�c��A��*
+
+train/learning_rateRI�9�a�$"       x=�	μ\�c��A��*
+
+
+train/loss�0LK�	o2       $V�	μ\�c��A��*#
+!
+train/policy_gradient_loss��C��;�\(       �pJ	μ\�c��A��*
+
+train/value_loss�j�Lz_        )�P	nR��c��A��*
+
+time/fps @�D�V5�'       ��F	nR��c��A��*
+
+train/approx_kl:ݚ7�V<c+       ��K	nR��c��A��*
+
+train/clip_fraction    Q7Z%(       �pJ	nR��c��A��*
+
+train/clip_range��L>Dlg�*       ����	nR��c��A��*
+
+train/entropy_loss����a�80       ���_	nR��c��A��*!
+
+train/explained_variance  ��Qt�o+       ��K	nR��c��A��*
+
+train/learning_rateRI�9L�~�"       x=�	nR��c��A��*
+
+
+train/lossOL�adF2       $V�	nR��c��A��*#
+!
+train/policy_gradient_lossk��_nx�(       �pJ	nR��c��A��*
+
+train/value_loss8�L�A�        )�P	Fꬿc��A��*
+
+time/fps `�D98`�'       ��F	Fꬿc��A��*
+
+train/approx_kl��i6�N7�+       ��K	Fꬿc��A��*
+
+train/clip_fraction    ׮E�(       �pJ	Fꬿc��A��*
+
+train/clip_range��L>��p*       ����	Fꬿc��A��*
+
+train/entropy_loss���6:��0       ���_	Fꬿc��A��*!
+
+train/explained_variance    E�.+       ��K	Fꬿc��A��*
+
+train/learning_rateRI�9�q."       x=�	Fꬿc��A��*
+
+
+train/loss�L�)�2       $V�	Fꬿc��A��*#
+!
+train/policy_gradient_lossz1�=
+^(       �pJ	Fꬿc��A��*
+
+train/value_loss�B�L�H�D        )�P	!eտc��A��*
+
+time/fps `�Dl�@'       ��F	!eտc��A��*
+
+train/approx_klMi�6�#Q[+       ��K	!eտc��A��*
+
+train/clip_fraction    Qv(       �pJ	!eտc��A��*
+
+train/clip_range��L>��dR*       ����	!eտc��A��*
+
+train/entropy_loss�����`0       ���_	!eտc��A��*!
+
+train/explained_variance  `��V��+       ��K	!eտc��A��*
+
+train/learning_rateRI�9Lhb�"       x=�	!eտc��A��*
+
+
+train/loss�RL��<2       $V�	!eտc��A��*#
+!
+train/policy_gradient_loss�`��nRR(       �pJ	!eտc��A��*
+
+train/value_loss�L��
+        )�P	���c��A��*
+
+time/fps `�D<���'       ��F	���c��A��*
+
+train/approx_kl�A5�p��+       ��K	���c��A��*
+
+train/clip_fraction    �!H�(       �pJ	E��c��A��*
+
+train/clip_range��L>F��*       ����	E��c��A��*
+
+train/entropy_loss& ���ǜ<0       ���_	E��c��A��*!
+
+train/explained_variance  P5p���+       ��K	E��c��A��*
+
+train/learning_rateRI�92�9�"       x=�	E��c��A��*
+
+
+train/loss�LG6�M2       $V�	E��c��A��*#
+!
+train/policy_gradient_loss�Tg���Z(       �pJ	E��c��A��*
+
+train/value_lossci�L����        )�P	%7(�c��A��*
+
+time/fps @�D,E'       ��F	%7(�c��A��*
+
+train/approx_kl
+�=7����+       ��K	%7(�c��A��*
+
+train/clip_fraction    (Ǥh(       �pJ	%7(�c��A��*
+
+train/clip_range��L>���*       ����	%7(�c��A��*
+
+train/entropy_loss�'��̗&e0       ���_	%7(�c��A��*!
+
+train/explained_variance  �4~.��+       ��K	%7(�c��A��*
+
+train/learning_rateRI�95 ��"       x=�	%7(�c��A��*
+
+
+train/loss�'LZ$�2       $V�	%7(�c��A��*#
+!
+train/policy_gradient_loss��4�k�D(       �pJ	%7(�c��A��*
+
+train/value_loss+��L	6�        )�P	XcQ�c��A��*
+
+time/fps  �D��^�'       ��F	XcQ�c��A��*
+
+train/approx_kl�^�5�]E�+       ��K	XcQ�c��A��*
+
+train/clip_fraction    (       �pJ	XcQ�c��A��*
+
+train/clip_range��L>��aH*       ����	XcQ�c��A��*
+
+train/entropy_lossJI����0       ���_	XcQ�c��A��*!
+
+train/explained_variance  ��ᚌ�+       ��K	XcQ�c��A��*
+
+train/learning_rateRI�9Ռ"       x=�	XcQ�c��A��*
+
+
+train/lossުL���{2       $V�	XcQ�c��A��*#
+!
+train/policy_gradient_loss�Tȷ9zy�(       �pJ	XcQ�c��A��*
+
+train/value_lossPӖL-A�        )�P	�z�c��A��*
+
+time/fps  �D	�ȁ'       ��F	�z�c��A��*
+
+train/approx_kl
+7���+       ��K	�z�c��A��*
+
+train/clip_fraction    0���(       �pJ	�z�c��A��*
+
+train/clip_range��L>!r��*       ����	�z�c��A��*
+
+train/entropy_lossO2��V�a�0       ���_	�z�c��A��*!
+
+train/explained_variance  (�
+�G+       ��K	�z�c��A��*
+
+train/learning_rateRI�9-��H"       x=�	�z�c��A��*
+
+
+train/losseVL�2       $V�	�z�c��A��*#
+!
+train/policy_gradient_loss����"g(       �pJ	�z�c��A��*
+
+train/value_loss���L�MH        )�P	6ˣ�c��A��*
+
+time/fps  �D5pG�'       ��F	�ۣ�c��A��*
+
+train/approx_klʹ@5`[��+       ��K	�ۣ�c��A��*
+
+train/clip_fraction    Ez�(       �pJ	�ۣ�c��A��*
+
+train/clip_range��L>5�(�*       ����	�ۣ�c��A��*
+
+train/entropy_loss/��]�0       ���_	�ۣ�c��A��*!
+
+train/explained_variance  (6��+       ��K	�ۣ�c��A��*
+
+train/learning_rateRI�9�^x"       x=�	�ۣ�c��A��*
+
+
+train/losspELb&�2       $V�	�ۣ�c��A��*#
+!
+train/policy_gradient_loss1P�����A(       �pJ	�ۣ�c��A��*
+
+train/value_loss\��L��?w        )�P	m_��c��A��*
+
+time/fps  �DբF�'       ��F	m_��c��A��*
+
+train/approx_klf�6�L�q+       ��K	m_��c��A��*
+
+train/clip_fraction    O��C(       �pJ	m_��c��A��*
+
+train/clip_range��L>��NC*       ����	m_��c��A��*
+
+train/entropy_loss�2��ڗ��0       ���_	m_��c��A��*!
+
+train/explained_variance  `���o+       ��K	m_��c��A��*
+
+train/learning_rateRI�9R,��"       x=�	m_��c��A��*
+
+
+train/loss��L����2       $V�	m_��c��A��*#
+!
+train/policy_gradient_loss��̸�8�w(       �pJ	m_��c��A��*
+
+train/value_losse&�L%��        )�P	!���c��A��*
+
+time/fps  �D+	}'       ��F	!���c��A��*
+
+train/approx_klڤ	7��Wt+       ��K	!���c��A��*
+
+train/clip_fraction    �q&(       �pJ	!���c��A��*
+
+train/clip_range��L>�^M*       ����	!���c��A��*
+
+train/entropy_lossw���>/[0       ���_	!���c��A��*!
+
+train/explained_variance  `�3FL�+       ��K	!���c��A��*
+
+train/learning_rateRI�9�X'�"       x=�	!���c��A��*
+
+
+train/lossQw"LEFG�2       $V�	!���c��A��*#
+!
+train/policy_gradient_loss{��PѶ((       �pJ	!���c��A��*
+
+train/value_loss�ОLE_u�        )�P	T��c��A��*
+
+time/fps  �D�/�'       ��F	���c��A��*
+
+train/approx_kl +6���v+       ��K	���c��A��*
+
+train/clip_fraction    �kk<(       �pJ	���c��A��*
+
+train/clip_range��L>����*       ����	���c��A��*
+
+train/entropy_lossu �����0       ���_	���c��A��*!
+
+train/explained_variance  �����.+       ��K	���c��A��*
+
+train/learning_rateRI�9�TG�"       x=�	���c��A��*
+
+
+train/loss��"L=^�B2       $V�	���c��A��*#
+!
+train/policy_gradient_loss�7
+��톧(       �pJ	���c��A��*
+
+train/value_loss��LpS<#        )�P	s�G�c��A��*
+
+time/fps  �D�;�'       ��F	�G�c��A��*
+
+train/approx_kl��6�2�+       ��K	�G�c��A��*
+
+train/clip_fraction    [��(       �pJ	�G�c��A��*
+
+train/clip_range��L>�)j3*       ����	�G�c��A��*
+
+train/entropy_loss��,]r0       ���_	�G�c��A��*!
+
+train/explained_variance   �9�";+       ��K	�G�c��A��*
+
+train/learning_rateRI�9f$"       x=�	�G�c��A��*
+
+
+train/loss�n)L���2       $V�	�G�c��A��*#
+!
+train/policy_gradient_lossW)縹��(       �pJ	�G�c��A��*
+
+train/value_loss?o�L�ē�        )�P	q�c��AЏ*
+
+time/fps  �D�5-�'       ��F	q�c��AЏ*
+
+train/approx_kl�.�6��i_+       ��K	q�c��AЏ*
+
+train/clip_fraction    �3p(       �pJ	q�c��AЏ*
+
+train/clip_range��L>��J�*       ����	q�c��AЏ*
+
+train/entropy_loss=%������0       ���_	q�c��AЏ*!
+
+train/explained_variance   ��M�+       ��K	q�c��AЏ*
+
+train/learning_rateRI�9%�"       x=�	q�c��AЏ*
+
+
+train/loss�[*L��
+2       $V�	q�c��AЏ*#
+!
+train/policy_gradient_loss�Ϭ�W�y(       �pJ	q�c��AЏ*
+
+train/value_loss�ʧL+	"�        )�P	ݐ��c��A��*
+
+time/fps �D�V�'       ��F	ݐ��c��A��*
+
+train/approx_klf�6�}9+       ��K	ݐ��c��A��*
+
+train/clip_fraction    }�[(       �pJ	ݐ��c��A��*
+
+train/clip_range��L>�s*       ����	ݐ��c��A��*
+
+train/entropy_loss���
+��0       ���_	ݐ��c��A��*!
+
+train/explained_variance  h6����+       ��K	ݐ��c��A��*
+
+train/learning_rateRI�9	f��"       x=�	ݐ��c��A��*
+
+
+train/lossN�*L�-C72       $V�	ݐ��c��A��*#
+!
+train/policy_gradient_loss�E)��%��(       �pJ	ݐ��c��A��*
+
+train/value_lossJ��L{���        )�P	���c��A�*
+
+time/fps ��D�S�t'       ��F	���c��A�*
+
+train/approx_kl �X6Īz�+       ��K	���c��A�*
+
+train/clip_fraction    `VG(       �pJ	���c��A�*
+
+train/clip_range��L>�v�*       ����	���c��A�*
+
+train/entropy_loss���ޞj0       ���_	���c��A�*!
+
+train/explained_variance    e�l�+       ��K	���c��A�*
+
+train/learning_rateRI�9]��"       x=�	���c��A�*
+
+
+train/lossnE)L �2       $V�	���c��A�*#
+!
+train/policy_gradient_loss>!+��fL�(       �pJ	���c��A�*
+
+train/value_lossՐ�L�U�        )�P	U3��c��A��*
+
+time/fps ��DjM��'       ��F	U3��c��A��*
+
+train/approx_kls27���v+       ��K	U3��c��A��*
+
+train/clip_fraction    ��W�(       �pJ	U3��c��A��*
+
+train/clip_range��L>o���*       ����	U3��c��A��*
+
+train/entropy_loss2���t�0       ���_	U3��c��A��*!
+
+train/explained_variance  ��:JrY+       ��K	�;��c��A��*
+
+train/learning_rateRI�9kF�"       x=�	�;��c��A��*
+
+
+train/loss�+(L��2       $V�	�;��c��A��*#
+!
+train/policy_gradient_loss�}��9��(       �pJ	�;��c��A��*
+
+train/value_loss<�L���A        )�P	
+��c��A��*
+
+time/fps ��D���'       ��F	
+��c��A��*
+
+train/approx_kl�+7
+�s+       ��K	
+��c��A��*
+
+train/clip_fraction    ���(       �pJ	
+��c��A��*
+
+train/clip_range��L>/*�_*       ����	
+��c��A��*
+
+train/entropy_loss�݋��S�?0       ���_	
+��c��A��*!
+
+train/explained_variance   ����p+       ��K	
+��c��A��*
+
+train/learning_rateRI�9���"       x=�	
+��c��A��*
+
+
+train/loss<'LZ�:2       $V�	
+��c��A��*#
+!
+train/policy_gradient_loss0A�}((       �pJ	
+��c��A��*
+
+train/value_loss�ԨLJ���        )�P	K�A�c��A�*
+
+time/fps ��D{�y'       ��F	K�A�c��A�*
+
+train/approx_kl  �3+��6+       ��K	K�A�c��A�*
+
+train/clip_fraction    ¹��(       �pJ	K�A�c��A�*
+
+train/clip_range��L>W���*       ����	K�A�c��A�*
+
+train/entropy_lossV����:K�0       ���_	K�A�c��A�*!
+
+train/explained_variance  ����B�+       ��K	K�A�c��A�*
+
+train/learning_rateRI�9��w"       x=�	K�A�c��A�*
+
+
+train/loss�Y"L�s��2       $V�	K�A�c��A�*#
+!
+train/policy_gradient_loss�PJ7~�f�(       �pJ	K�A�c��A�*
+
+train/value_loss!	�L��:�        )�P	�!i�c��A��*
+
+time/fps ��D��Z'       ��F	�!i�c��A��*
+
+train/approx_kl歘6o��C+       ��K	�!i�c��A��*
+
+train/clip_fraction    Z#��(       �pJ	�!i�c��A��*
+
+train/clip_range��L>���	*       ����	�!i�c��A��*
+
+train/entropy_loss�������f0       ���_	�!i�c��A��*!
+
+train/explained_variance    �i�+       ��K	�!i�c��A��*
+
+train/learning_rateRI�9g��w"       x=�	�!i�c��A��*
+
+
+train/lossSPL��_Y2       $V�	�!i�c��A��*#
+!
+train/policy_gradient_loss������D(       �pJ	�!i�c��A��*
+
+train/value_lossy��L����        )�P	�'��c��A��*
+
+time/fps ��D�iu�'       ��F	�'��c��A��*
+
+train/approx_kl�N6��+       ��K	�'��c��A��*
+
+train/clip_fraction    U��(       �pJ	�'��c��A��*
+
+train/clip_range��L>^���*       ����	�'��c��A��*
+
+train/entropy_lossoˋ�a<l�0       ���_	�'��c��A��*!
+
+train/explained_variance  �3�8+       ��K	�'��c��A��*
+
+train/learning_rateRI�9H�)L"       x=�	�'��c��A��*
+
+
+train/loss�L-L� ��2       $V�	�'��c��A��*#
+!
+train/policy_gradient_loss
+�*��Dy(       �pJ	�'��c��A��*
+
+train/value_loss��L=��?        )�P	�ռ�c��A��*
+
+time/fps ��D�Ac'       ��F	�ռ�c��A��*
+
+train/approx_klf�4Ł�j+       ��K	�ռ�c��A��*
+
+train/clip_fraction    ��m�(       �pJ	�ռ�c��A��*
+
+train/clip_range��L>�ձ*       ����	�ռ�c��A��*
+
+train/entropy_loss�닿m(l�0       ���_	�ռ�c��A��*!
+
+train/explained_variance  �4%���+       ��K	�ռ�c��A��*
+
+train/learning_rateRI�9HIA"       x=�	�ռ�c��A��*
+
+
+train/loss,�3Lw rb2       $V�	�ռ�c��A��*#
+!
+train/policy_gradient_loss T��i�{(       �pJ	�ռ�c��A��*
+
+train/value_loss@@�L��6�        )�P	=��c��A��*
+
+time/fps ��D�^'       ��F	=��c��A��*
+
+train/approx_kl���6�޾+       ��K	=��c��A��*
+
+train/clip_fraction    �ݹ(       �pJ	=��c��A��*
+
+train/clip_range��L>4���*       ����	=��c��A��*
+
+train/entropy_loss�拿�<�0       ���_	=��c��A��*!
+
+train/explained_variance  �4[*��+       ��K	=��c��A��*
+
+train/learning_rateRI�9���5"       x=�	=��c��A��*
+
+
+train/lossq5-L_�2       $V�	=��c��A��*#
+!
+train/policy_gradient_loss1+���Hs(       �pJ	=��c��A��*
+
+train/value_loss�L��a3        )�P	���c��A��*
+
+time/fps `�DE~KC'       ��F	���c��A��*
+
+train/approx_kl�|�4�@p�+       ��K	���c��A��*
+
+train/clip_fraction    jg"(       �pJ	���c��A��*
+
+train/clip_range��L>��N�*       ����	���c��A��*
+
+train/entropy_loss�΋�捹0       ���_	���c��A��*!
+
+train/explained_variance    b�8+       ��K	���c��A��*
+
+train/learning_rateRI�9w�6�"       x=�	���c��A��*
+
+
+train/lossK�.LVH2       $V�	���c��A��*#
+!
+train/policy_gradient_lossW���{(       �pJ	���c��A��*
+
+train/value_loss\,�Lؓ�j        )�P	^�>�c��A��*
+
+time/fps  �DBI��'       ��F	^�>�c��A��*
+
+train/approx_klf��5Zբ>+       ��K	^�>�c��A��*
+
+train/clip_fraction    �@�h(       �pJ	^�>�c��A��*
+
+train/clip_range��L>`X>q*       ����	^�>�c��A��*
+
+train/entropy_loss�ً�E�Z0       ���_	^�>�c��A��*!
+
+train/explained_variance   �?���+       ��K	^�>�c��A��*
+
+train/learning_rateRI�9����"       x=�	^�>�c��A��*
+
+
+train/loss��7L����2       $V�	^�>�c��A��*#
+!
+train/policy_gradient_loss`�9��M�(       �pJ	^�>�c��A��*
+
+train/value_loss��L7|�        )�P	ff�c��A��*
+
+time/fps  �Djϔ�'       ��F	ff�c��A��*
+
+train/approx_kl�=6Yu$+       ��K	ff�c��A��*
+
+train/clip_fraction    �l��(       �pJ	zvf�c��A��*
+
+train/clip_range��L>Z�Pa*       ����	zvf�c��A��*
+
+train/entropy_loss�鋿�A�0       ���_	zvf�c��A��*!
+
+train/explained_variance    ��Q'+       ��K	zvf�c��A��*
+
+train/learning_rateRI�9�:�"       x=�	zvf�c��A��*
+
+
+train/loss��9L�c�2       $V�	zvf�c��A��*#
+!
+train/policy_gradient_loss����V��0(       �pJ	zvf�c��A��*
+
+train/value_lossMr�Lӣ�        )�P	綑�c��A��*
+
+time/fps  �D�
+$~'       ��F	綑�c��A��*
+
+train/approx_kl��-7Fd�+       ��K	綑�c��A��*
+
+train/clip_fraction    D��(       �pJ	綑�c��A��*
+
+train/clip_range��L>�&{�*       ����	綑�c��A��*
+
+train/entropy_loss9����[N�0       ���_	綑�c��A��*!
+
+train/explained_variance  @5�]z+       ��K	綑�c��A��*
+
+train/learning_rateRI�9��T"       x=�	綑�c��A��*
+
+
+train/loss]�.LS~(�2       $V�	綑�c��A��*#
+!
+train/policy_gradient_loss"@�侽#(       �pJ	綑�c��A��*
+
+train/value_loss�*�LV8N�        )�P	I���c��A��*
+
+time/fps  �D��T'       ��F	I���c��A��*
+
+train/approx_kl �p6�R#
++       ��K	I���c��A��*
+
+train/clip_fraction    u���(       �pJ	I���c��A��*
+
+train/clip_range��L>��*       ����	I���c��A��*
+
+train/entropy_loss�苿��n�0       ���_	I���c��A��*!
+
+train/explained_variance  ����H+       ��K	I���c��A��*
+
+train/learning_rateRI�9hC�X"       x=�	I���c��A��*
+
+
+train/loss9.L~J�\2       $V�	I���c��A��*#
+!
+train/policy_gradient_loss��W�w-�(       �pJ	I���c��A��*
+
+train/value_loss���L���        )�P	w��c��A��*
+
+time/fps  �D!� '       ��F	w��c��A��*
+
+train/approx_kl3�!6$�i]+       ��K	w��c��A��*
+
+train/clip_fraction    ��#(       �pJ	w��c��A��*
+
+train/clip_range��L>f7��*       ����	w��c��A��*
+
+train/entropy_loss�닿F��X0       ���_	w��c��A��*!
+
+train/explained_variance  ��z�K+       ��K	w��c��A��*
+
+train/learning_rateRI�9'#��"       x=�	w��c��A��*
+
+
+train/loss� 8LF�#2       $V�	w��c��A��*#
+!
+train/policy_gradient_loss]�-���e�(       �pJ	w��c��A��*
+
+train/value_lossf�L䂦1        )�P	`��c��A��*
+
+time/fps @�DtVe6'       ��F	`��c��A��*
+
+train/approx_klfQZ6J"��+       ��K	`��c��A��*
+
+train/clip_fraction    �%:(       �pJ	`��c��A��*
+
+train/clip_range��L>���6*       ����	`��c��A��*
+
+train/entropy_loss�ڋ����0       ���_	`��c��A��*!
+
+train/explained_variance  �3r��+       ��K	`��c��A��*
+
+train/learning_rateRI�9��1�"       x=�	`��c��A��*
+
+
+train/loss2�3L����2       $V�	`��c��A��*#
+!
+train/policy_gradient_lossV�P�S	�1(       �pJ	`��c��A��*
+
+train/value_loss�f�Le��g        )�P	#�0�c��A��*
+
+time/fps `�D�M�9'       ��F	#�0�c��A��*
+
+train/approx_kl���6�2d�+       ��K	#�0�c��A��*
+
+train/clip_fraction    ��1(       �pJ	#�0�c��A��*
+
+train/clip_range��L>�'�c*       ����	#�0�c��A��*
+
+train/entropy_loss�苿��0       ���_	#�0�c��A��*!
+
+train/explained_variance    v|�+       ��K	��0�c��A��*
+
+train/learning_rateRI�97�"       x=�	��0�c��A��*
+
+
+train/loss��,LF?�#2       $V�	��0�c��A��*#
+!
+train/policy_gradient_loss��ڮ	Y(       �pJ	��0�c��A��*
+
+train/value_loss�a�L�D�X        )�P	b�Y�c��A��*
+
+time/fps `�D!���'       ��F	b�Y�c��A��*
+
+train/approx_kl 0�5��.+       ��K	b�Y�c��A��*
+
+train/clip_fraction    �'�d(       �pJ	b�Y�c��A��*
+
+train/clip_range��L>d�e*       ����	b�Y�c��A��*
+
+train/entropy_loss9狿�	�0       ���_	b�Y�c��A��*!
+
+train/explained_variance  �4��+       ��K	b�Y�c��A��*
+
+train/learning_rateRI�9|���"       x=�	b�Y�c��A��*
+
+
+train/loss	�+LKX�2       $V�	b�Y�c��A��*#
+!
+train/policy_gradient_loss$����*�(       �pJ	b�Y�c��A��*
+
+train/value_loss��L�=��        )�P	����c��A��*
+
+time/fps `�D�m�%'       ��F	����c��A��*
+
+train/approx_kl3��5�RR++       ��K	����c��A��*
+
+train/clip_fraction    s�u�(       �pJ	����c��A��*
+
+train/clip_range��L>Q�K�*       ����	����c��A��*
+
+train/entropy_loss�e0       ���_	����c��A��*!
+
+train/explained_variance   ��dt�+       ��K	����c��A��*
+
+train/learning_rateRI�9n8a&"       x=�	����c��A��*
+
+
+train/loss�Lk���2       $V�	����c��A��*#
+!
+train/policy_gradient_loss���͐;�(       �pJ	����c��A��*
+
+train/value_loss&ĞL57۱        )�P	�L��c��A��*
+
+time/fps ��D�ݞ�'       ��F	�L��c��A��*
+
+train/approx_kl&67l�-+       ��K	�L��c��A��*
+
+train/clip_fraction    ��5(       �pJ	�L��c��A��*
+
+train/clip_range��L>��*       ����	�L��c��A��*
+
+train/entropy_loss����ҿ��0       ���_	�L��c��A��*!
+
+train/explained_variance    �6�Y+       ��K	�L��c��A��*
+
+train/learning_rateRI�9ئ�"       x=�	�L��c��A��*
+
+
+train/loss(�LS���2       $V�	�L��c��A��*#
+!
+train/policy_gradient_loss�8��&6�(       �pJ	�L��c��A��*
+
+train/value_lossP��L�E�        )�P	���c��A��*
+
+time/fps ��D=�A�'       ��F	���c��A��*
+
+train/approx_kl7�6&A�M+       ��K	���c��A��*
+
+train/clip_fraction    i�j{(       �pJ	���c��A��*
+
+train/clip_range��L>|c*       ����	���c��A��*
+
+train/entropy_loss4����þ0       ���_	���c��A��*!
+
+train/explained_variance    ��Y�+       ��K	���c��A��*
+
+train/learning_rateRI�9tiN�"       x=�	���c��A��*
+
+
+train/loss��LI���2       $V�	���c��A��*#
+!
+train/policy_gradient_loss���׽�(       �pJ	���c��A��*
+
+train/value_loss��L���4        )�P	����c��A��*
+
+time/fps ��DDb�'       ��F	����c��A��*
+
+train/approx_kl�#�6�TW+       ��K	����c��A��*
+
+train/clip_fraction    B`�(       �pJ	����c��A��*
+
+train/clip_range��L>4�X�*       ����	����c��A��*
+
+train/entropy_loss�����ǂ0       ���_	����c��A��*!
+
+train/explained_variance   ��ǀ�+       ��K	����c��A��*
+
+train/learning_rateRI�9?��"       x=�	����c��A��*
+
+
+train/loss�
+L����2       $V�	����c��A��*#
+!
+train/policy_gradient_loss����gS�(       �pJ	����c��A��*
+
+train/value_loss|8�L�8;t        )�P	�m!�c��A��*
+
+time/fps ��D��
+'       ��F	�m!�c��A��*
+
+train/approx_kl���6�c��+       ��K	�m!�c��A��*
+
+train/clip_fraction    �s�(       �pJ	�m!�c��A��*
+
+train/clip_range��L>3,�*       ����	�m!�c��A��*
+
+train/entropy_loss����=s�0       ���_	�m!�c��A��*!
+
+train/explained_variance   ��g�p+       ��K	�m!�c��A��*
+
+train/learning_rateRI�9�L��"       x=�	�m!�c��A��*
+
+
+train/loss��K��h�2       $V�	�m!�c��A��*#
+!
+train/policy_gradient_loss�"��kq��(       �pJ	�m!�c��A��*
+
+train/value_loss/]L9�        )�P	��H�c��AЮ*
+
+time/fps ��D�:��'       ��F	��H�c��AЮ*
+
+train/approx_kl��k7� �+       ��K	��H�c��AЮ*
+
+train/clip_fraction    ���(       �pJ	��H�c��AЮ*
+
+train/clip_range��L>9�*       ����	��H�c��AЮ*
+
+train/entropy_loss�����K0       ���_	��H�c��AЮ*!
+
+train/explained_variance  �3�N��+       ��K	��H�c��AЮ*
+
+train/learning_rateRI�9�Ń�"       x=�	��H�c��AЮ*
+
+
+train/loss]^�K?;ܺ2       $V�	��H�c��AЮ*#
+!
+train/policy_gradient_loss�c7�{��(       �pJ	��H�c��AЮ*
+
+train/value_loss?UeL�ُ�        )�P	��p�c��A��*
+
+time/fps �D���M'       ��F	��p�c��A��*
+
+train/approx_kl���4�^
+k+       ��K	��p�c��A��*
+
+train/clip_fraction    ��3(       �pJ	��p�c��A��*
+
+train/clip_range��L>�_�*       ����	��p�c��A��*
+
+train/entropy_loss�ร�0       ���_	��p�c��A��*!
+
+train/explained_variance    a�uF+       ��K	��p�c��A��*
+
+train/learning_rateRI�9��"       x=�	��p�c��A��*
+
+
+train/loss ��KX��`2       $V�	��p�c��A��*#
+!
+train/policy_gradient_loss.�7�D-i(       �pJ	��p�c��A��*
+
+train/value_loss��nLe��i        )�P	�֘�c��A�*
+
+time/fps �D�!�''       ��F	Eߘ�c��A�*
+
+train/approx_kl:�7w&1�+       ��K	Eߘ�c��A�*
+
+train/clip_fraction    .6�5(       �pJ	Eߘ�c��A�*
+
+train/clip_range��L>��q*       ����	Eߘ�c��A�*
+
+train/entropy_loss1���lB0       ���_	Eߘ�c��A�*!
+
+train/explained_variance    Qj�t+       ��K	Eߘ�c��A�*
+
+train/learning_rateRI�93���"       x=�	Eߘ�c��A�*
+
+
+train/loss�d�K���2       $V�	Eߘ�c��A�*#
+!
+train/policy_gradient_loss�W��j��(       �pJ	Eߘ�c��A�*
+
+train/value_loss�ugLG�a        )�P	a	��c��A��*
+
+time/fps  �D�*\�'       ��F	a	��c��A��*
+
+train/approx_kl �/5�
+��+       ��K	a	��c��A��*
+
+train/clip_fraction     ��)(       �pJ	a	��c��A��*
+
+train/clip_range��L>���*       ����	a	��c��A��*
+
+train/entropy_lossD����FB0       ���_	a	��c��A��*!
+
+train/explained_variance    Jmё+       ��K	a	��c��A��*
+
+train/learning_rateRI�9��ݥ"       x=�	a	��c��A��*
+
+
+train/loss{��K�5�.2       $V�	a	��c��A��*#
+!
+train/policy_gradient_loss���7d�v(       �pJ	a	��c��A��*
+
+train/value_loss
+�TL�J��        )�P	2���c��A��*
+
+time/fps  �D���'       ��F	2���c��A��*
+
+train/approx_kl�9T6�+0+       ��K	2���c��A��*
+
+train/clip_fraction    �B(       �pJ	2���c��A��*
+
+train/clip_range��L>7� V*       ����	2���c��A��*
+
+train/entropy_loss���]��0       ���_	2���c��A��*!
+
+train/explained_variance  �3!�9�+       ��K	2���c��A��*
+
+train/learning_rateRI�9#߇�"       x=�	2���c��A��*
+
+
+train/loss���K���2       $V�	2���c��A��*#
+!
+train/policy_gradient_loss��72�(       �pJ	2���c��A��*
+
+train/value_loss}ZOLv�        )�P	o��c��A��*
+
+time/fps  �D~NQ�'       ��F	o��c��A��*
+
+train/approx_klM��6���+       ��K	o��c��A��*
+
+train/clip_fraction    ��u(       �pJ	o��c��A��*
+
+train/clip_range��L>� S�*       ����	o��c��A��*
+
+train/entropy_lossZ��30�0       ���_	o��c��A��*!
+
+train/explained_variance   4"���+       ��K	o��c��A��*
+
+train/learning_rateRI�9� S�"       x=�	o��c��A��*
+
+
+train/loss�+�Ku��2       $V�	o��c��A��*#
+!
+train/policy_gradient_loss���)�(       �pJ	o��c��A��*
+
+train/value_loss�}ML�e��        )�P	fV7�c��A��*
+
+time/fps  �Dbp7'       ��F	fV7�c��A��*
+
+train/approx_klf�6�~b�+       ��K	fV7�c��A��*
+
+train/clip_fraction    ּB(       �pJ	fV7�c��A��*
+
+train/clip_range��L>�=�*       ����	fV7�c��A��*
+
+train/entropy_loss{��6s0       ���_	fV7�c��A��*!
+
+train/explained_variance  �3*��V+       ��K	fV7�c��A��*
+
+train/learning_rateRI�9�#+�"       x=�	fV7�c��A��*
+
+
+train/loss�?�K����2       $V�	fV7�c��A��*#
+!
+train/policy_gradient_lossכ?��@�(       �pJ	fV7�c��A��*
+
+train/value_loss��DL�        )�P	U`�c��A��*
+
+time/fps  �D��<@'       ��F	U`�c��A��*
+
+train/approx_kl)�6:�y�+       ��K	U`�c��A��*
+
+train/clip_fraction    �P��(       �pJ	U`�c��A��*
+
+train/clip_range��L>N��*       ����	U`�c��A��*
+
+train/entropy_loss���#��0       ���_	U`�c��A��*!
+
+train/explained_variance  �3��y+       ��K	U`�c��A��*
+
+train/learning_rateRI�9�w�z"       x=�	U`�c��A��*
+
+
+train/loss�{�KP�R 2       $V�	U`�c��A��*#
+!
+train/policy_gradient_loss�䏸�Ѧ#(       �pJ	U`�c��A��*
+
+train/value_loss��GLe@��        )�P	���c��A��*
+
+time/fps  �D�h�w'       ��F	���c��A��*
+
+train/approx_kl͑7��L+       ��K	���c��A��*
+
+train/clip_fraction    9�ȭ(       �pJ	���c��A��*
+
+train/clip_range��L>[A6�*       ����	���c��A��*
+
+train/entropy_loss����j��0       ���_	���c��A��*!
+
+train/explained_variance    ����+       ��K	���c��A��*
+
+train/learning_rateRI�9DG�1"       x=�	���c��A��*
+
+
+train/loss��K2u�2       $V�	���c��A��*#
+!
+train/policy_gradient_loss�θ��&(       �pJ	���c��A��*
+
+train/value_loss��BL��	        )�P	�ܰ�c��A��*
+
+time/fps @�D���Y'       ��F	�ܰ�c��A��*
+
+train/approx_kls#27L��D+       ��K	�ܰ�c��A��*
+
+train/clip_fraction    6��(       �pJ	�ܰ�c��A��*
+
+train/clip_range��L>{��*       ����	�ܰ�c��A��*
+
+train/entropy_loss�$��o��0       ���_	�ܰ�c��A��*!
+
+train/explained_variance   4���+       ��K	�ܰ�c��A��*
+
+train/learning_rateRI�9+4Δ"       x=�	}��c��A��*
+
+
+train/loss~�K�r�o2       $V�	}��c��A��*#
+!
+train/policy_gradient_loss�ظ�}�(       �pJ	}��c��A��*
+
+train/value_loss��>L�W�        )�P	\��c��A��*
+
+time/fps @�D��o)'       ��F	\��c��A��*
+
+train/approx_kl&s�7Ա�6+       ��K	\��c��A��*
+
+train/clip_fraction    �ѥi(       �pJ	\��c��A��*
+
+train/clip_range��L>w/�*       ����	\��c��A��*
+
+train/entropy_loss`6��b�0       ���_	\��c��A��*!
+
+train/explained_variance    H��4+       ��K	\��c��A��*
+
+train/learning_rateRI�9����"       x=�	\��c��A��*
+
+
+train/lossf,�Kc���2       $V�	\��c��A��*#
+!
+train/policy_gradient_loss�����(       �pJ	\��c��A��*
+
+train/value_loss�AL�"�T        )�P	p� �c��A��*
+
+time/fps `�D����'       ��F	p� �c��A��*
+
+train/approx_kl���6�x޳+       ��K	p� �c��A��*
+
+train/clip_fraction    (���(       �pJ	p� �c��A��*
+
+train/clip_range��L>aPs*       ����	p� �c��A��*
+
+train/entropy_loss�G����.0       ���_	p� �c��A��*!
+
+train/explained_variance    �X0�+       ��K	p� �c��A��*
+
+train/learning_rateRI�959�?"       x=�	p� �c��A��*
+
+
+train/loss���Kp��I2       $V�	p� �c��A��*#
+!
+train/policy_gradient_loss�_��֤��(       �pJ	p� �c��A��*
+
+train/value_lossq^QL�9�=        )�P	�G*�c��A��*
+
+time/fps @�DٶHe'       ��F	�G*�c��A��*
+
+train/approx_kl��
+7���V+       ��K	�G*�c��A��*
+
+train/clip_fraction    ~�,�(       �pJ	�G*�c��A��*
+
+train/clip_range��L>5�/N*       ����	�G*�c��A��*
+
+train/entropy_loss�L���aM�0       ���_	�G*�c��A��*!
+
+train/explained_variance  �4���+       ��K	�G*�c��A��*
+
+train/learning_rateRI�9�:"       x=�	�G*�c��A��*
+
+
+train/loss���K<e=2       $V�	�G*�c��A��*#
+!
+train/policy_gradient_lossn���-�(       �pJ	�G*�c��A��*
+
+train/value_lossC+xL��0�        )�P	��T�c��A��*
+
+time/fps  �DV��'       ��F	��T�c��A��*
+
+train/approx_kl ��6U폈+       ��K	��T�c��A��*
+
+train/clip_fraction    �ӧ�(       �pJ	��T�c��A��*
+
+train/clip_range��L>/}*       ����	��T�c��A��*
+
+train/entropy_lossLN��v��50       ���_	��T�c��A��*!
+
+train/explained_variance   �"M�7+       ��K	��T�c��A��*
+
+train/learning_rateRI�9��!"       x=�	��T�c��A��*
+
+
+train/loss��LY�qb2       $V�	��T�c��A��*#
+!
+train/policy_gradient_loss`XԸ%C(       �pJ	��T�c��A��*
+
+train/value_lossWR�L���P        )�P	��{�c��A��*
+
+time/fps @�Dvu�U'       ��F	��{�c��A��*
+
+train/approx_kl *r6-٪�+       ��K	��{�c��A��*
+
+train/clip_fraction    ����(       �pJ	��{�c��A��*
+
+train/clip_range��L>�8�~*       ����	��{�c��A��*
+
+train/entropy_loss\W��0d]0       ���_	��{�c��A��*!
+
+train/explained_variance   �+j+       ��K	��{�c��A��*
+
+train/learning_rateRI�9���"       x=�	��{�c��A��*
+
+
+train/loss��L��2       $V�	��{�c��A��*#
+!
+train/policy_gradient_loss����׷`(       �pJ	��{�c��A��*
+
+train/value_loss�ʄL�pt        )�P	�ܤ�c��A��*
+
+time/fps @�D�;�'       ��F	�ܤ�c��A��*
+
+train/approx_kl3�j3{I�+       ��K	�ܤ�c��A��*
+
+train/clip_fraction    ���(       �pJ	�ܤ�c��A��*
+
+train/clip_range��L>����*       ����	�ܤ�c��A��*
+
+train/entropy_loss�Z��t͡0       ���_	�ܤ�c��A��*!
+
+train/explained_variance    ��1.+       ��K	�ܤ�c��A��*
+
+train/learning_rateRI�9ѱ5"       x=�	�ܤ�c��A��*
+
+
+train/lossb�K.s��2       $V�	�ܤ�c��A��*#
+!
+train/policy_gradient_loss��6��8�(       �pJ	�ܤ�c��A��*
+
+train/value_lossRlsL��        )�P	P���c��AИ*
+
+time/fps @�Dә�'       ��F	P���c��AИ*
+
+train/approx_kl3�7�U<<+       ��K	P���c��AИ*
+
+train/clip_fraction    ����(       �pJ	P���c��AИ*
+
+train/clip_range��L>��*       ����	P���c��AИ*
+
+train/entropy_lossTW���?�.0       ���_	P���c��AИ*!
+
+train/explained_variance   4�#Vk+       ��K	P���c��AИ*
+
+train/learning_rateRI�9�͍"       x=�	P���c��AИ*
+
+
+train/loss���KHwc�2       $V�	P���c��AИ*#
+!
+train/policy_gradient_loss0x�Ĉϰ(       �pJ	P���c��AИ*
+
+train/value_lossd�rL?���        )�P	8���c��A��*
+
+time/fps `�DX���'       ��F	8���c��A��*
+
+train/approx_klf��66sP�+       ��K	8���c��A��*
+
+train/clip_fraction    ��1s(       �pJ	8���c��A��*
+
+train/clip_range��L>>�x�*       ����	8���c��A��*
+
+train/entropy_loss�V���Q��0       ���_	8���c��A��*!
+
+train/explained_variance   ��IQ�+       ��K	8���c��A��*
+
+train/learning_rateRI�9���"       x=�	8���c��A��*
+
+
+train/loss��Kl{I�2       $V�	8���c��A��*#
+!
+train/policy_gradient_loss!���2���(       �pJ	8���c��A��*
+
+train/value_loss��gLq&��        )�P	�3�c��A�*
+
+time/fps ��DQ�c'       ��F	�3�c��A�*
+
+train/approx_klʹ�4���+       ��K	�3�c��A�*
+
+train/clip_fraction    �	��(       �pJ	�3�c��A�*
+
+train/clip_range��L>7�kz*       ����	�3�c��A�*
+
+train/entropy_loss�[��J$0       ���_	�3�c��A�*!
+
+train/explained_variance   ���>�+       ��K	�3�c��A�*
+
+train/learning_rateRI�9��W"       x=�	�3�c��A�*
+
+
+train/loss��K1f�2       $V�	�3�c��A�*#
+!
+train/policy_gradient_loss�7Ɠ(       �pJ	�3�c��A�*
+
+train/value_loss2�oLɣ�        )�P	.�B�c��A��*
+
+time/fps ��D=��
+'       ��F	.�B�c��A��*
+
+train/approx_kl��D7�kR+       ��K	.�B�c��A��*
+
+train/clip_fraction    �	�(       �pJ	.�B�c��A��*
+
+train/clip_range��L>��L*       ����	.�B�c��A��*
+
+train/entropy_loss�X�����r0       ���_	.�B�c��A��*!
+
+train/explained_variance   ��K,+       ��K	.�B�c��A��*
+
+train/learning_rateRI�9��,/"       x=�	.�B�c��A��*
+
+
+train/loss�/�K�OW2       $V�	.�B�c��A��*#
+!
+train/policy_gradient_lossy��b��(       �pJ	.�B�c��A��*
+
+train/value_loss�vL��|�        )�P	�2l�c��A��*
+
+time/fps ��D���^'       ��F	�2l�c��A��*
+
+train/approx_kl���6�>�+       ��K	�2l�c��A��*
+
+train/clip_fraction    ܄��(       �pJ	�2l�c��A��*
+
+train/clip_range��L>�Y=*       ����	�2l�c��A��*
+
+train/entropy_lossFR���Ǩ�0       ���_	�2l�c��A��*!
+
+train/explained_variance    ~	'G+       ��K	�2l�c��A��*
+
+train/learning_rateRI�9�]�	"       x=�	�2l�c��A��*
+
+
+train/loss�)�K0%�(2       $V�	�2l�c��A��*#
+!
+train/policy_gradient_loss�ᖸ���(       �pJ	�2l�c��A��*
+
+train/value_lossM�ALgj��        )�P	"���c��A�*
+
+time/fps `�D�o��'       ��F	"���c��A�*
+
+train/approx_kl 4f6I�]�+       ��K	"���c��A�*
+
+train/clip_fraction    	��H(       �pJ	"���c��A�*
+
+train/clip_range��L>I�"�*       ����	"���c��A�*
+
+train/entropy_loss�S������0       ���_	"���c��A�*!
+
+train/explained_variance  ������+       ��K	"���c��A�*
+
+train/learning_rateRI�9<��"       x=�	"���c��A�*
+
+
+train/loss��L� l2       $V�	"���c��A�*#
+!
+train/policy_gradient_loss����~��$(       �pJ	"���c��A�*
+
+train/value_lossG9�L��u        )�P	�
+��c��A��*
+
+time/fps ��Db��'       ��F	�
+��c��A��*
+
+train/approx_kl�>�6Z�W�+       ��K	�
+��c��A��*
+
+train/clip_fraction    ���(       �pJ	�
+��c��A��*
+
+train/clip_range��L>W��*       ����	�
+��c��A��*
+
+train/entropy_lossP�����0       ���_	�
+��c��A��*!
+
+train/explained_variance  �31�|+       ��K	�
+��c��A��*
+
+train/learning_rateRI�9�ޚ,"       x=�	�
+��c��A��*
+
+
+train/loss5�L��P2       $V�	�
+��c��A��*#
+!
+train/policy_gradient_loss~���\.y(       �pJ	�
+��c��A��*
+
+train/value_loss[��L�)П        )�P	�3��c��A��*
+
+time/fps ��D���y'       ��F	�3��c��A��*
+
+train/approx_klf�%6�y1[+       ��K	�3��c��A��*
+
+train/clip_fraction    ��K(       �pJ	�3��c��A��*
+
+train/clip_range��L>�<�*       ����	�3��c��A��*
+
+train/entropy_loss�O��=h�0       ���_	�3��c��A��*!
+
+train/explained_variance  �4�}�h+       ��K	�3��c��A��*
+
+train/learning_rateRI�9Է��"       x=�	�3��c��A��*
+
+
+train/loss�`L�
+��2       $V�	�3��c��A��*#
+!
+train/policy_gradient_loss��]��:AK(       �pJ	�3��c��A��*
+
+train/value_loss"n�L��E�        )�P	�d�c��A��*
+
+time/fps `�D�H
+�'       ��F	�d�c��A��*
+
+train/approx_klM47���+       ��K	�d�c��A��*
+
+train/clip_fraction    Ԕ2�(       �pJ	�d�c��A��*
+
+train/clip_range��L>V���*       ����	�d�c��A��*
+
+train/entropy_loss,O��E,9�0       ���_	�d�c��A��*!
+
+train/explained_variance  @4<(��+       ��K	�d�c��A��*
+
+train/learning_rateRI�9�+v"       x=�	�d�c��A��*
+
+
+train/loss(	L��?�2       $V�	�d�c��A��*#
+!
+train/policy_gradient_loss("�q���(       �pJ	�d�c��A��*
+
+train/value_loss!��L���        )�P	�@:�c��A��*
+
+time/fps `�D�E�'       ��F	�@:�c��A��*
+
+train/approx_klf��59 ��+       ��K	�@:�c��A��*
+
+train/clip_fraction    L��6(       �pJ	�@:�c��A��*
+
+train/clip_range��L>=�o*       ����	�@:�c��A��*
+
+train/entropy_loss.N��b/>V0       ���_	�@:�c��A��*!
+
+train/explained_variance  ��(�k!+       ��K	�@:�c��A��*
+
+train/learning_rateRI�9QA��"       x=�	�@:�c��A��*
+
+
+train/loss��L����2       $V�	�@:�c��A��*#
+!
+train/policy_gradient_loss���Ӑ�(       �pJ	�@:�c��A��*
+
+train/value_loss�k�LCZ        )�P	#�d�c��A��*
+
+time/fps @�Dd�'       ��F	#�d�c��A��*
+
+train/approx_kl3iP6�"3"+       ��K	#�d�c��A��*
+
+train/clip_fraction    7�\
+(       �pJ	#�d�c��A��*
+
+train/clip_range��L>yԼ*       ����	#�d�c��A��*
+
+train/entropy_loss�S��� �0       ���_	#�d�c��A��*!
+
+train/explained_variance   �RF@�+       ��K	#�d�c��A��*
+
+train/learning_rateRI�9*[x�"       x=�	#�d�c��A��*
+
+
+train/loss�N$L�A�M2       $V�	#�d�c��A��*#
+!
+train/policy_gradient_loss?"Z����(       �pJ	#�d�c��A��*
+
+train/value_lossK��L��2�        )�P	�$��c��A��*
+
+time/fps @�Da/u�'       ��F	�,��c��A��*
+
+train/approx_kl�s�6@	��+       ��K	�,��c��A��*
+
+train/clip_fraction    n���(       �pJ	�,��c��A��*
+
+train/clip_range��L>9�@�*       ����	�,��c��A��*
+
+train/entropy_lossR��z��t0       ���_	�,��c��A��*!
+
+train/explained_variance    ���(+       ��K	�,��c��A��*
+
+train/learning_rateRI�9s
+�<"       x=�	�,��c��A��*
+
+
+train/loss��LalEa2       $V�	�,��c��A��*#
+!
+train/policy_gradient_loss;��[��2(       �pJ	�,��c��A��*
+
+train/value_lossO�L�-
+E        )�P	l���c��A��*
+
+time/fps @�DU�.'       ��F	l���c��A��*
+
+train/approx_kl���5�E�?+       ��K	l���c��A��*
+
+train/clip_fraction    ���(       �pJ	l���c��A��*
+
+train/clip_range��L>��**       ����	l���c��A��*
+
+train/entropy_loss�T��|��0       ���_	l���c��A��*!
+
+train/explained_variance  �5!�v�+       ��K	l���c��A��*
+
+train/learning_rateRI�9���;"       x=�	l���c��A��*
+
+
+train/loss�vL�;y�2       $V�	l���c��A��*#
+!
+train/policy_gradient_loss~E�Ş�#(       �pJ	l���c��A��*
+
+train/value_lossgm�L�X�        )�P	D���c��A��*
+
+time/fps @�D�q{'       ��F	D���c��A��*
+
+train/approx_klMm�6H. V+       ��K	D���c��A��*
+
+train/clip_fraction    -��(       �pJ	D���c��A��*
+
+train/clip_range��L> ��*       ����	D���c��A��*
+
+train/entropy_loss�Z��ķ�]0       ���_	D���c��A��*!
+
+train/explained_variance   �:]=+       ��K	D���c��A��*
+
+train/learning_rateRI�90'��"       x=�	D���c��A��*
+
+
+train/loss��Lg���2       $V�	D���c��A��*#
+!
+train/policy_gradient_lossH�X����(       �pJ	D���c��A��*
+
+train/value_loss�ŝL�^T        )�P	jn
+�c��A��*
+
+time/fps  �DC$��'       ��F	jn
+�c��A��*
+
+train/approx_kl���6oO£+       ��K	jn
+�c��A��*
+
+train/clip_fraction    �pG(       �pJ	jn
+�c��A��*
+
+train/clip_range��L>��C�*       ����	jn
+�c��A��*
+
+train/entropy_loss�]���RUR0       ���_	jn
+�c��A��*!
+
+train/explained_variance   ��&K+       ��K	jn
+�c��A��*
+
+train/learning_rateRI�9�Wk�"       x=�	jn
+�c��A��*
+
+
+train/lossåLVb�[2       $V�	jn
+�c��A��*#
+!
+train/policy_gradient_loss(K����]O(       �pJ	jn
+�c��A��*
+
+train/value_loss%,�L]*7        )�P	o�4�c��A��*
+
+time/fps  �Dt�~�'       ��F	o�4�c��A��*
+
+train/approx_kl͙�6��sC+       ��K	o�4�c��A��*
+
+train/clip_fraction    ����(       �pJ	o�4�c��A��*
+
+train/clip_range��L>�(7R*       ����	o�4�c��A��*
+
+train/entropy_loss�i��C�M0       ���_	o�4�c��A��*!
+
+train/explained_variance   �>��+       ��K	o�4�c��A��*
+
+train/learning_rateRI�9�%�6"       x=�	o�4�c��A��*
+
+
+train/loss>"Lw��2       $V�	o�4�c��A��*#
+!
+train/policy_gradient_loss����>2�r(       �pJ	o�4�c��A��*
+
+train/value_loss�f�L�ʅ        )�P	�]�c��AЂ*
+
+time/fps  �D����'       ��F	�]�c��AЂ*
+
+train/approx_klfX16���+       ��K	�]�c��AЂ*
+
+train/clip_fraction    ]Ǫ�(       �pJ	�]�c��AЂ*
+
+train/clip_range��L>�*�*       ����	�]�c��AЂ*
+
+train/entropy_loss�p������0       ���_	�]�c��AЂ*!
+
+train/explained_variance  ����;�+       ��K	�]�c��AЂ*
+
+train/learning_rateRI�9` �7"       x=�	�]�c��AЂ*
+
+
+train/loss�'L���2       $V�	�]�c��AЂ*#
+!
+train/policy_gradient_lossB�U��P$(       �pJ	�]�c��AЂ*
+
+train/value_lossD��LS��        )�P	Z��c��A��*
+
+time/fps @�DG��'       ��F	Z��c��A��*
+
+train/approx_kl ;,6Fo�
++       ��K	Z��c��A��*
+
+train/clip_fraction    ��(       �pJ	Z��c��A��*
+
+train/clip_range��L>JE�*       ����	Z��c��A��*
+
+train/entropy_loss�o����{0       ���_	Z��c��A��*!
+
+train/explained_variance  @4Kǳ+       ��K	Z��c��A��*
+
+train/learning_rateRI�9�Ԗ"       x=�	Z��c��A��*
+
+
+train/loss�u(LK��2       $V�	Z��c��A��*#
+!
+train/policy_gradient_loss�=��2[6(       �pJ	Z��c��A��*
+
+train/value_lossBΧL�K�
+        )�P	尭�c��A��*
+
+time/fps  �D�!S'       ��F	尭�c��A��*
+
+train/approx_kl37Q6��5�+       ��K	尭�c��A��*
+
+train/clip_fraction    f���(       �pJ	尭�c��A��*
+
+train/clip_range��L>�6�*       ����	尭�c��A��*
+
+train/entropy_loss�m��E
+h�0       ���_	尭�c��A��*!
+
+train/explained_variance    �7T+       ��K	尭�c��A��*
+
+train/learning_rateRI�9[�b"       x=�	尭�c��A��*
+
+
+train/loss{M)L�"�Q2       $V�	尭�c��A��*#
+!
+train/policy_gradient_loss[-����(       �pJ	尭�c��A��*
+
+train/value_lossx|�L��|�        )�P	+?��c��A��*
+
+time/fps @�D^��%'       ��F	+?��c��A��*
+
+train/approx_kl�p5b�Xv+       ��K	+?��c��A��*
+
+train/clip_fraction    ��2(       �pJ	+?��c��A��*
+
+train/clip_range��L>� W~*       ����	+?��c��A��*
+
+train/entropy_loss�w����#�0       ���_	+?��c��A��*!
+
+train/explained_variance   ���+       ��K	+?��c��A��*
+
+train/learning_rateRI�9&���"       x=�	+?��c��A��*
+
+
+train/loss�e'L��2       $V�	+?��c��A��*#
+!
+train/policy_gradient_lossb�]�sb�(       �pJ	+?��c��A��*
+
+train/value_loss���L�0�        )�P	�`��c��A��*
+
+time/fps @�D�Cik'       ��F	�`��c��A��*
+
+train/approx_kl�27$��+       ��K	�`��c��A��*
+
+train/clip_fraction    �ci(       �pJ	�`��c��A��*
+
+train/clip_range��L>r�_*       ����	�`��c��A��*
+
+train/entropy_loss�q������0       ���_	�`��c��A��*!
+
+train/explained_variance     ���+       ��K	�`��c��A��*
+
+train/learning_rateRI�9��N�"       x=�	�`��c��A��*
+
+
+train/loss��&L�"R2       $V�	�`��c��A��*#
+!
+train/policy_gradient_loss�G�����(       �pJ	�`��c��A��*
+
+train/value_loss���LQwB�        )�P	+'�c��A�*
+
+time/fps @�D{�A�'       ��F	+'�c��A�*
+
+train/approx_kl͢L7��N�+       ��K	+'�c��A�*
+
+train/clip_fraction    i3R(       �pJ	+'�c��A�*
+
+train/clip_range��L>ž�*       ����	+'�c��A�*
+
+train/entropy_lossx���cH0       ���_	+'�c��A�*!
+
+train/explained_variance  �����+       ��K	+'�c��A�*
+
+train/learning_rateRI�9
+��"       x=�	+'�c��A�*
+
+
+train/loss 	L
+BA{2       $V�	+'�c��A�*#
+!
+train/policy_gradient_loss��K���>8(       �pJ	+'�c��A�*
+
+train/value_lossUB�L��        )�P	0iQ�c��A��*
+
+time/fps  �D��r''       ��F	0iQ�c��A��*
+
+train/approx_klf�6�~�M+       ��K	0iQ�c��A��*
+
+train/clip_fraction    ��(       �pJ	0iQ�c��A��*
+
+train/clip_range��L>Q�W�*       ����	0iQ�c��A��*
+
+train/entropy_lossq����j�0       ���_	0iQ�c��A��*!
+
+train/explained_variance  �4�w�+       ��K	0iQ�c��A��*
+
+train/learning_rateRI�9�3=t"       x=�	0iQ�c��A��*
+
+
+train/lossަ#Lac�2       $V�	0iQ�c��A��*#
+!
+train/policy_gradient_lossX�1U��(       �pJ	0iQ�c��A��*
+
+train/value_loss�/�L�˵�        )�P	��{�c��A��*
+
+time/fps  �D,�۾'       ��F	��{�c��A��*
+
+train/approx_kl��'5�)a;+       ��K	��{�c��A��*
+
+train/clip_fraction    A�T(       �pJ	��{�c��A��*
+
+train/clip_range��L>��`�*       ����	��{�c��A��*
+
+train/entropy_loss{o���^0       ���_	��{�c��A��*!
+
+train/explained_variance  @4�"�+       ��K	��{�c��A��*
+
+train/learning_rateRI�9<�A�"       x=�	��{�c��A��*
+
+
+train/lossGC!L-�2       $V�	��{�c��A��*#
+!
+train/policy_gradient_loss�f趖RM�(       �pJ	��{�c��A��*
+
+train/value_lossВ�L���i        )�P	�W��c��Aз*
+
+time/fps  �D�˅8'       ��F	�W��c��Aз*
+
+train/approx_kl3��6�g�a+       ��K	�W��c��Aз*
+
+train/clip_fraction    �N��(       �pJ	�W��c��Aз*
+
+train/clip_range��L>��B�*       ����	�W��c��Aз*
+
+train/entropy_loss(o����0       ���_	�W��c��Aз*!
+
+train/explained_variance  �4�Jh9+       ��K	�W��c��Aз*
+
+train/learning_rateRI�9�w�/"       x=�	�W��c��Aз*
+
+
+train/loss>�,Lhd��2       $V�	�W��c��Aз*#
+!
+train/policy_gradient_loss�����,��(       �pJ	�W��c��Aз*
+
+train/value_loss��Ls�[        )�P	��c��A��*
+
+time/fps  �D�RT'       ��F	��c��A��*
+
+train/approx_kl��U6�:�+       ��K	��c��A��*
+
+train/clip_fraction    ���(       �pJ	��c��A��*
+
+train/clip_range��L>.�,e*       ����	��c��A��*
+
+train/entropy_loss�f��ua|c0       ���_	��c��A��*!
+
+train/explained_variance  �4N���+       ��K	��c��A��*
+
+train/learning_rateRI�9��"       x=�	��c��A��*
+
+
+train/loss� .L���52       $V�	��c��A��*#
+!
+train/policy_gradient_loss��V�X��(       �pJ	��c��A��*
+
+train/value_lossʾ�L��!�        )�P	���c��A��*
+
+time/fps  �D%�v'       ��F	���c��A��*
+
+train/approx_kl�67U +       ��K	���c��A��*
+
+train/clip_fraction    �v'(       �pJ	���c��A��*
+
+train/clip_range��L>üN�*       ����	���c��A��*
+
+train/entropy_lossxf���\˔0       ���_	���c��A��*!
+
+train/explained_variance    ���+       ��K	���c��A��*
+
+train/learning_rateRI�9�<b�"       x=�	���c��A��*
+
+
+train/loss��+L�t�2       $V�	���c��A��*#
+!
+train/policy_gradient_loss��(�Ҷ.�(       �pJ	���c��A��*
+
+train/value_lossª�L��5        )�P	�p"�c��A��*
+
+time/fps  �DӬ�t'       ��F	�p"�c��A��*
+
+train/approx_kl ~�5D+       ��K	�p"�c��A��*
+
+train/clip_fraction    h0��(       �pJ	�p"�c��A��*
+
+train/clip_range��L>�Zc*       ����	�p"�c��A��*
+
+train/entropy_loss@m��-|��0       ���_	�p"�c��A��*!
+
+train/explained_variance    dI�+       ��K	�p"�c��A��*
+
+train/learning_rateRI�9u��v"       x=�	�p"�c��A��*
+
+
+train/loss�r-LX	��2       $V�	�p"�c��A��*#
+!
+train/policy_gradient_loss�6!�f`�R(       �pJ	�p"�c��A��*
+
+train/value_loss�L���        )�P	�/L�c��A��*
+
+time/fps  �DC�%�'       ��F	�/L�c��A��*
+
+train/approx_kl�\5'�r+       ��K	�/L�c��A��*
+
+train/clip_fraction    �A��(       �pJ	�/L�c��A��*
+
+train/clip_range��L>ш�5*       ����	�/L�c��A��*
+
+train/entropy_loss@o����T�0       ���_	�/L�c��A��*!
+
+train/explained_variance  �3����+       ��K	�/L�c��A��*
+
+train/learning_rateRI�9� ]"       x=�	�/L�c��A��*
+
+
+train/loss�`&L��"p2       $V�	�/L�c��A��*#
+!
+train/policy_gradient_lossK����&�(       �pJ	�/L�c��A��*
+
+train/value_loss뱩L�
+A        )�P	,ou�c��A��*
+
+time/fps �D�Cm'       ��F	,ou�c��A��*
+
+train/approx_klfDk6�*��+       ��K	,ou�c��A��*
+
+train/clip_fraction    ��ʿ(       �pJ	,ou�c��A��*
+
+train/clip_range��L>�;j�*       ����	,ou�c��A��*
+
+train/entropy_losso��Js<{0       ���_	,ou�c��A��*!
+
+train/explained_variance   ���e!+       ��K	,ou�c��A��*
+
+train/learning_rateRI�9�	�;"       x=�	,ou�c��A��*
+
+
+train/loss]�'L>+�32       $V�	,ou�c��A��*#
+!
+train/policy_gradient_lossF4p�
+a�(       �pJ	,ou�c��A��*
+
+train/value_loss6�L�j\^        )�P	ľ��c��A��*
+
+time/fps �D-+Î'       ��F	ľ��c��A��*
+
+train/approx_kl�\T7p��+       ��K	ľ��c��A��*
+
+train/clip_fraction    ���(       �pJ	ľ��c��A��*
+
+train/clip_range��L>�4�S*       ����	ľ��c��A��*
+
+train/entropy_lossgt��@�W\0       ���_	ľ��c��A��*!
+
+train/explained_variance  �4��+       ��K	ľ��c��A��*
+
+train/learning_rateRI�9�ϩ "       x=�	ľ��c��A��*
+
+
+train/loss�:"LgL��2       $V�	ľ��c��A��*#
+!
+train/policy_gradient_lossH�O�J;�(       �pJ	ľ��c��A��*
+
+train/value_loss؊�Ln���        )�P	=U��c��A��*
+
+time/fps �D��'       ��F	=U��c��A��*
+
+train/approx_kl T/68�R�+       ��K	=U��c��A��*
+
+train/clip_fraction    �b��(       �pJ	=U��c��A��*
+
+train/clip_range��L>�+@*       ����	=U��c��A��*
+
+train/entropy_loss�z��U�|�0       ���_	=U��c��A��*!
+
+train/explained_variance  �3?�y+       ��K	=U��c��A��*
+
+train/learning_rateRI�9���m"       x=�	=U��c��A��*
+
+
+train/loss��%L3� 2       $V�	=U��c��A��*#
+!
+train/policy_gradient_lossZ���"���(       �pJ	=U��c��A��*
+
+train/value_loss��L�nZ        )�P	j#��c��A��*
+
+time/fps �D�ӕ'       ��F	j#��c��A��*
+
+train/approx_kl���4֟��+       ��K	j#��c��A��*
+
+train/clip_fraction    ���(       �pJ	j#��c��A��*
+
+train/clip_range��L>����*       ����	j#��c��A��*
+
+train/entropy_lossOs��*
+�C0       ���_	j#��c��A��*!
+
+train/explained_variance  ��M�AH+       ��K	j#��c��A��*
+
+train/learning_rateRI�9�35"       x=�	j#��c��A��*
+
+
+train/loss�t L���2       $V�	j#��c��A��*#
+!
+train/policy_gradient_loss�Ϸ2b�H(       �pJ	j#��c��A��*
+
+train/value_lossh��L��(�        )�P	�!�c��A��*
+
+time/fps �D�riw'       ��F	�!�c��A��*
+
+train/approx_kl47���+       ��K	�!�c��A��*
+
+train/clip_fraction    ��(       �pJ	�!�c��A��*
+
+train/clip_range��L>�,*       ����	�!�c��A��*
+
+train/entropy_losslq��>��y0       ���_	�!�c��A��*!
+
+train/explained_variance  ��o�%+       ��K	�!�c��A��*
+
+train/learning_rateRI�9��8&"       x=�	�!�c��A��*
+
+
+train/lossB�"Lg/[52       $V�	�!�c��A��*#
+!
+train/policy_gradient_loss�~�{<�(       �pJ	�!�c��A��*
+
+train/value_loss+Q�L+Y��        )�P	}cA�c��A��*
+
+time/fps  �D U��'       ��F	}cA�c��A��*
+
+train/approx_kl3.�6�c+       ��K	}cA�c��A��*
+
+train/clip_fraction    �p�d(       �pJ	}cA�c��A��*
+
+train/clip_range��L>Fb��*       ����	}cA�c��A��*
+
+train/entropy_lossRk��r�30       ���_	}cA�c��A��*!
+
+train/explained_variance  ���Uv+       ��K	}cA�c��A��*
+
+train/learning_rateRI�9�W��"       x=�	}cA�c��A��*
+
+
+train/loss��&L;�ru2       $V�	}cA�c��A��*#
+!
+train/policy_gradient_loss�Ō�>d7(       �pJ	}cA�c��A��*
+
+train/value_loss�_�Lڬ�        )�P	�Kk�c��A��*
+
+time/fps �D���
+'       ��F	�Kk�c��A��*
+
+train/approx_kl ��6S���+       ��K	�Kk�c��A��*
+
+train/clip_fraction    N�L?(       �pJ	�Kk�c��A��*
+
+train/clip_range��L>��M�*       ����	�Kk�c��A��*
+
+train/entropy_losse����<0       ���_	�Kk�c��A��*!
+
+train/explained_variance    ;�d�+       ��K	�Kk�c��A��*
+
+train/learning_rateRI�9S8h�"       x=�	�Kk�c��A��*
+
+
+train/loss2�&L��P�2       $V�	�Kk�c��A��*#
+!
+train/policy_gradient_lossw�p�
+���(       �pJ	�Kk�c��A��*
+
+train/value_loss�:�L�E�        )�P	�k��c��A��*
+
+time/fps  �D݈�'       ��F	�k��c��A��*
+
+train/approx_kl�u�6oA��+       ��K	�k��c��A��*
+
+train/clip_fraction    ��.`(       �pJ	�k��c��A��*
+
+train/clip_range��L>?�5*       ����	�k��c��A��*
+
+train/entropy_loss�b��8�0       ���_	�k��c��A��*!
+
+train/explained_variance   ��2<�+       ��K	�k��c��A��*
+
+train/learning_rateRI�9�
+�"       x=�	�k��c��A��*
+
+
+train/loss6�*L�^��2       $V�	�k��c��A��*#
+!
+train/policy_gradient_loss�ǌ���RS(       �pJ	�k��c��A��*
+
+train/value_lossiЯL~@�i        )�P	̺�c��A��*
+
+time/fps  �D���'       ��F	̺�c��A��*
+
+train/approx_kl͠Z6�E}+       ��K	̺�c��A��*
+
+train/clip_fraction    q�k�(       �pJ	̺�c��A��*
+
+train/clip_range��L>OL�**       ����	̺�c��A��*
+
+train/entropy_loss)^��n$��0       ���_	̺�c��A��*!
+
+train/explained_variance  �3u6;�+       ��K	̺�c��A��*
+
+train/learning_rateRI�9lG)"       x=�	̺�c��A��*
+
+
+train/loss�8.L:�K�2       $V�	̺�c��A��*#
+!
+train/policy_gradient_loss��k�����(       �pJ	̺�c��A��*
+
+train/value_loss!ٮL�`�n        )�P	Ơ��c��A��*
+
+time/fps  �D��'       ��F	Ơ��c��A��*
+
+train/approx_kl���7r/�x+       ��K	Ơ��c��A��*
+
+train/clip_fraction    XgD�(       �pJ	Ơ��c��A��*
+
+train/clip_range��L>xV�Q*       ����	Ơ��c��A��*
+
+train/entropy_loss$\��0       ���_	Ơ��c��A��*!
+
+train/explained_variance    �T)�+       ��K	Ơ��c��A��*
+
+train/learning_rateRI�9e�p/"       x=�	Ơ��c��A��*
+
+
+train/loss�*L�~�2       $V�	Ơ��c��A��*#
+!
+train/policy_gradient_loss�}Y��O��(       �pJ	Ơ��c��A��*
+
+train/value_loss%�L��#        )�P	S��c��A��*
+
+time/fps  �D��.'       ��F	S��c��A��*
+
+train/approx_kl�m6���/+       ��K	S��c��A��*
+
+train/clip_fraction    1��Y(       �pJ	S��c��A��*
+
+train/clip_range��L>�h��*       ����	S��c��A��*
+
+train/entropy_loss[Y���d^�0       ���_	S��c��A��*!
+
+train/explained_variance  @4���$+       ��K	S��c��A��*
+
+train/learning_rateRI�9,��"       x=�	S��c��A��*
+
+
+train/loss�x L�PC92       $V�	S��c��A��*#
+!
+train/policy_gradient_loss._=��x(       �pJ	S��c��A��*
+
+train/value_loss��Lc2��        )�P	�4�c��AС*
+
+time/fps  �D�z�'       ��F	�4�c��AС*
+
+train/approx_kl�j�5qt�+       ��K	�4�c��AС*
+
+train/clip_fraction    +r{�(       �pJ	�4�c��AС*
+
+train/clip_range��L>��*       ����	z�4�c��AС*
+
+train/entropy_loss�Z���RG0       ���_	z�4�c��AС*!
+
+train/explained_variance   ��Y�+       ��K	z�4�c��AС*
+
+train/learning_rateRI�9���"       x=�	z�4�c��AС*
+
+
+train/loss0�LOl?�2       $V�	z�4�c��AС*#
+!
+train/policy_gradient_loss:v
+�Ӆ�h(       �pJ	z�4�c��AС*
+
+train/value_loss�"�L�V�p        )�P	�"\�c��A��*
+
+time/fps  �D�m��'       ��F	�"\�c��A��*
+
+train/approx_kl L�5��[�+       ��K	�"\�c��A��*
+
+train/clip_fraction    ��;�(       �pJ	�"\�c��A��*
+
+train/clip_range��L>��ɸ*       ����	�"\�c��A��*
+
+train/entropy_loss�]���,0       ���_	�"\�c��A��*!
+
+train/explained_variance    �2��+       ��K	�"\�c��A��*
+
+train/learning_rateRI�9�p"       x=�	�"\�c��A��*
+
+
+train/loss�vL��-2       $V�	�"\�c��A��*#
+!
+train/policy_gradient_loss�#���m-W(       �pJ	�"\�c��A��*
+
+train/value_lossA�L��L�        )�P	AE��c��A�*
+
+time/fps  �D���'       ��F	AE��c��A�*
+
+train/approx_kl3�D6�g��+       ��K	AE��c��A�*
+
+train/clip_fraction    �;)(       �pJ	AE��c��A�*
+
+train/clip_range��L>��M�*       ����	AE��c��A�*
+
+train/entropy_lossw\��鼎�0       ���_	AE��c��A�*!
+
+train/explained_variance    B�|�+       ��K	AE��c��A�*
+
+train/learning_rateRI�9ϭ�"       x=�	AE��c��A�*
+
+
+train/loss
+4L�L��2       $V�	AE��c��A�*#
+!
+train/policy_gradient_loss����8%�(       �pJ	AE��c��A�*
+
+train/value_loss���L�ݒ�        )�P	�w��c��A��*
+
+time/fps  �D�dJ'       ��F	�w��c��A��*
+
+train/approx_klfv52�+U+       ��K	�w��c��A��*
+
+train/clip_fraction    Dx6�(       �pJ	�w��c��A��*
+
+train/clip_range��L>���H*       ����	�w��c��A��*
+
+train/entropy_losstV�����>0       ���_	�w��c��A��*!
+
+train/explained_variance    ��։+       ��K	�w��c��A��*
+
+train/learning_rateRI�99O"�"       x=�	�w��c��A��*
+
+
+train/lossm�Ln��2       $V�	�w��c��A��*#
+!
+train/policy_gradient_loss�+Z��V�{(       �pJ	�w��c��A��*
+
+train/value_loss+��L5�#}        )�P	A��c��A��*
+
+time/fps  �D�PE�'       ��F	A��c��A��*
+
+train/approx_kl�k�6G��+       ��K	A��c��A��*
+
+train/clip_fraction    �	��(       �pJ	A��c��A��*
+
+train/clip_range��L>���*       ����	A��c��A��*
+
+train/entropy_loss�\���/�V0       ���_	A��c��A��*!
+
+train/explained_variance  @4Jl�x+       ��K	A��c��A��*
+
+train/learning_rateRI�9��*"       x=�	A��c��A��*
+
+
+train/loss�� L���:2       $V�	A��c��A��*#
+!
+train/policy_gradient_loss	���n�ʆ(       �pJ	A��c��A��*
+
+train/value_loss� �LIX�|        )�P	�D��c��A��*
+
+time/fps  �D�K1j'       ��F	�D��c��A��*
+
+train/approx_kl&+m7�wA+       ��K	�D��c��A��*
+
+train/clip_fraction    �_(       �pJ	�D��c��A��*
+
+train/clip_range��L>$/�*       ����	�D��c��A��*
+
+train/entropy_lossa���I��0       ���_	�D��c��A��*!
+
+train/explained_variance    Z��+       ��K	�D��c��A��*
+
+train/learning_rateRI�9��*�"       x=�	�D��c��A��*
+
+
+train/loss��1LN>�2       $V�	�D��c��A��*#
+!
+train/policy_gradient_lossyA6���ns(       �pJ	�D��c��A��*
+
+train/value_lossE�L��7        )�P	�'�c��A��*
+
+time/fps  �Dp���'       ��F	�'�c��A��*
+
+train/approx_klf��5��K�+       ��K	�'�c��A��*
+
+train/clip_fraction    �R(       �pJ	�'�c��A��*
+
+train/clip_range��L>( �*       ����	�'�c��A��*
+
+train/entropy_loss�k����_$0       ���_	�'�c��A��*!
+
+train/explained_variance    ���+       ��K	�'�c��A��*
+
+train/learning_rateRI�9���"       x=�	�'�c��A��*
+
+
+train/loss�20L�f�2       $V�	�'�c��A��*#
+!
+train/policy_gradient_loss�O���c(       �pJ	�'�c��A��*
+
+train/value_loss��L�5�         )�P	�N�c��A��*
+
+time/fps  �D2��'       ��F	�N�c��A��*
+
+train/approx_kl�<7��ئ+       ��K	�N�c��A��*
+
+train/clip_fraction    oد(       �pJ	�N�c��A��*
+
+train/clip_range��L>��5�*       ����	�N�c��A��*
+
+train/entropy_loss�l��~O��0       ���_	�N�c��A��*!
+
+train/explained_variance  �3�1K+       ��K	�N�c��A��*
+
+train/learning_rateRI�9r"       x=�	�N�c��A��*
+
+
+train/lossA'8L`'�(2       $V�	�N�c��A��*#
+!
+train/policy_gradient_loss;�;�+tZ](       �pJ	�N�c��A��*
+
+train/value_loss�y�Li��        )�P	�Mw�c��A��*
+
+time/fps  �D�A�'       ��F	�Mw�c��A��*
+
+train/approx_kl3]F6:K�+       ��K	�Mw�c��A��*
+
+train/clip_fraction    -�#(       �pJ	�Mw�c��A��*
+
+train/clip_range��L>u߹,*       ����	�Mw�c��A��*
+
+train/entropy_losss��2�`]0       ���_	�Mw�c��A��*!
+
+train/explained_variance  �3j,|+       ��K	�Mw�c��A��*
+
+train/learning_rateRI�90�}"       x=�	�Mw�c��A��*
+
+
+train/loss��1L���r2       $V�	�Mw�c��A��*#
+!
+train/policy_gradient_loss�s9���0@(       �pJ	�Mw�c��A��*
+
+train/value_lossc�Lq!�/        )�P	�ء�c��A��*
+
+time/fps  �D_��6'       ��F	�ء�c��A��*
+
+train/approx_klf�5�#�+       ��K	�ء�c��A��*
+
+train/clip_fraction    4� �(       �pJ	�ء�c��A��*
+
+train/clip_range��L>^��8*       ����	�ء�c��A��*
+
+train/entropy_lossIs��Z�0       ���_	�ء�c��A��*!
+
+train/explained_variance  ��>�֫+       ��K	�ء�c��A��*
+
+train/learning_rateRI�9)=41"       x=�	�ء�c��A��*
+
+
+train/lossF�,L|���2       $V�	�ء�c��A��*#
+!
+train/policy_gradient_loss�Eз/��(       �pJ	�ء�c��A��*
+
+train/value_loss��Lڴu�        )�P	����c��A��*
+
+time/fps  �D����'       ��F	����c��A��*
+
+train/approx_kl��"6��x�+       ��K	����c��A��*
+
+train/clip_fraction    (��(       �pJ	����c��A��*
+
+train/clip_range��L>����*       ����	����c��A��*
+
+train/entropy_lossVw�����b0       ���_	����c��A��*!
+
+train/explained_variance    ș�d+       ��K	����c��A��*
+
+train/learning_rateRI�95.?E"       x=�	����c��A��*
+
+
+train/lossb�1L��12       $V�	����c��A��*#
+!
+train/policy_gradient_loss6��j��(       �pJ	����c��A��*
+
+train/value_loss�5�LF	�p        )�P	� ��c��A��*
+
+time/fps  �D>p'       ��F	� ��c��A��*
+
+train/approx_kl3CC4@^�+       ��K	��c��A��*
+
+train/clip_fraction    �0HV(       �pJ	��c��A��*
+
+train/clip_range��L>w���*       ����	��c��A��*
+
+train/entropy_loss	v��w�)0       ���_	��c��A��*!
+
+train/explained_variance    )��+       ��K	��c��A��*
+
+train/learning_rateRI�9��l�"       x=�	��c��A��*
+
+
+train/lossZ�8L��k�2       $V�	��c��A��*#
+!
+train/policy_gradient_loss\A5�5(       �pJ	��c��A��*
+
+train/value_lossx��L5��        )�P	Qa�c��A��*
+
+time/fps  �DY=�T'       ��F	Qa�c��A��*
+
+train/approx_kl3'�5��+       ��K	Qa�c��A��*
+
+train/clip_fraction    aDq�(       �pJ	Qa�c��A��*
+
+train/clip_range��L>nNݝ*       ����	Qa�c��A��*
+
+train/entropy_lossru���p��0       ���_	Qa�c��A��*!
+
+train/explained_variance   4��q#+       ��K	Qa�c��A��*
+
+train/learning_rateRI�9\�E"       x=�	Qa�c��A��*
+
+
+train/loss�1L��.2       $V�	Qa�c��A��*#
+!
+train/policy_gradient_loss�0���W��(       �pJ	Qa�c��A��*
+
+train/value_lossfX�L��Z�        )�P	;�D�c��A��*
+
+time/fps  �D<D<'       ��F	;�D�c��A��*
+
+train/approx_kl ��5��+       ��K	;�D�c��A��*
+
+train/clip_fraction    7,�(       �pJ	;�D�c��A��*
+
+train/clip_range��L>���*       ����	;�D�c��A��*
+
+train/entropy_lossJw����:0       ���_	;�D�c��A��*!
+
+train/explained_variance   �Tn�[+       ��K	;�D�c��A��*
+
+train/learning_rateRI�9*��$"       x=�	;�D�c��A��*
+
+
+train/loss
+{/L�\ʡ2       $V�	;�D�c��A��*#
+!
+train/policy_gradient_loss�z��ꁴV(       �pJ	;�D�c��A��*
+
+train/value_loss��LݾX�        )�P	�Rn�c��A��*
+
+time/fps  �D>f؃'       ��F	�Rn�c��A��*
+
+train/approx_kl3�66v%ӈ+       ��K	�Rn�c��A��*
+
+train/clip_fraction    5JI(       �pJ	�Rn�c��A��*
+
+train/clip_range��L>d1D2*       ����	�Rn�c��A��*
+
+train/entropy_lossy��A�:0       ���_	�Rn�c��A��*!
+
+train/explained_variance  ��@7|�+       ��K	�Rn�c��A��*
+
+train/learning_rateRI�9(�m�"       x=�	�Rn�c��A��*
+
+
+train/loss~�1L��;2       $V�	�Rn�c��A��*#
+!
+train/policy_gradient_loss+���M�K(       �pJ	�Rn�c��A��*
+
+train/value_lossP��L:f�        )�P	\��c��A��*
+
+time/fps  �D�I�'       ��F	\��c��A��*
+
+train/approx_kl3�f6��+       ��K	\��c��A��*
+
+train/clip_fraction    ����(       �pJ	\��c��A��*
+
+train/clip_range��L>�h��*       ����	\��c��A��*
+
+train/entropy_loss�{���
+g�0       ���_	\��c��A��*!
+
+train/explained_variance  �4G�%�+       ��K	\��c��A��*
+
+train/learning_rateRI�9���B"       x=�	\��c��A��*
+
+
+train/loss58L)Ϸ�2       $V�	\��c��A��*#
+!
+train/policy_gradient_lossg�Q�Uw�O(       �pJ	\��c��A��*
+
+train/value_lossWʴL��        )�P	�*��c��AЋ*
+
+time/fps @�D�sw'       ��F	�*��c��AЋ*
+
+train/approx_kl���67Հ�+       ��K	�*��c��AЋ*
+
+train/clip_fraction    ��\(       �pJ	�*��c��AЋ*
+
+train/clip_range��L>�3�*       ����	�*��c��AЋ*
+
+train/entropy_loss�~��^�D�0       ���_	�*��c��AЋ*!
+
+train/explained_variance   ���]@+       ��K	�*��c��AЋ*
+
+train/learning_rateRI�9���"       x=�	�*��c��AЋ*
+
+
+train/lossP�=L�* �2       $V�	�*��c��AЋ*#
+!
+train/policy_gradient_lossWo�����f(       �pJ	�*��c��AЋ*
+
+train/value_loss颼L�'p        )�P	�r��c��A��*
+
+time/fps @�DBu=*'       ��F	�r��c��A��*
+
+train/approx_klf�B6���+       ��K	�r��c��A��*
+
+train/clip_fraction    ���(       �pJ	�r��c��A��*
+
+train/clip_range��L>��C*       ����	�r��c��A��*
+
+train/entropy_loss{�����0       ���_	�r��c��A��*!
+
+train/explained_variance  �4�c�-+       ��K	�r��c��A��*
+
+train/learning_rateRI�9שQ"       x=�	�r��c��A��*
+
+
+train/lossV�FLp}�2       $V�	�r��c��A��*#
+!
+train/policy_gradient_lossF犸�}-((       �pJ	�r��c��A��*
+
+train/value_lossu��L���        )�P	�+
+�c��A�*
+
+time/fps @�D$_z�'       ��F	�+
+�c��A�*
+
+train/approx_kl��O5���+       ��K	�+
+�c��A�*
+
+train/clip_fraction    Gҟ�(       �pJ	�+
+�c��A�*
+
+train/clip_range��L>�bw*       ����	�+
+�c��A�*
+
+train/entropy_loss������0       ���_	�+
+�c��A�*!
+
+train/explained_variance    ���+       ��K	�+
+�c��A�*
+
+train/learning_rateRI�9:��<"       x=�	�+
+�c��A�*
+
+
+train/loss�IOL�ei2       $V�	�+
+�c��A�*#
+!
+train/policy_gradient_loss\"˶�m�$(       �pJ	�+
+�c��A�*
+
+train/value_lossɭ�L|��=        )�P	��5�c��A��*
+
+time/fps @�D��B'       ��F	��5�c��A��*
+
+train/approx_kl36��7D+       ��K	��5�c��A��*
+
+train/clip_fraction    ��Ћ(       �pJ	��5�c��A��*
+
+train/clip_range��L>
+�i*       ����	��5�c��A��*
+
+train/entropy_loss񀌿���0       ���_	��5�c��A��*!
+
+train/explained_variance  5p��F+       ��K	��5�c��A��*
+
+train/learning_rateRI�9���"       x=�	��5�c��A��*
+
+
+train/lossB.UL�ȟ�2       $V�	��5�c��A��*#
+!
+train/policy_gradient_loss����{WI�(       �pJ	��5�c��A��*
+
+train/value_loss+��L�n�0        )�P	�M_�c��A��*
+
+time/fps @�D=��U'       ��F	�M_�c��A��*
+
+train/approx_kl3)�5�b+       ��K	�M_�c��A��*
+
+train/clip_fraction    �J�2(       �pJ	�M_�c��A��*
+
+train/clip_range��L>0�|*       ����	�M_�c��A��*
+
+train/entropy_loss����?EU0       ���_	�M_�c��A��*!
+
+train/explained_variance  ��q���+       ��K	�M_�c��A��*
+
+train/learning_rateRI�9�s��"       x=�	�M_�c��A��*
+
+
+train/loss�WKLml72       $V�	�M_�c��A��*#
+!
+train/policy_gradient_lossp�P����E(       �pJ	�M_�c��A��*
+
+train/value_lossQs�L ��
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_19/events.out.tfevents.1724755308.Trading.93308.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_19/events.out.tfevents.1724755308.Trading.93308.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_19/events.out.tfevents.1724755308.Trading.93308.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_19/events.out.tfevents.1724755308.Trading.93308.0	
@@ -0,0 +1,2804 @@
+H       ��H�	3)[k��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer��0�       QKD	���\k��A�N*
+
+time/fps @�D`)y#       ��wC	���\k��A�N*
+
+train/reward1�p��`��        )�P	L$�_k��A��*
+
+time/fps ��D��A '       ��F	L$�_k��A��*
+
+train/approx_kl��88-��+       ��K	L$�_k��A��*
+
+train/clip_fraction    I��h(       �pJ	L$�_k��A��*
+
+train/clip_range��L>���*       ����	L$�_k��A��*
+
+train/entropy_loss������BW0       ���_	L$�_k��A��*!
+
+train/explained_variance  ���+       ��K	L$�_k��A��*
+
+train/learning_rateRI�9T=��"       x=�	L$�_k��A��*
+
+
+train/loss,�I�z`2       $V�	L$�_k��A��*#
+!
+train/policy_gradient_lossj�Ÿ�3��$       B+�M	L$�_k��A��*
+
+train/reward�"u�\PHm(       �pJ	L$�_k��A��*
+
+train/value_loss�,[Jr���        )�P	Q�ck��A��*
+
+time/fps �mDZ��'       ��F	Q�ck��A��*
+
+train/approx_kla^S6=�f�+       ��K	Q�ck��A��*
+
+train/clip_fraction    ۬\O(       �pJ	Q�ck��A��*
+
+train/clip_range��L>� �*       ����	Q�ck��A��*
+
+train/entropy_loss����8�*0       ���_	Q�ck��A��*!
+
+train/explained_variance  r���8+       ��K	Q�ck��A��*
+
+train/learning_rateRI�9�x�"       x=�	Q�ck��A��*
+
+
+train/lossd�J��1H2       $V�	Q�ck��A��*#
+!
+train/policy_gradient_lossg⁷�:�$       B+�M	Q�ck��A��*
+
+train/rewardHz�����(       �pJ	Q�ck��A��*
+
+train/value_lossI�K��        )�P	��
+fk��A��*
+
+time/fps @gD'b�'       ��F	��
+fk��A��*
+
+train/approx_kl�Z�3��s�+       ��K	��
+fk��A��*
+
+train/clip_fraction    �|�(       �pJ	��
+fk��A��*
+
+train/clip_range��L>_��*       ����	��
+fk��A��*
+
+train/entropy_lossa���E�
+�0       ���_	��
+fk��A��*!
+
+train/explained_variance  е���+       ��K	��
+fk��A��*
+
+train/learning_rateRI�9�e�"       x=�	��
+fk��A��*
+
+
+train/loss2K1]��2       $V�	��
+fk��A��*#
+!
+train/policy_gradient_loss�Z�5�0�h$       B+�M	��
+fk��A��*
+
+train/rewardÒÊ*��(       �pJ	��
+fk��A��*
+
+train/value_loss}�K�x�,        )�P	�0�hk��A��*
+
+time/fps  dD�H['       ��F	�0�hk��A��*
+
+train/approx_klaN*6+�JC+       ��K	�0�hk��A��*
+
+train/clip_fraction    �Q_(       �pJ	�0�hk��A��*
+
+train/clip_range��L>߁@*       ����	�0�hk��A��*
+
+train/entropy_loss؜���Qo0       ���_	�0�hk��A��*!
+
+train/explained_variance   4_�V�+       ��K	�0�hk��A��*
+
+train/learning_rateRI�9ݫe�"       x=�	�0�hk��A��*
+
+
+train/loss&�2K���2       $V�	�0�hk��A��*#
+!
+train/policy_gradient_lossLpT�~ ��$       B+�M	�0�hk��A��*
+
+train/reward9�r���(       �pJ	�0�hk��A��*
+
+train/value_lossڵKًZj        )�P	X�kk��A��*
+
+time/fps �aD�H��'       ��F	X�kk��A��*
+
+train/approx_kl¼�6�<��+       ��K	X�kk��A��*
+
+train/clip_fraction    ��(       �pJ	X�kk��A��*
+
+train/clip_range��L>��cP*       ����	X�kk��A��*
+
+train/entropy_losst����gE0       ���_	X�kk��A��*!
+
+train/explained_variance  ��N�A�+       ��K	X�kk��A��*
+
+train/learning_rateRI�9��+�"       x=�	X�kk��A��*
+
+
+train/loss��KT���2       $V�	X�kk��A��*#
+!
+train/policy_gradient_loss22�C�,$       B+�M	X�kk��A��*
+
+train/reward;���Q�u�(       �pJ	X�kk��A��*
+
+train/value_loss�Kɕ(�        )�P	�5�nk��A��*
+
+time/fps  `Du\�'       ��F	�5�nk��A��*
+
+train/approx_kl��6��ǘ+       ��K	�5�nk��A��*
+
+train/clip_fraction    �N˯(       �pJ	�5�nk��A��*
+
+train/clip_range��L>�x�r*       ����	�5�nk��A��*
+
+train/entropy_loss(���ܜ��0       ���_	�5�nk��A��*!
+
+train/explained_variance   �	rtA+       ��K	�5�nk��A��*
+
+train/learning_rateRI�9�w�+"       x=�	�5�nk��A��*
+
+
+train/lossV�?J�hP2       $V�	�5�nk��A��*#
+!
+train/policy_gradient_loss��Q�_N��$       B+�M	�5�nk��A��*
+
+train/reward���6MXB(       �pJ	�5�nk��A��*
+
+train/value_loss�-�J_i        )�P	g�qk��A��*
+
+time/fps  _D���'       ��F	g�qk��A��*
+
+train/approx_kl�7�Gk+       ��K	g�qk��A��*
+
+train/clip_fraction    U}��(       �pJ	g�qk��A��*
+
+train/clip_range��L>�]�*       ����	g�qk��A��*
+
+train/entropy_lossT���	X�_0       ���_	g�qk��A��*!
+
+train/explained_variance    
+���+       ��K	g�qk��A��*
+
+train/learning_rateRI�9Hf�H"       x=�	g�qk��A��*
+
+
+train/loss�zQJ�k#2       $V�	g�qk��A��*#
+!
+train/policy_gradient_loss��巠�A}$       B+�M	g�qk��A��*
+
+train/reward�������(       �pJ	g�qk��A��*
+
+train/value_loss�7�JK4        )�P	G��tk��A��*
+
+time/fps �^D����'       ��F	G��tk��A��*
+
+train/approx_klG�h6�B.B+       ��K	G��tk��A��*
+
+train/clip_fraction    ZL*�(       �pJ	G��tk��A��*
+
+train/clip_range��L>sI5
+*       ����	G��tk��A��*
+
+train/entropy_loss�����0       ���_	G��tk��A��*!
+
+train/explained_variance   ����+       ��K	G��tk��A��*
+
+train/learning_rateRI�9gs "       x=�	G��tk��A��*
+
+
+train/loss�q
+K�>Jl2       $V�	G��tk��A��*#
+!
+train/policy_gradient_lossDA~����$       B+�M	G��tk��A��*
+
+train/reward���U�(       �pJ	G��tk��A��*
+
+train/value_loss��K���        )�P	�wk��A��*
+
+time/fps  ^D_�'       ��F	�wk��A��*
+
+train/approx_kl�4t�+       ��K	�wk��A��*
+
+train/clip_fraction    8�P%(       �pJ	�wk��A��*
+
+train/clip_range��L>/fH*       ����	�wk��A��*
+
+train/entropy_lossw���,�p`0       ���_	�wk��A��*!
+
+train/explained_variance    �S[+       ��K	�wk��A��*
+
+train/learning_rateRI�9E5d�"       x=�	�wk��A��*
+
+
+train/loss���K+X�2       $V�	�wk��A��*#
+!
+train/policy_gradient_loss��ʵ�Nb�$       B+�M	�wk��A��*
+
+train/reward-!���Kqg(       �pJ	�wk��A��*
+
+train/value_loss�L���        )�P	�gzk��A��*
+
+time/fps �]DP�h'       ��F	�gzk��A��*
+
+train/approx_kl��93�p�[+       ��K	�gzk��A��*
+
+train/clip_fraction    i��(       �pJ	�gzk��A��*
+
+train/clip_range��L>�0T*       ����	�gzk��A��*
+
+train/entropy_lossl���j҆�0       ���_	�gzk��A��*!
+
+train/explained_variance    ��5�+       ��K	�gzk��A��*
+
+train/learning_rateRI�9t��"       x=�	�gzk��A��*
+
+
+train/lossPyxK��.�2       $V�	�gzk��A��*#
+!
+train/policy_gradient_lossd��5�7�Y$       B+�M	�gzk��A��*
+
+train/reward�ա��Z,(       �pJ	�gzk��A��*
+
+train/value_lossH��K`��&        )�P	v`N}k��A��*
+
+time/fps @]D�@�''       ��F	v`N}k��A��*
+
+train/approx_klQ�6��`+       ��K	v`N}k��A��*
+
+train/clip_fraction    ���(       �pJ	v`N}k��A��*
+
+train/clip_range��L>b�1*       ����	v`N}k��A��*
+
+train/entropy_loss8����'�s0       ���_	v`N}k��A��*!
+
+train/explained_variance    x
+�+       ��K	v`N}k��A��*
+
+train/learning_rateRI�9���t"       x=�	v`N}k��A��*
+
+
+train/loss���KQ)s�2       $V�	v`N}k��A��*#
+!
+train/policy_gradient_loss���
+�#I$       B+�M	v`N}k��A��*
+
+train/reward�ݝ�N\��(       �pJ	v`N}k��A��*
+
+train/value_loss�zL�(ۗ        )�P	�'B�k��A��*
+
+time/fps �\D����'       ��F	�'B�k��A��*
+
+train/approx_kl�:6�T�v+       ��K	�'B�k��A��*
+
+train/clip_fraction    �CJ�(       �pJ	�'B�k��A��*
+
+train/clip_range��L>��o*       ����	�'B�k��A��*
+
+train/entropy_lossU���;�0       ���_	�'B�k��A��*!
+
+train/explained_variance    r�Z�+       ��K	�'B�k��A��*
+
+train/learning_rateRI�9��nB"       x=�	�'B�k��A��*
+
+
+train/loss�mZK���,2       $V�	�'B�k��A��*#
+!
+train/policy_gradient_loss�������$       B+�M	�'B�k��A��*
+
+train/reward�������(       �pJ	�'B�k��A��*
+
+train/value_loss�!�K�¿        )�P	�!L�k��A��*
+
+time/fps �[D۳��'       ��F	�!L�k��A��*
+
+train/approx_kl1�+6�`��+       ��K	�!L�k��A��*
+
+train/clip_fraction    �D5(       �pJ	�!L�k��A��*
+
+train/clip_range��L>(�@�*       ����	�!L�k��A��*
+
+train/entropy_loss������A0       ���_	�!L�k��A��*!
+
+train/explained_variance    �1��+       ��K	�!L�k��A��*
+
+train/learning_rateRI�9��p�"       x=�	�!L�k��A��*
+
+
+train/lossq_KFr��2       $V�	�!L�k��A��*#
+!
+train/policy_gradient_loss���_�$       B+�M	�!L�k��A��*
+
+train/reward/��ÕD�T(       �pJ	�!L�k��A��*
+
+train/value_loss9��K�Z�        )�P	�=`�k��A��	*
+
+time/fps �ZDp힭'       ��F	�=`�k��A��	*
+
+train/approx_klQI�5�(�	+       ��K	�=`�k��A��	*
+
+train/clip_fraction    a/�(       �pJ	�=`�k��A��	*
+
+train/clip_range��L>���*       ����	�=`�k��A��	*
+
+train/entropy_loss�����4�0       ���_	�=`�k��A��	*!
+
+train/explained_variance   4\~��+       ��K	�=`�k��A��	*
+
+train/learning_rateRI�9\�ܟ"       x=�	�=`�k��A��	*
+
+
+train/loss�ieK2ϰ�2       $V�	�=`�k��A��	*#
+!
+train/policy_gradient_loss������	�$       B+�M	�=`�k��A��	*
+
+train/reward�l��X��J(       �pJ	�=`�k��A��	*
+
+train/value_loss�s�K)~�        )�P	���k��A��	*
+
+time/fps @YDd�\Q'       ��F	���k��A��	*
+
+train/approx_kl�4�#o+       ��K	���k��A��	*
+
+train/clip_fraction    ��Y(       �pJ	���k��A��	*
+
+train/clip_range��L>��g*       ����	���k��A��	*
+
+train/entropy_loss����|/�@0       ���_	���k��A��	*!
+
+train/explained_variance    �+>�+       ��K	���k��A��	*
+
+train/learning_rateRI�9����"       x=�	���k��A��	*
+
+
+train/loss�]�K�s8�2       $V�	���k��A��	*#
+!
+train/policy_gradient_lossd�.5m�$       B+�M	���k��A��	*
+
+train/rewardk������(       �pJ	���k��A��	*
+
+train/value_loss�xL��        )�P	��k��A�
+*
+
+time/fps �XDaW--'       ��F	��k��A�
+*
+
+train/approx_klh <6�ʹ+       ��K	��k��A�
+*
+
+train/clip_fraction    ��(       �pJ	��k��A�
+*
+
+train/clip_range��L>P���*       ����	��k��A�
+*
+
+train/entropy_lossP���و%�0       ���_	��k��A�
+*!
+
+train/explained_variance    �t�Z+       ��K	��k��A�
+*
+
+train/learning_rateRI�9ꮐ�"       x=�	��k��A�
+*
+
+
+train/loss��K�2��2       $V�	6���k��A�
+*#
+!
+train/policy_gradient_loss�E·_n�$       B+�M	6���k��A�
+*
+
+train/rewardr���1���(       �pJ	6���k��A�
+*
+
+train/value_loss�DBLϚ�        )�P	����k��A��*
+
+time/fps �WD���'       ��F	����k��A��*
+
+train/approx_klQ�"3�n�+       ��K	����k��A��*
+
+train/clip_fraction    ���(       �pJ	����k��A��*
+
+train/clip_range��L>Y`�*       ����	����k��A��*
+
+train/entropy_lossӜ���ڣ0       ���_	����k��A��*!
+
+train/explained_variance   ���L+       ��K	����k��A��*
+
+train/learning_rateRI�9
+��L"       x=�	����k��A��*
+
+
+train/loss�^�K֗e�2       $V�	����k��A��*#
+!
+train/policy_gradient_lossa5w5��$       B+�M	����k��A��*
+
+train/reward�����ز(       �pJ	����k��A��*
+
+train/value_loss-cTL�Z�        )�P	ðŒk��A��*
+
+time/fps  WD���'       ��F	ðŒk��A��*
+
+train/approx_kl�e/5��p+       ��K	ðŒk��A��*
+
+train/clip_fraction    ���g(       �pJ	ðŒk��A��*
+
+train/clip_range��L>�S�*       ����	ðŒk��A��*
+
+train/entropy_loss����P��T0       ���_	ðŒk��A��*!
+
+train/explained_variance    ����+       ��K	ðŒk��A��*
+
+train/learning_rateRI�9����"       x=�	ðŒk��A��*
+
+
+train/loss��Ktl��2       $V�	ðŒk��A��*#
+!
+train/policy_gradient_loss�d����}y$       B+�M	ðŒk��A��*
+
+train/reward����y�(       �pJ	ðŒk��A��*
+
+train/value_loss��!L���        )�P	�'ߕk��A��*
+
+time/fps �VDO'��'       ��F	�'ߕk��A��*
+
+train/approx_kl�F5���+       ��K	�'ߕk��A��*
+
+train/clip_fraction    ���.(       �pJ	�'ߕk��A��*
+
+train/clip_range��L>|H�*       ����	�'ߕk��A��*
+
+train/entropy_loss+�����|*0       ���_	�'ߕk��A��*!
+
+train/explained_variance   ��~'�+       ��K	�'ߕk��A��*
+
+train/learning_rateRI�9��"       x=�	�'ߕk��A��*
+
+
+train/lossk�K�ܤe2       $V�	�'ߕk��A��*#
+!
+train/policy_gradient_loss_٧�Aț$       B+�M	�'ߕk��A��*
+
+train/reward����Ik�(       �pJ	�'ߕk��A��*
+
+train/value_loss��BL!=G3        )�P	<��k��A��*
+
+time/fps �UD��|'       ��F	<��k��A��*
+
+train/approx_kl���4�#�7+       ��K	<��k��A��*
+
+train/clip_fraction    ��me(       �pJ	<��k��A��*
+
+train/clip_range��L>]�� *       ����	<��k��A��*
+
+train/entropy_loss؛��c�q�0       ���_	<��k��A��*!
+
+train/explained_variance   �XUZ�+       ��K	<��k��A��*
+
+train/learning_rateRI�9��H�"       x=�	<��k��A��*
+
+
+train/loss^+�K���2       $V�	<��k��A��*#
+!
+train/policy_gradient_loss�=ҵK0g$       B+�M	<��k��A��*
+
+train/reward�òs�(       �pJ	<��k��A��*
+
+train/value_loss��<LƈA�        )�P	�6.�k��A��
+*
+
+time/fps  UD��'       ��F	�6.�k��A��
+*
+
+train/approx_klWK>5�S%�+       ��K	�6.�k��A��
+*
+
+train/clip_fraction    ��y�(       �pJ	�6.�k��A��
+*
+
+train/clip_range��L>���*       ����	�6.�k��A��
+*
+
+train/entropy_loss�������J0       ���_	�6.�k��A��
+*!
+
+train/explained_variance   ��;Jl+       ��K	�6.�k��A��
+*
+
+train/learning_rateRI�9�]A"       x=�	�6.�k��A��
+*
+
+
+train/loss���K��=f2       $V�	�6.�k��A��
+*#
+!
+train/policy_gradient_loss+���[�$       B+�M	�6.�k��A��
+*
+
+train/rewardh0���#�(       �pJ	�6.�k��A��
+*
+
+train/value_lossM�'L&9צ        )�P	��\�k��A��*
+
+time/fps @TDSJu'       ��F	��\�k��A��*
+
+train/approx_kl���5І��+       ��K	��\�k��A��*
+
+train/clip_fraction    ��8M(       �pJ	��\�k��A��*
+
+train/clip_range��L>|}ۻ*       ����	��\�k��A��*
+
+train/entropy_loss�\��e0       ���_	��\�k��A��*!
+
+train/explained_variance    $���+       ��K	��\�k��A��*
+
+train/learning_rateRI�9�b�"       x=�	��\�k��A��*
+
+
+train/loss�O�K���=2       $V�	��\�k��A��*#
+!
+train/policy_gradient_lossJ�V��f�$       B+�M	��\�k��A��*
+
+train/reward�T�ð��(       �pJ	��\�k��A��*
+
+train/value_loss+�&L=�Pk        )�P	�ބ�k��A��*
+
+time/fps �SDR �i'       ��F	�ބ�k��A��*
+
+train/approx_klx��4��+       ��K	�ބ�k��A��*
+
+train/clip_fraction    ����(       �pJ	�ބ�k��A��*
+
+train/clip_range��L>�Y�v*       ����	�ބ�k��A��*
+
+train/entropy_loss����PŞ0       ���_	�ބ�k��A��*!
+
+train/explained_variance    GZ��+       ��K	�ބ�k��A��*
+
+train/learning_rateRI�9���$"       x=�	�ބ�k��A��*
+
+
+train/loss:��KEʡ2       $V�	�ބ�k��A��*#
+!
+train/policy_gradient_losso2�@ZH�$       B+�M	�ބ�k��A��*
+
+train/reward;��Ò�p�(       �pJ	�ބ�k��A��*
+
+train/value_loss�l&L����        )�P	1���k��A�*
+
+time/fps �SD��J�'       ��F	1���k��A�*
+
+train/approx_kl�u�6tD+       ��K	1���k��A�*
+
+train/clip_fraction    �l(       �pJ	1���k��A�*
+
+train/clip_range��L>��(*       ����	1���k��A�*
+
+train/entropy_loss'������0       ���_	1���k��A�*!
+
+train/explained_variance  �3�F+       ��K	1���k��A�*
+
+train/learning_rateRI�9��H�"       x=�	1���k��A�*
+
+
+train/lossM�K�B��2       $V�	1���k��A�*#
+!
+train/policy_gradient_lossvI�#I�$       B+�M	1���k��A�*
+
+train/reward�C���~�(       �pJ	����k��A�*
+
+train/value_loss*�*L�^.s        )�P	&��k��A��*
+
+time/fps @SD�`�'       ��F	&��k��A��*
+
+train/approx_kl�GD5�g)+       ��K	&��k��A��*
+
+train/clip_fraction    y��(       �pJ	&��k��A��*
+
+train/clip_range��L>WZq]*       ����	&��k��A��*
+
+train/entropy_loss���}��0       ���_	&��k��A��*!
+
+train/explained_variance    v���+       ��K	&��k��A��*
+
+train/learning_rateRI�9�./G"       x=�	&��k��A��*
+
+
+train/loss���K�82       $V�	&��k��A��*#
+!
+train/policy_gradient_loss�g���K[�$       B+�M	&��k��A��*
+
+train/reward�R����[l(       �pJ	&��k��A��*
+
+train/value_loss}o.Lq�        )�P	dƹ�k��A��*
+
+time/fps  SD����'       ��F	dƹ�k��A��*
+
+train/approx_kl~W)5�8�n+       ��K	dƹ�k��A��*
+
+train/clip_fraction    �u�g(       �pJ	dƹ�k��A��*
+
+train/clip_range��L>˥&l*       ����	dƹ�k��A��*
+
+train/entropy_lossQ���[��0       ���_	dƹ�k��A��*!
+
+train/explained_variance    ����+       ��K	dƹ�k��A��*
+
+train/learning_rateRI�9yN��"       x=�	dƹ�k��A��*
+
+
+train/loss�X�K؍@b2       $V�	dƹ�k��A��*#
+!
+train/policy_gradient_loss�y���|��$       B+�M	dƹ�k��A��*
+
+train/reward?���|�!M(       �pJ	dƹ�k��A��*
+
+train/value_loss�51L)��        )�P	�ܮk��A��*
+
+time/fps �RDat�U'       ��F	�ܮk��A��*
+
+train/approx_kl$��5"#b+       ��K	�ܮk��A��*
+
+train/clip_fraction    ��(       �pJ	�ܮk��A��*
+
+train/clip_range��L>���*       ����	�ܮk��A��*
+
+train/entropy_lossY���!}H0       ���_	�ܮk��A��*!
+
+train/explained_variance    g�p+       ��K	�ܮk��A��*
+
+train/learning_rateRI�9����"       x=�	�ܮk��A��*
+
+
+train/lossG�K3��2       $V�	�ܮk��A��*#
+!
+train/policy_gradient_lossY�㶾�P$       B+�M	�ܮk��A��*
+
+train/reward�\��Y�b�(       �pJ	�ܮk��A��*
+
+train/value_loss.m)L{Cb�        )�P	E��k��A��*
+
+time/fps @RD	��'       ��F	E��k��A��*
+
+train/approx_kla+6�ef�+       ��K	E��k��A��*
+
+train/clip_fraction    �ˉ�(       �pJ	E��k��A��*
+
+train/clip_range��L>uAk*       ����	E��k��A��*
+
+train/entropy_lossޙ��h��0       ���_	E��k��A��*!
+
+train/explained_variance    _���+       ��K	E��k��A��*
+
+train/learning_rateRI�9���"       x=�	E��k��A��*
+
+
+train/loss��{K�dP'2       $V�	E��k��A��*#
+!
+train/policy_gradient_loss������s$       B+�M	E��k��A��*
+
+train/reward_Y���X(       �pJ	E��k��A��*
+
+train/value_loss��K{ K        )�P	yq+�k��A��*
+
+time/fps �QD#qN'       ��F	yq+�k��A��*
+
+train/approx_kl�Ԧ6N�+       ��K	yq+�k��A��*
+
+train/clip_fraction    �ͥ(       �pJ	yq+�k��A��*
+
+train/clip_range��L>�hi>*       ����	yq+�k��A��*
+
+train/entropy_lossј�����0       ���_	yq+�k��A��*!
+
+train/explained_variance   ����+       ��K	yq+�k��A��*
+
+train/learning_rateRI�9����"       x=�	yq+�k��A��*
+
+
+train/lossK�VK�y�2       $V�	yq+�k��A��*#
+!
+train/policy_gradient_loss i���18�$       B+�M	yq+�k��A��*
+
+train/reward�Ǖ��w(       �pJ	yq+�k��A��*
+
+train/value_loss��K�cI        )�P	�I�k��A��*
+
+time/fps �QD)	��'       ��F	�I�k��A��*
+
+train/approx_kl�ڰ5��~+       ��K	�I�k��A��*
+
+train/clip_fraction    �>+�(       �pJ	�I�k��A��*
+
+train/clip_range��L>�*       ����	�I�k��A��*
+
+train/entropy_loss$���P���0       ���_	�I�k��A��*!
+
+train/explained_variance    �.)+       ��K	�I�k��A��*
+
+train/learning_rateRI�9�2"       x=�	�I�k��A��*
+
+
+train/lossF�2K�PC�2       $V�	�I�k��A��*#
+!
+train/policy_gradient_loss�1Ѷ^���$       B+�M	�I�k��A��*
+
+train/reward9w^×	@/(       �pJ	�I�k��A��*
+
+train/value_lossR��K�p��        )�P	�l�k��A��*
+
+time/fps @QDs��'       ��F	�l�k��A��*
+
+train/approx_klө7Ćub+       ��K	�l�k��A��*
+
+train/clip_fraction    �N�9(       �pJ	�l�k��A��*
+
+train/clip_range��L>���
+*       ����	�l�k��A��*
+
+train/entropy_loss����]?�40       ���_	�l�k��A��*!
+
+train/explained_variance   5%�$$+       ��K	�l�k��A��*
+
+train/learning_rateRI�9�u˵"       x=�	�l�k��A��*
+
+
+train/loss��K��%�2       $V�	�l�k��A��*#
+!
+train/policy_gradient_lossz�-���U$       B+�M	�l�k��A��*
+
+train/reward�U~�g>R�(       �pJ	�l�k��A��*
+
+train/value_loss,+�K����        )�P	\D��k��A�*
+
+time/fps  QD��'       ��F	\D��k��A�*
+
+train/approx_kl��36��B�+       ��K	\D��k��A�*
+
+train/clip_fraction    @ӣf(       �pJ	\D��k��A�*
+
+train/clip_range��L>�z	�*       ����	\D��k��A�*
+
+train/entropy_loss���e
+ �0       ���_	\D��k��A�*!
+
+train/explained_variance    m���+       ��K	\D��k��A�*
+
+train/learning_rateRI�9=��"       x=�	\D��k��A�*
+
+
+train/loss9�K�?�2       $V�	\D��k��A�*#
+!
+train/policy_gradient_loss[27��7�$       B+�M	\D��k��A�*
+
+train/reward��ô.��(       �pJ	\D��k��A�*
+
+train/value_loss��K��Mw        )�P	g4��k��A��*
+
+time/fps �PD��>�'       ��F	g4��k��A��*
+
+train/approx_kl�p�5M��+       ��K	g4��k��A��*
+
+train/clip_fraction    ,VZ(       �pJ	g4��k��A��*
+
+train/clip_range��L>ͱ�*       ����	g4��k��A��*
+
+train/entropy_loss����Tqo�0       ���_	g4��k��A��*!
+
+train/explained_variance  �4M_�3+       ��K	g4��k��A��*
+
+train/learning_rateRI�91���"       x=�	g4��k��A��*
+
+
+train/lossD�K��@�2       $V�	g4��k��A��*#
+!
+train/policy_gradient_loss���_��$       B+�M	g4��k��A��*
+
+train/reward˱�P02r(       �pJ	g4��k��A��*
+
+train/value_loss��K3=�        )�P	����k��A��*
+
+time/fps �PDjJ��'       ��F	����k��A��*
+
+train/approx_klW{H6h��+       ��K	����k��A��*
+
+train/clip_fraction    �W��(       �pJ	����k��A��*
+
+train/clip_range��L>�5��*       ����	����k��A��*
+
+train/entropy_loss����Or�0       ���_	����k��A��*!
+
+train/explained_variance    ,͈+       ��K	����k��A��*
+
+train/learning_rateRI�9�p�["       x=�	����k��A��*
+
+
+train/lossT}DK+@1�2       $V�	����k��A��*#
+!
+train/policy_gradient_loss������$k$       B+�M	����k��A��*
+
+train/reward���F��(       �pJ	����k��A��*
+
+train/value_lossC��K����        )�P	.��k��A��*
+
+time/fps  PDlg$�'       ��F	.��k��A��*
+
+train/approx_klGv�4&�u+       ��K	.��k��A��*
+
+train/clip_fraction    G�(       �pJ	.��k��A��*
+
+train/clip_range��L>Q��B*       ����	.��k��A��*
+
+train/entropy_loss뛌��u�0       ���_	.��k��A��*!
+
+train/explained_variance   4%�I@+       ��K	.��k��A��*
+
+train/learning_rateRI�9���"       x=�	.��k��A��*
+
+
+train/lossf��K��҅2       $V�	.��k��A��*#
+!
+train/policy_gradient_loss�Z�sN|a$       B+�M	.��k��A��*
+
+train/reward�c��/F�(       �pJ	.��k��A��*
+
+train/value_loss�LO��	        )�P	��X�k��A��*
+
+time/fps �OD���Z'       ��F	��X�k��A��*
+
+train/approx_kl�9>6=!>+       ��K	��X�k��A��*
+
+train/clip_fraction    Q;.�(       �pJ	��X�k��A��*
+
+train/clip_range��L>�� �*       ����	��X�k��A��*
+
+train/entropy_loss*������H0       ���_	��X�k��A��*!
+
+train/explained_variance   ���+       ��K	��X�k��A��*
+
+train/learning_rateRI�9L�
+x"       x=�	��X�k��A��*
+
+
+train/loss�3�KET%2       $V�	��X�k��A��*#
+!
+train/policy_gradient_loss;����?$       B+�M	��X�k��A��*
+
+train/reward�V��Rd�(       �pJ	��X�k��A��*
+
+train/value_loss��L�s4B        )�P	�To�k��A��*
+
+time/fps �ODL���'       ��F	�To�k��A��*
+
+train/approx_kl�$/6���+       ��K	�To�k��A��*
+
+train/clip_fraction    ���6(       �pJ	�To�k��A��*
+
+train/clip_range��L> X�]*       ����	�To�k��A��*
+
+train/entropy_loss�����\�>0       ���_	�To�k��A��*!
+
+train/explained_variance    ^�� +       ��K	�To�k��A��*
+
+train/learning_rateRI�9���"       x=�	�To�k��A��*
+
+
+train/lossg��Kj�=2       $V�	�To�k��A��*#
+!
+train/policy_gradient_lossm�������$       B+�M	�To�k��A��*
+
+train/reward4��â(�(       �pJ	�To�k��A��*
+
+train/value_loss)�
+L��k        )�P	�7��k��A��*
+
+time/fps �OD�ǟ'       ��F	�7��k��A��*
+
+train/approx_kl4p�6P,f+       ��K	�7��k��A��*
+
+train/clip_fraction    ��V^(       �pJ	�7��k��A��*
+
+train/clip_range��L>v�nG*       ����	�7��k��A��*
+
+train/entropy_lossP���0�*�0       ���_	�7��k��A��*!
+
+train/explained_variance    �.g�+       ��K	�7��k��A��*
+
+train/learning_rateRI�9���"       x=�	�7��k��A��*
+
+
+train/loss��GKZ��2       $V�	�7��k��A��*#
+!
+train/policy_gradient_lossґ�7�_($       B+�M	�7��k��A��*
+
+train/reward�ҝ�|{(d(       �pJ	�7��k��A��*
+
+train/value_lossK�K��'        )�P	����k��A��*
+
+time/fps @OD�U|'       ��F	����k��A��*
+
+train/approx_klK�26
+a�+       ��K	����k��A��*
+
+train/clip_fraction    $�ã(       �pJ	����k��A��*
+
+train/clip_range��L>f��*       ����	����k��A��*
+
+train/entropy_loss뜌��߆s0       ���_	����k��A��*!
+
+train/explained_variance   4[	�+       ��K	����k��A��*
+
+train/learning_rateRI�9�'"       x=�	����k��A��*
+
+
+train/loss�ZK���2       $V�	����k��A��*#
+!
+train/policy_gradient_loss���[l�T$       B+�M	����k��A��*
+
+train/reward��ð���(       �pJ	����k��A��*
+
+train/value_loss1��K{�~�        )�P	H���k��A��*
+
+time/fps  OD��X�'       ��F	H���k��A��*
+
+train/approx_kla�7�G'z+       ��K	H���k��A��*
+
+train/clip_fraction    �>(       �pJ	H���k��A��*
+
+train/clip_range��L>��EL*       ����	H���k��A��*
+
+train/entropy_loss�������i0       ���_	H���k��A��*!
+
+train/explained_variance    �kf+       ��K	H���k��A��*
+
+train/learning_rateRI�9%��"       x=�	H���k��A��*
+
+
+train/loss=JK��K2       $V�	H���k��A��*#
+!
+train/policy_gradient_loss��k�eT�#$       B+�M	H���k��A��*
+
+train/reward��������(       �pJ	H���k��A��*
+
+train/value_lossV�K1��        )�P	��k��A��*
+
+time/fps  ODh���'       ��F	��k��A��*
+
+train/approx_kl�c6��0+       ��K	��k��A��*
+
+train/clip_fraction    �vP(       �pJ	��k��A��*
+
+train/clip_range��L>l�w*       ����	��k��A��*
+
+train/entropy_lossl����H�?0       ���_	��k��A��*!
+
+train/explained_variance    ��^�+       ��K	��k��A��*
+
+train/learning_rateRI�9�Od�"       x=�	��k��A��*
+
+
+train/loss�-XK�x��2       $V�	��k��A��*#
+!
+train/policy_gradient_loss]k?��]�n$       B+�M	��k��A��*
+
+train/rewardk����K��(       �pJ	��k��A��*
+
+train/value_lossY��KV!�6        )�P	,�k��A��*
+
+time/fps �ND(�Lk'       ��F	,�k��A��*
+
+train/approx_kl h;6�޲w+       ��K	,�k��A��*
+
+train/clip_fraction    �sr=(       �pJ	,�k��A��*
+
+train/clip_range��L>GE/:*       ����	,�k��A��*
+
+train/entropy_loss����5A?0       ���_	,�k��A��*!
+
+train/explained_variance    %"d+       ��K	,�k��A��*
+
+train/learning_rateRI�9{LF�"       x=�	,�k��A��*
+
+
+train/lossWp]K�k��2       $V�	,�k��A��*#
+!
+train/policy_gradient_loss찑�Y�$       B+�M	,�k��A��*
+
+train/reward����p6�(       �pJ	,�k��A��*
+
+train/value_loss.��K�S��        )�P	�1�k��A��*
+
+time/fps �ND���'       ��F	�1�k��A��*
+
+train/approx_kl�c16e=��+       ��K	�1�k��A��*
+
+train/clip_fraction    ���=(       �pJ	�1�k��A��*
+
+train/clip_range��L>Yu; *       ����	�1�k��A��*
+
+train/entropy_loss����q"�\0       ���_	�1�k��A��*!
+
+train/explained_variance   �u *+       ��K	�1�k��A��*
+
+train/learning_rateRI�9�h��"       x=�	�1�k��A��*
+
+
+train/loss��=K��ls2       $V�	�1�k��A��*#
+!
+train/policy_gradient_loss,m~���h�$       B+�M	�1�k��A��*
+
+train/reward_­�.�dj(       �pJ	�1�k��A��*
+
+train/value_loss
+��K�{&�        )�P	%Z�k��A��*
+
+time/fps �ND"�O'       ��F	%Z�k��A��*
+
+train/approx_kla�26H�Ƙ+       ��K	%Z�k��A��*
+
+train/clip_fraction    {R3�(       �pJ	%Z�k��A��*
+
+train/clip_range��L>l���*       ����	%Z�k��A��*
+
+train/entropy_loss�����ͯ�0       ���_	%Z�k��A��*!
+
+train/explained_variance    T�&�+       ��K	%Z�k��A��*
+
+train/learning_rateRI�9�yT"       x=�	%Z�k��A��*
+
+
+train/loss�)nK���$2       $V�	%Z�k��A��*#
+!
+train/policy_gradient_loss����H B$       B+�M	%Z�k��A��*
+
+train/rewardAtl�
+Ѡ`(       �pJ	%Z�k��A��*
+
+train/value_loss;��K ��        )�P	�j��k��A��*
+
+time/fps @ND�
+'       ��F	�j��k��A��*
+
+train/approx_kl>#x5��0{+       ��K	�j��k��A��*
+
+train/clip_fraction    ���(       �pJ	�j��k��A��*
+
+train/clip_range��L>B�F�*       ����	�j��k��A��*
+
+train/entropy_loss ���?
+=0       ���_	�j��k��A��*!
+
+train/explained_variance    �+       ��K	�j��k��A��*
+
+train/learning_rateRI�9���^"       x=�	�j��k��A��*
+
+
+train/lossV�:K�c<72       $V�	�j��k��A��*#
+!
+train/policy_gradient_loss��k��č�$       B+�M	�j��k��A��*
+
+train/rewardB�J�P&��(       �pJ	�j��k��A��*
+
+train/value_losssҩK�Δ�        )�P	>��k��A��*
+
+time/fps  ND�ž
+'       ��F	>��k��A��*
+
+train/approx_kl�1�6�o�f+       ��K	>��k��A��*
+
+train/clip_fraction    �G(       �pJ	>��k��A��*
+
+train/clip_range��L>_5S�*       ����	>��k��A��*
+
+train/entropy_lossᙌ�m�0       ���_	>��k��A��*!
+
+train/explained_variance    ���3+       ��K	>��k��A��*
+
+train/learning_rateRI�9h��"       x=�	>��k��A��*
+
+
+train/loss���J�;[2       $V�	>��k��A��*#
+!
+train/policy_gradient_loss��{�Z�WG$       B+�M	>��k��A��*
+
+train/reward=JC�:@�(       �pJ	>��k��A��*
+
+train/value_loss�xK6��o        )�P	~���k��A��*
+
+time/fps �MD���'       ��F	~���k��A��*
+
+train/approx_kl �R7�[�+       ��K	~���k��A��*
+
+train/clip_fraction    Cv�(       �pJ	~���k��A��*
+
+train/clip_range��L>H�J�*       ����	~���k��A��*
+
+train/entropy_lossn�����0       ���_	~���k��A��*!
+
+train/explained_variance    搘E+       ��K	~���k��A��*
+
+train/learning_rateRI�9�bO"       x=�	~���k��A��*
+
+
+train/loss�X�J( �2       $V�	~���k��A��*#
+!
+train/policy_gradient_loss�H��$       B+�M	~���k��A��*
+
+train/reward��7��&��(       �pJ	~���k��A��*
+
+train/value_lossϱK���        )�P	F�'�k��A��*
+
+time/fps �MDGq<�'       ��F	F�'�k��A��*
+
+train/approx_kl���5��ǈ+       ��K	F�'�k��A��*
+
+train/clip_fraction    �_�(       �pJ	F�'�k��A��*
+
+train/clip_range��L>lw�*       ����	F�'�k��A��*
+
+train/entropy_lossޛ��:��o0       ���_	F�'�k��A��*!
+
+train/explained_variance    �u�+       ��K	F�'�k��A��*
+
+train/learning_rateRI�9�sz"       x=�	F�'�k��A��*
+
+
+train/loss��JS� 2       $V�	��'�k��A��*#
+!
+train/policy_gradient_loss�q���1^$       B+�M	��'�k��A��*
+
+train/rewarde])Ê�+s(       �pJ	��'�k��A��*
+
+train/value_lossa_K���        )�P	a�U�k��A��*
+
+time/fps �MDS�,'       ��F	a�U�k��A��*
+
+train/approx_kl�o�5<��+       ��K	a�U�k��A��*
+
+train/clip_fraction    ���(       �pJ	a�U�k��A��*
+
+train/clip_range��L>�gP*       ����	a�U�k��A��*
+
+train/entropy_lossw����fKJ0       ���_	a�U�k��A��*!
+
+train/explained_variance    ��2+       ��K	a�U�k��A��*
+
+train/learning_rateRI�9���"       x=�	a�U�k��A��*
+
+
+train/loss�JZ'��2       $V�	a�U�k��A��*#
+!
+train/policy_gradient_lossܮ��zTt7$       B+�M	a�U�k��A��*
+
+train/reward��Öz
+(       �pJ	a�U�k��A��*
+
+train/value_loss�?K��{        )�P	W�}�k��A��*
+
+time/fps �MD̺�'       ��F	W�}�k��A��*
+
+train/approx_kl���6���+       ��K	W�}�k��A��*
+
+train/clip_fraction    ���0(       �pJ	W�}�k��A��*
+
+train/clip_range��L>�X�*       ����	W�}�k��A��*
+
+train/entropy_loss��������0       ���_	W�}�k��A��*!
+
+train/explained_variance    �YV�+       ��K	W�}�k��A��*
+
+train/learning_rateRI�9��"       x=�	W�}�k��A��*
+
+
+train/loss�6Jk i2       $V�	W�}�k��A��*#
+!
+train/policy_gradient_loss��}�`���$       B+�M	W�}�k��A��*
+
+train/reward����϶(       �pJ	W�}�k��A��*
+
+train/value_loss�C�J���        )�P	�P��k��A��*
+
+time/fps @MDi��W'       ��F	�c��k��A��*
+
+train/approx_klDЕ7%8+       ��K	�c��k��A��*
+
+train/clip_fraction    �V(       �pJ	�c��k��A��*
+
+train/clip_range��L>��[*       ����	�c��k��A��*
+
+train/entropy_lossÛ���V�0       ���_	�c��k��A��*!
+
+train/explained_variance    ���+       ��K	�c��k��A��*
+
+train/learning_rateRI�9�`;�"       x=�	�c��k��A��*
+
+
+train/loss13gI�P�N2       $V�	�c��k��A��*#
+!
+train/policy_gradient_loss6�鷄�:I$       B+�M	�c��k��A��*
+
+train/reward�%�B���(       �pJ	�c��k��A��*
+
+train/value_lossh�I:��|        )�P		X��k��A�� *
+
+time/fps @MD��#'       ��F		X��k��A�� *
+
+train/approx_klT2�6F�3�+       ��K		X��k��A�� *
+
+train/clip_fraction    ��U�(       �pJ		X��k��A�� *
+
+train/clip_range��L>��*       ����		X��k��A�� *
+
+train/entropy_lossʛ���L�?0       ���_		X��k��A�� *!
+
+train/explained_variance  �3�4�+       ��K		X��k��A�� *
+
+train/learning_rateRI�9��W"       x=�		X��k��A�� *
+
+
+train/lossK-zI
+�<2       $V�		X��k��A�� *#
+!
+train/policy_gradient_loss�簶n��$       B+�M		X��k��A�� *
+
+train/reward�WCP�8(       �pJ		X��k��A�� *
+
+train/value_loss*YJ7	�        )�P	�k� l��A��!*
+
+time/fps @MD'�l�'       ��F	�k� l��A��!*
+
+train/approx_klG��6����+       ��K	�k� l��A��!*
+
+train/clip_fraction    �GA�(       �pJ	�k� l��A��!*
+
+train/clip_range��L>ϒ�k*       ����	�k� l��A��!*
+
+train/entropy_loss�������0       ���_	�k� l��A��!*!
+
+train/explained_variance  @4Ѫ�"+       ��K	�k� l��A��!*
+
+train/learning_rateRI�9�"       x=�	�k� l��A��!*
+
+
+train/loss�K�&�C2       $V�	�k� l��A��!*#
+!
+train/policy_gradient_loss��<�$       B+�M	�k� l��A��!*
+
+train/reward�[E���t�(       �pJ	�k� l��A��!*
+
+train/value_loss�d�K�t8>        )�P	y��l��A��!*
+
+time/fps @MD�S'       ��F	y��l��A��!*
+
+train/approx_klu$�4��+       ��K	y��l��A��!*
+
+train/clip_fraction    T�B�(       �pJ	y��l��A��!*
+
+train/clip_range��L>L�E*       ����	y��l��A��!*
+
+train/entropy_loss嚌��W�0       ���_	y��l��A��!*!
+
+train/explained_variance    �\�7+       ��K	y��l��A��!*
+
+train/learning_rateRI�9I�ou"       x=�	y��l��A��!*
+
+
+train/loss���Iq�U�2       $V�	y��l��A��!*#
+!
+train/policy_gradient_loss�W%7V�$       B+�M	y��l��A��!*
+
+train/reward?>[B�ޝ(       �pJ	y��l��A��!*
+
+train/value_loss�;J>y        )�P	� l��A��"*
+
+time/fps @MD�,+>'       ��F	� l��A��"*
+
+train/approx_kl�8�2z$+       ��K	� l��A��"*
+
+train/clip_fraction    �o�5(       �pJ	� l��A��"*
+
+train/clip_range��L>,d�*       ����	� l��A��"*
+
+train/entropy_loss@����t�0       ���_	� l��A��"*!
+
+train/explained_variance    ��є+       ��K	� l��A��"*
+
+train/learning_rateRI�9�Vʹ"       x=�	� l��A��"*
+
+
+train/loss�OH�N2       $V�	� l��A��"*#
+!
+train/policy_gradient_loss')�λ�>$       B+�M	� l��A��"*
+
+train/reward��oB�w��(       �pJ	� l��A��"*
+
+train/value_loss�E�Hj7��        )�P	2�
+l��A��#*
+
+time/fps @MDJm�'       ��F	2�
+l��A��#*
+
+train/approx_kl*!�7�r�,+       ��K	2�
+l��A��#*
+
+train/clip_fraction    ��(       �pJ	2�
+l��A��#*
+
+train/clip_range��L>�«Y*       ����	2�
+l��A��#*
+
+train/entropy_loss	���ڑ��0       ���_	2�
+l��A��#*!
+
+train/explained_variance    ��ʇ+       ��K	2�
+l��A��#*
+
+train/learning_rateRI�9�H�"       x=�	2�
+l��A��#*
+
+
+train/loss�G[Ig\�2       $V�	2�
+l��A��#*#
+!
+train/policy_gradient_loss���A�� $       B+�M	2�
+l��A��#*
+
+train/reward�|#C:�I(       �pJ	2�
+l��A��#*
+
+train/value_loss�.�Iv�]        )�P	��7
+l��A��#*
+
+time/fps  MDH�y'       ��F	��7
+l��A��#*
+
+train/approx_klJ�7���+       ��K	��7
+l��A��#*
+
+train/clip_fraction    a�)(       �pJ	��7
+l��A��#*
+
+train/clip_range��L>K>�<*       ����	��7
+l��A��#*
+
+train/entropy_loss;���9���0       ���_	��7
+l��A��#*!
+
+train/explained_variance    �WS`+       ��K	��7
+l��A��#*
+
+train/learning_rateRI�9\8դ"       x=�	��7
+l��A��#*
+
+
+train/loss�E�Iʽ�2       $V�	��7
+l��A��#*#
+!
+train/policy_gradient_lossO4\�˲p9$       B+�M	��7
+l��A��#*
+
+train/reward54�B� 8(       �pJ	��7
+l��A��#*
+
+train/value_lossx�0J�Q־        )�P	\~Ul��A��$*
+
+time/fps  MD%�AZ'       ��F	\~Ul��A��$*
+
+train/approx_kl[�7�=�+       ��K	\~Ul��A��$*
+
+train/clip_fraction    �M�s(       �pJ	\~Ul��A��$*
+
+train/clip_range��L>.M��*       ����	\~Ul��A��$*
+
+train/entropy_loss�������/0       ���_	\~Ul��A��$*!
+
+train/explained_variance    �!�+       ��K	\~Ul��A��$*
+
+train/learning_rateRI�9�t��"       x=�	\~Ul��A��$*
+
+
+train/loss.5�J<đ�2       $V�	\~Ul��A��$*#
+!
+train/policy_gradient_loss⣝���Ӝ$       B+�M	\~Ul��A��$*
+
+train/rewardQ��p��(       �pJ	\~Ul��A��$*
+
+train/value_loss>!KM�B        )�P	��ol��A��$*
+
+time/fps  MD1��9'       ��F	��ol��A��$*
+
+train/approx_kl��N9a��\+       ��K	��ol��A��$*
+
+train/clip_fraction    @G�(       �pJ	��ol��A��$*
+
+train/clip_range��L>9=�*       ����	��ol��A��$*
+
+train/entropy_loss����#}�0       ���_	��ol��A��$*!
+
+train/explained_variance    ^Ϋ+       ��K	��ol��A��$*
+
+train/learning_rateRI�9=�k"       x=�	��ol��A��$*
+
+
+train/loss8�HBd�2       $V�	��ol��A��$*#
+!
+train/policy_gradient_loss��}�ew4�$       B+�M	��ol��A��$*
+
+train/rewardf +��զ(       �pJ	��ol��A��$*
+
+train/value_loss
+T,I��Ax        )�P	�}l��A��%*
+
+time/fps  MD���%'       ��F	�}l��A��%*
+
+train/approx_klQi<5&��A+       ��K	�}l��A��%*
+
+train/clip_fraction    ��Lp(       �pJ	�}l��A��%*
+
+train/clip_range��L>��8L*       ����	�}l��A��%*
+
+train/entropy_lossK���S��0       ���_	�}l��A��%*!
+
+train/explained_variance  �3z�eV+       ��K	�}l��A��%*
+
+train/learning_rateRI�9��i�"       x=�	�}l��A��%*
+
+
+train/loss�v�I$3}92       $V�	�}l��A��%*#
+!
+train/policy_gradient_lossA����I�$       B+�M	�}l��A��%*
+
+train/reward��>�Ri�m(       �pJ	�}l��A��%*
+
+train/value_losseDJ>��        )�P	���l��A��&*
+
+time/fps  MD�s�'       ��F	���l��A��&*
+
+train/approx_kl*�\5%�+       ��K	���l��A��&*
+
+train/clip_fraction    �6b�(       �pJ	���l��A��&*
+
+train/clip_range��L>�$�*       ����	���l��A��&*
+
+train/entropy_loss@�������0       ���_	���l��A��&*!
+
+train/explained_variance    =ܷl+       ��K	���l��A��&*
+
+train/learning_rateRI�9эvS"       x=�	���l��A��&*
+
+
+train/lossz�NJ��&2       $V�	���l��A��&*#
+!
+train/policy_gradient_loss����dp�J$       B+�M	���l��A��&*
+
+train/reward�ï`��(       �pJ	���l��A��&*
+
+train/value_lossK��J;+��        )�P	�2�l��A��&*
+
+time/fps  MD��'       ��F	�2�l��A��&*
+
+train/approx_kl�e6B�kW+       ��K	�2�l��A��&*
+
+train/clip_fraction    d	�Q(       �pJ	�2�l��A��&*
+
+train/clip_range��L>���*       ����	�2�l��A��&*
+
+train/entropy_loss���C� O0       ���_	�2�l��A��&*!
+
+train/explained_variance    g��+       ��K	�2�l��A��&*
+
+train/learning_rateRI�9Y���"       x=�	�2�l��A��&*
+
+
+train/loss��EJ8�2       $V�	�2�l��A��&*#
+!
+train/policy_gradient_loss$�a�t��$       B+�M	�2�l��A��&*
+
+train/reward`?WÝ#q�(       �pJ	�2�l��A��&*
+
+train/value_loss���J;
+�        )�P	�>�l��A��'*
+
+time/fps  MD��E'       ��F	�>�l��A��'*
+
+train/approx_kl^�76,��+       ��K	�>�l��A��'*
+
+train/clip_fraction    =*�(       �pJ	�>�l��A��'*
+
+train/clip_range��L>���*       ����	�>�l��A��'*
+
+train/entropy_loss��������0       ���_	�>�l��A��'*!
+
+train/explained_variance  �3��z�+       ��K	�>�l��A��'*
+
+train/learning_rateRI�9@��"       x=�	�>�l��A��'*
+
+
+train/loss�t�J�xH�2       $V�	�>�l��A��'*#
+!
+train/policy_gradient_loss�@�8D��$       B+�M	�>�l��A��'*
+
+train/reward
+a�ÆԱ2(       �pJ	�>�l��A��'*
+
+train/value_lossۄ9K'��        )�P	���"l��A��'*
+
+time/fps  MD	���'       ��F	���"l��A��'*
+
+train/approx_kl�Zc4�i�+       ��K	���"l��A��'*
+
+train/clip_fraction    mȑ�(       �pJ	���"l��A��'*
+
+train/clip_range��L>�o�$*       ����	���"l��A��'*
+
+train/entropy_loss�����T��0       ���_	���"l��A��'*!
+
+train/explained_variance  �4=HC�+       ��K	���"l��A��'*
+
+train/learning_rateRI�9p�T"       x=�	���"l��A��'*
+
+
+train/loss���J���2       $V�	���"l��A��'*#
+!
+train/policy_gradient_loss��_5��Qv$       B+�M	���"l��A��'*
+
+train/reward��ÃL�A(       �pJ	���"l��A��'*
+
+train/value_loss��OK�Xx        )�P	�&l��A��(*
+
+time/fps �LD�3Ar'       ��F	�&l��A��(*
+
+train/approx_kl�@�5I]��+       ��K	�&l��A��(*
+
+train/clip_fraction    ���(       �pJ	�&l��A��(*
+
+train/clip_range��L>9b*       ����	�&l��A��(*
+
+train/entropy_loss����q�+0       ���_	�&l��A��(*!
+
+train/explained_variance  @4./'+       ��K	�&l��A��(*
+
+train/learning_rateRI�9
+�h"       x=�	�&l��A��(*
+
+
+train/lossmLKD��2       $V�	�&l��A��(*#
+!
+train/policy_gradient_loss."��c�v$       B+�M	�&l��A��(*
+
+train/reward&@��8��(       �pJ	�&l��A��(*
+
+train/value_loss?�K���        )�P	l)l��A��)*
+
+time/fps �LDΕ93'       ��F	l)l��A��)*
+
+train/approx_kl�	24�e+       ��K	l)l��A��)*
+
+train/clip_fraction    �N�(       �pJ	l)l��A��)*
+
+train/clip_range��L>kԎx*       ����	l)l��A��)*
+
+train/entropy_loss(�����9�0       ���_	l)l��A��)*!
+
+train/explained_variance    �@B+       ��K	l)l��A��)*
+
+train/learning_rateRI�9�w��"       x=�	l)l��A��)*
+
+
+train/loss��#K:��2       $V�	l)l��A��)*#
+!
+train/policy_gradient_loss1�$5�{��$       B+�M	l)l��A��)*
+
+train/reward���n^��(       �pJ	l)l��A��)*
+
+train/value_loss�;�Kغ^G        )�P	P(@,l��A��)*
+
+time/fps �LDʃi�'       ��F	P(@,l��A��)*
+
+train/approx_kl-n�3�G+       ��K	P(@,l��A��)*
+
+train/clip_fraction    �,b�(       �pJ	P(@,l��A��)*
+
+train/clip_range��L>3���*       ����	P(@,l��A��)*
+
+train/entropy_losst���柌�0       ���_	P(@,l��A��)*!
+
+train/explained_variance  `5�nEp+       ��K	P(@,l��A��)*
+
+train/learning_rateRI�9}�v3"       x=�	P(@,l��A��)*
+
+
+train/loss�K�*H2       $V�	P(@,l��A��)*#
+!
+train/policy_gradient_loss���5�t�$       B+�M	P(@,l��A��)*
+
+train/reward�?B��0��(       �pJ	P(@,l��A��)*
+
+train/value_loss�T�K��	�        )�P	�]/l��A�**
+
+time/fps �LDL�;'       ��F	�]/l��A�**
+
+train/approx_kl�F%7����+       ��K	�]/l��A�**
+
+train/clip_fraction    �O�(       �pJ	�]/l��A�**
+
+train/clip_range��L>Y�Qn*       ����	�]/l��A�**
+
+train/entropy_loss����F��0       ���_	�]/l��A�**!
+
+train/explained_variance    u�j�+       ��K	4]/l��A�**
+
+train/learning_rateRI�9t#�&"       x=�	4]/l��A�**
+
+
+train/loss*d^J/ �#2       $V�	4]/l��A�**#
+!
+train/policy_gradient_loss"���b$       B+�M	4]/l��A�**
+
+train/reward�ޞ��A0�(       �pJ	4]/l��A�**
+
+train/value_loss�M�J줲B        )�P	Ə|2l��A��+*
+
+time/fps �LD��J'       ��F	,�|2l��A��+*
+
+train/approx_kl�!2:,��+       ��K	,�|2l��A��+*
+
+train/clip_fraction    d��P(       �pJ	,�|2l��A��+*
+
+train/clip_range��L>���[*       ����	,�|2l��A��+*
+
+train/entropy_loss`���w��0       ���_	,�|2l��A��+*!
+
+train/explained_variance    -�%�+       ��K	,�|2l��A��+*
+
+train/learning_rateRI�9Yͅ�"       x=�	,�|2l��A��+*
+
+
+train/loss�8�G��|X2       $V�	,�|2l��A��+*#
+!
+train/policy_gradient_lossq빲HE$       B+�M	,�|2l��A��+*
+
+train/rewardt��F�(       �pJ	,�|2l��A��+*
+
+train/value_losspy=Hq-O�        )�P	0�5l��A��+*
+
+time/fps �LD�B'       ��F	0�5l��A��+*
+
+train/approx_klAn�7��={+       ��K	0�5l��A��+*
+
+train/clip_fraction    > ��(       �pJ	0�5l��A��+*
+
+train/clip_range��L>��+�*       ����	0�5l��A��+*
+
+train/entropy_loss1w��?~	0       ���_	0�5l��A��+*!
+
+train/explained_variance    �Oſ+       ��K	0�5l��A��+*
+
+train/learning_rateRI�9C��)"       x=�	0�5l��A��+*
+
+
+train/lossJETIa>�2       $V�	0�5l��A��+*#
+!
+train/policy_gradient_loss�$���D�$       B+�M	0�5l��A��+*
+
+train/reward���A���(       �pJ	0�5l��A��+*
+
+train/value_lossz	�I�I�        )�P	|K�8l��A��,*
+
+time/fps �LD�xc�'       ��F	|K�8l��A��,*
+
+train/approx_kl1w7�y�+       ��K	|K�8l��A��,*
+
+train/clip_fraction    :��(       �pJ	|K�8l��A��,*
+
+train/clip_range��L>R
+~�*       ����	|K�8l��A��,*
+
+train/entropy_lossOx���<Y�0       ���_	|K�8l��A��,*!
+
+train/explained_variance    ��S�+       ��K	|K�8l��A��,*
+
+train/learning_rateRI�9
+���"       x=�	|K�8l��A��,*
+
+
+train/lossb�I�*�G2       $V�	|K�8l��A��,*#
+!
+train/policy_gradient_loss?i�����$       B+�M	|K�8l��A��,*
+
+train/reward�:$Su:(       �pJ	|K�8l��A��,*
+
+train/value_loss\�2J�F        )�P	G��;l��A��,*
+
+time/fps �LD��a'       ��F	G��;l��A��,*
+
+train/approx_kl�!9z�+       ��K	G��;l��A��,*
+
+train/clip_fraction    )��T(       �pJ	G��;l��A��,*
+
+train/clip_range��L>h=*       ����	G��;l��A��,*
+
+train/entropy_loss�~���0       ���_	G��;l��A��,*!
+
+train/explained_variance    ���+       ��K	G��;l��A��,*
+
+train/learning_rateRI�94��"       x=�	G��;l��A��,*
+
+
+train/loss�EH�L�2       $V�	G��;l��A��,*#
+!
+train/policy_gradient_loss���Ex$       B+�M	G��;l��A��,*
+
+train/rewardO�{[(       �pJ	G��;l��A��,*
+
+train/value_lossI��H�"~r        )�P	e?l��A��-*
+
+time/fps �LD��'       ��F	e?l��A��-*
+
+train/approx_kl1O_8��+       ��K	e?l��A��-*
+
+train/clip_fraction    0�	/(       �pJ	e?l��A��-*
+
+train/clip_range��L>t�~x*       ����	e?l��A��-*
+
+train/entropy_lossR~���n��0       ���_	e?l��A��-*!
+
+train/explained_variance  �3y��+       ��K	e?l��A��-*
+
+train/learning_rateRI�9-%-�"       x=�	e?l��A��-*
+
+
+train/loss�s�G���2       $V�	e?l��A��-*#
+!
+train/policy_gradient_loss����f;$       B+�M	e?l��A��-*
+
+train/reward@�����:P(       �pJ	e?l��A��-*
+
+train/value_loss��^H�o         )�P	|�0Bl��A��.*
+
+time/fps �LD��(F'       ��F	|�0Bl��A��.*
+
+train/approx_kl�\�9F#:U+       ��K	|�0Bl��A��.*
+
+train/clip_fraction    �i��(       �pJ	|�0Bl��A��.*
+
+train/clip_range��L>�6�*       ����	|�0Bl��A��.*
+
+train/entropy_lossdq��2�v0       ���_	|�0Bl��A��.*!
+
+train/explained_variance   ���+       ��K	|�0Bl��A��.*
+
+train/learning_rateRI�9�U^"       x=�	|�0Bl��A��.*
+
+
+train/lossیeG�>J!2       $V�	|�0Bl��A��.*#
+!
+train/policy_gradient_loss��\�m�$       B+�M	|�0Bl��A��.*
+
+train/reward�S�A">Ֆ(       �pJ	|�0Bl��A��.*
+
+train/value_loss���G����        )�P	�bYEl��A��.*
+
+time/fps @LD�G�'       ��F	�bYEl��A��.*
+
+train/approx_kl��9e��+       ��K	�bYEl��A��.*
+
+train/clip_fraction    ��5(       �pJ	�bYEl��A��.*
+
+train/clip_range��L>)20*       ����	�bYEl��A��.*
+
+train/entropy_loss܅��y���0       ���_	�bYEl��A��.*!
+
+train/explained_variance   4OTW�+       ��K	�bYEl��A��.*
+
+train/learning_rateRI�9��/�"       x=�	�bYEl��A��.*
+
+
+train/loss
+xH���2       $V�	�bYEl��A��.*#
+!
+train/policy_gradient_loss����p�$       B+�M	�bYEl��A��.*
+
+train/rewardH`@C4bqN(       �pJ	�bYEl��A��.*
+
+train/value_loss>
+�H��M�        )�P	6��Hl��A�/*
+
+time/fps @LDrhM�'       ��F	6��Hl��A�/*
+
+train/approx_kl    /�˻+       ��K	6��Hl��A�/*
+
+train/clip_fraction    ��<(       �pJ	6��Hl��A�/*
+
+train/clip_range��L>&�P*       ����	6��Hl��A�/*
+
+train/entropy_loss�������0       ���_	6��Hl��A�/*!
+
+train/explained_variance    f^J�+       ��K	6��Hl��A�/*
+
+train/learning_rateRI�9�Ŷ�"       x=�	6��Hl��A�/*
+
+
+train/loss�y�Ib���2       $V�	6��Hl��A�/*#
+!
+train/policy_gradient_loss��l�r�$       B+�M	6��Hl��A�/*
+
+train/rewardA�WB��'�(       �pJ	6��Hl��A�/*
+
+train/value_loss쑕J
+�bO        )�P	u%�Kl��A��/*
+
+time/fps @LD3�'       ��F	u%�Kl��A��/*
+
+train/approx_kl܄M3D�_+       ��K	u%�Kl��A��/*
+
+train/clip_fraction    �;�X(       �pJ	u%�Kl��A��/*
+
+train/clip_range��L>`��H*       ����	u%�Kl��A��/*
+
+train/entropy_losss�����x�0       ���_	u%�Kl��A��/*!
+
+train/explained_variance   �H��+       ��K	u%�Kl��A��/*
+
+train/learning_rateRI�9Ӥ��"       x=�	u%�Kl��A��/*
+
+
+train/loss;�J|5q�2       $V�	u%�Kl��A��/*#
+!
+train/policy_gradient_lossd���iD�$       B+�M	u%�Kl��A��/*
+
+train/reward���[$(       �pJ	u%�Kl��A��/*
+
+train/value_lossĤ�Jj�&#        )�P	���Nl��A��0*
+
+time/fps @LDJ��'       ��F	���Nl��A��0*
+
+train/approx_kl?x8>b��+       ��K	���Nl��A��0*
+
+train/clip_fraction    �O�_(       �pJ	���Nl��A��0*
+
+train/clip_range��L>F:d3*       ����	���Nl��A��0*
+
+train/entropy_lossi������0       ���_	���Nl��A��0*!
+
+train/explained_variance   �j��(+       ��K	���Nl��A��0*
+
+train/learning_rateRI�9{x��"       x=�	���Nl��A��0*
+
+
+train/loss���G]��2       $V�	���Nl��A��0*#
+!
+train/policy_gradient_lossu� �ݜ�N$       B+�M	���Nl��A��0*
+
+train/reward��B��-+(       �pJ	���Nl��A��0*
+
+train/value_lossÒHG�        )�P	�b�Ql��A��1*
+
+time/fps  LD���t'       ��F	�b�Ql��A��1*
+
+train/approx_kl���5�:�+       ��K	�b�Ql��A��1*
+
+train/clip_fraction    !�ګ(       �pJ	�b�Ql��A��1*
+
+train/clip_range��L>�
+*       ����	�b�Ql��A��1*
+
+train/entropy_loss&����5�0       ���_	�b�Ql��A��1*!
+
+train/explained_variance    #�)+       ��K	�b�Ql��A��1*
+
+train/learning_rateRI�9��/"       x=�	�b�Ql��A��1*
+
+
+train/lossL�[I]-��2       $V�	�b�Ql��A��1*#
+!
+train/policy_gradient_loss����ƃ�D$       B+�M	�b�Ql��A��1*
+
+train/reward���C!�6�(       �pJ	�b�Ql��A��1*
+
+train/value_loss��I��        )�P	�;Ul��A��1*
+
+time/fps  LD*ݓ'       ��F	�;Ul��A��1*
+
+train/approx_klh Z2�>w�+       ��K	�;Ul��A��1*
+
+train/clip_fraction    �x0(       �pJ	�;Ul��A��1*
+
+train/clip_range��L>��XD*       ����	�;Ul��A��1*
+
+train/entropy_loss�{r��0       ���_	�;Ul��A��1*!
+
+train/explained_variance    �A��+       ��K	�;Ul��A��1*
+
+train/learning_rateRI�9zO0�"       x=�	�;Ul��A��1*
+
+
+train/loss�usK䎮�2       $V�	�;Ul��A��1*#
+!
+train/policy_gradient_loss6<Z4B �A$       B+�M	�;Ul��A��1*
+
+train/reward{��C�/(       �pJ	�;Ul��A��1*
+
+train/value_loss�\�K��֟        )�P	֥5Xl��A��2*
+
+time/fps  LD�<:'       ��F	֥5Xl��A��2*
+
+train/approx_klb�4"ܘ+       ��K	֥5Xl��A��2*
+
+train/clip_fraction    �n��(       �pJ	֥5Xl��A��2*
+
+train/clip_range��L>/�]*       ����	֥5Xl��A��2*
+
+train/entropy_lossᗌ��@W0       ���_	֥5Xl��A��2*!
+
+train/explained_variance    �U�+       ��K	֥5Xl��A��2*
+
+train/learning_rateRI�9�U�Q"       x=�	֥5Xl��A��2*
+
+
+train/loss@�KZv2       $V�	֥5Xl��A��2*#
+!
+train/policy_gradient_loss�@���Z@3$       B+�M	֥5Xl��A��2*
+
+train/reward�Q�C�86(       �pJ	֥5Xl��A��2*
+
+train/value_loss�<dL?��        )�P	�Y[l��A��3*
+
+time/fps  LD/�ܿ'       ��F	�Y[l��A��3*
+
+train/approx_kln�5s��)+       ��K	�Y[l��A��3*
+
+train/clip_fraction    Zy*�(       �pJ	�Y[l��A��3*
+
+train/clip_range��L>�}>�*       ����	�Y[l��A��3*
+
+train/entropy_loss�����t�!0       ���_	�Y[l��A��3*!
+
+train/explained_variance   �zTr+       ��K	�Y[l��A��3*
+
+train/learning_rateRI�9gRF"       x=�	�Y[l��A��3*
+
+
+train/loss��L����2       $V�	�Y[l��A��3*#
+!
+train/policy_gradient_loss[ж��e�$       B+�M	�Y[l��A��3*
+
+train/rewardV��C����(       �pJ	�Y[l��A��3*
+
+train/value_loss�n�L<A        )�P	by|^l��A��3*
+
+time/fps  LD���'       ��F	�z|^l��A��3*
+
+train/approx_kl1_,6λan+       ��K	�z|^l��A��3*
+
+train/clip_fraction    /��Z(       �pJ	�z|^l��A��3*
+
+train/clip_range��L>��ۃ*       ����	�z|^l��A��3*
+
+train/entropy_loss����7�0       ���_	�z|^l��A��3*!
+
+train/explained_variance    .��+       ��K	�z|^l��A��3*
+
+train/learning_rateRI�921��"       x=�	�z|^l��A��3*
+
+
+train/loss�K=�;N2       $V�	�z|^l��A��3*#
+!
+train/policy_gradient_loss����d��$       B+�M	�z|^l��A��3*
+
+train/reward��C�
+](       �pJ	�z|^l��A��3*
+
+train/value_loss� L-�        )�P	St�al��A�4*
+
+time/fps  LD>
+�'       ��F	St�al��A�4*
+
+train/approx_klQ��4'4ߚ+       ��K	St�al��A�4*
+
+train/clip_fraction    �e(       �pJ	St�al��A�4*
+
+train/clip_range��L>�"�<*       ����	St�al��A�4*
+
+train/entropy_loss1������0       ���_	St�al��A�4*!
+
+train/explained_variance    Wӽ=+       ��K	St�al��A�4*
+
+train/learning_rateRI�9����"       x=�	St�al��A�4*
+
+
+train/loss���K؀��2       $V�	St�al��A�4*#
+!
+train/policy_gradient_loss�j����m$       B+�M	St�al��A�4*
+
+train/rewardh��C�IÈ(       �pJ	St�al��A�4*
+
+train/value_lossdBL3V��        )�P	�U�dl��A��4*
+
+time/fps  LD8a'       ��F	�U�dl��A��4*
+
+train/approx_kl�*7���T+       ��K	�U�dl��A��4*
+
+train/clip_fraction    �l�(       �pJ	�U�dl��A��4*
+
+train/clip_range��L>�vm*       ����	�U�dl��A��4*
+
+train/entropy_loss8���G�0       ���_	�U�dl��A��4*!
+
+train/explained_variance   4X0s�+       ��K	�U�dl��A��4*
+
+train/learning_rateRI�9
+_H$"       x=�	�U�dl��A��4*
+
+
+train/losspuNK�Z82       $V�	�U�dl��A��4*#
+!
+train/policy_gradient_lossgp����B�$       B+�M	�U�dl��A��4*
+
+train/reward�*C4�r.(       �pJ	�U�dl��A��4*
+
+train/value_loss�6�KǠvJ        )�P	��gl��A��5*
+
+time/fps �KDn嬑'       ��F	��gl��A��5*
+
+train/approx_klQI�5G
+P+       ��K	��gl��A��5*
+
+train/clip_fraction    [
+�(       �pJ	��gl��A��5*
+
+train/clip_range��L>�'{&*       ����	��gl��A��5*
+
+train/entropy_loss_����F�0       ���_	��gl��A��5*!
+
+train/explained_variance    �ΐ+       ��K	��gl��A��5*
+
+train/learning_rateRI�9#�)4"       x=�	��gl��A��5*
+
+
+train/loss��K=A)�2       $V�	��gl��A��5*#
+!
+train/policy_gradient_loss�䯶Ve��$       B+�M	��gl��A��5*
+
+train/reward`#CP�6�(       �pJ	��gl��A��5*
+
+train/value_loss���KX&�        )�P	�]�jl��A��6*
+
+time/fps �KDp��h'       ��F	�]�jl��A��6*
+
+train/approx_kl�d6�3ސ+       ��K	�]�jl��A��6*
+
+train/clip_fraction    nyj(       �pJ	�]�jl��A��6*
+
+train/clip_range��L>����*       ����	�]�jl��A��6*
+
+train/entropy_lossR���ݶ�0       ���_	�]�jl��A��6*!
+
+train/explained_variance   ��T�+       ��K	�]�jl��A��6*
+
+train/learning_rateRI�9sRN�"       x=�	�]�jl��A��6*
+
+
+train/loss*dmJPA�{2       $V�	�]�jl��A��6*#
+!
+train/policy_gradient_loss����Az$       B+�M	�]�jl��A��6*
+
+train/reward1dcB|/NP(       �pJ	�]�jl��A��6*
+
+train/value_lossh%�J���"        )�P	�$nl��A��6*
+
+time/fps �KD�E�'       ��F	�$nl��A��6*
+
+train/approx_kl���7	�~�+       ��K	�$nl��A��6*
+
+train/clip_fraction    ߁w(       �pJ	�$nl��A��6*
+
+train/clip_range��L>���*       ����	�$nl��A��6*
+
+train/entropy_loss�����f�0       ���_	�$nl��A��6*!
+
+train/explained_variance    �9��+       ��K	�$nl��A��6*
+
+train/learning_rateRI�9$U�"       x=�	�$nl��A��6*
+
+
+train/loss�I�)�k2       $V�	�$nl��A��6*#
+!
+train/policy_gradient_lossSG>�'�0�$       B+�M	�$nl��A��6*
+
+train/rewardC�C�[�(       �pJ	�$nl��A��6*
+
+train/value_loss��J��%        )�P	�Uql��A��7*
+
+time/fps �KD��nW'       ��F	�Uql��A��7*
+
+train/approx_kln��6!�+       ��K	�Uql��A��7*
+
+train/clip_fraction    �/��(       �pJ	�Uql��A��7*
+
+train/clip_range��L>�o�*       ����	�Uql��A��7*
+
+train/entropy_lossz���ܽ`�0       ���_	�Uql��A��7*!
+
+train/explained_variance    ��$+       ��K	�Uql��A��7*
+
+train/learning_rateRI�9ٖ�"       x=�	�Uql��A��7*
+
+
+train/loss��J����2       $V�	�Uql��A��7*#
+!
+train/policy_gradient_loss\7Ƕ��!�$       B+�M	�Uql��A��7*
+
+train/reward�)C���~(       �pJ	�Uql��A��7*
+
+train/value_loss��Jo��        )�P	2тtl��A��7*
+
+time/fps �KD�l^&'       ��F	2тtl��A��7*
+
+train/approx_klT�;4z��3+       ��K	2тtl��A��7*
+
+train/clip_fraction    Wz�(       �pJ	2тtl��A��7*
+
+train/clip_range��L>�U*       ����	2тtl��A��7*
+
+train/entropy_lossі�����0       ���_	2тtl��A��7*!
+
+train/explained_variance  @4�rް+       ��K	2тtl��A��7*
+
+train/learning_rateRI�9JQ�/"       x=�	2тtl��A��7*
+
+
+train/loss�:�J�5Ħ2       $V�	2тtl��A��7*#
+!
+train/policy_gradient_losszl�5�*>$       B+�M	2тtl��A��7*
+
+train/rewardW�
+C����(       �pJ	2тtl��A��7*
+
+train/value_loss�\WK�*SX        )�P	̬wl��A��8*
+
+time/fps �KD�m��'       ��F	̬wl��A��8*
+
+train/approx_kl
+� 6�(D+       ��K	̬wl��A��8*
+
+train/clip_fraction    �W�~(       �pJ	̬wl��A��8*
+
+train/clip_range��L>X�5`*       ����	̬wl��A��8*
+
+train/entropy_lossJ�����u0       ���_	̬wl��A��8*!
+
+train/explained_variance   4���+       ��K	̬wl��A��8*
+
+train/learning_rateRI�9�p��"       x=�	̬wl��A��8*
+
+
+train/loss��J�4�2       $V�	̬wl��A��8*#
+!
+train/policy_gradient_lossV�ܶ�W�$       B+�M	̬wl��A��8*
+
+train/reward?�C��?(       �pJ	̬wl��A��8*
+
+train/value_loss�i*K�@�        )�P	���zl��A��9*
+
+time/fps �KD�V|'       ��F	���zl��A��9*
+
+train/approx_kl���7��o+       ��K	���zl��A��9*
+
+train/clip_fraction    d�u(       �pJ	���zl��A��9*
+
+train/clip_range��L>�aZ*       ����	���zl��A��9*
+
+train/entropy_loss甌�(
+^y0       ���_	���zl��A��9*!
+
+train/explained_variance    �S�+       ��K	���zl��A��9*
+
+train/learning_rateRI�9Y���"       x=�	���zl��A��9*
+
+
+train/lossq�J��2       $V�	���zl��A��9*#
+!
+train/policy_gradient_losslޖ��֗�$       B+�M	���zl��A��9*
+
+train/reward$�zC�H
+q(       �pJ	���zl��A��9*
+
+train/value_lossJ
+K��'K        )�P	~l��A��9*
+
+time/fps �KD"�@'       ��F	~l��A��9*
+
+train/approx_kl$�s68��A+       ��K	h~l��A��9*
+
+train/clip_fraction    �ʽS(       �pJ	h~l��A��9*
+
+train/clip_range��L>`���*       ����	h~l��A��9*
+
+train/entropy_loss���ؿ�`0       ���_	h~l��A��9*!
+
+train/explained_variance  �3ˠ)+       ��K	h~l��A��9*
+
+train/learning_rateRI�9y!��"       x=�	h~l��A��9*
+
+
+train/lossr:�J�bK2       $V�	h~l��A��9*#
+!
+train/policy_gradient_loss�����f�$       B+�M	h~l��A��9*
+
+train/reward�mD�9y�(       �pJ	h~l��A��9*
+
+train/value_loss��xK��;�        )�P	�
+.�l��A��:*
+
+time/fps �KD<�S�'       ��F	H.�l��A��:*
+
+train/approx_kl�<�5�&�
++       ��K	H.�l��A��:*
+
+train/clip_fraction    ԏ�(       �pJ	H.�l��A��:*
+
+train/clip_range��L>��*       ����	H.�l��A��:*
+
+train/entropy_lossٔ��~Qo0       ���_	H.�l��A��:*!
+
+train/explained_variance    ��+       ��K	H.�l��A��:*
+
+train/learning_rateRI�9�8*X"       x=�	H.�l��A��:*
+
+
+train/loss�rL�B�2       $V�	H.�l��A��:*#
+!
+train/policy_gradient_loss��2�S!�-$       B+�M	H.�l��A��:*
+
+train/reward�P�DT���(       �pJ	H.�l��A��:*
+
+train/value_lossH��LoD�b        )�P	/�W�l��A��;*
+
+time/fps �KD���'       ��F	/�W�l��A��;*
+
+train/approx_kl��2�C��+       ��K	/�W�l��A��;*
+
+train/clip_fraction    ����(       �pJ	/�W�l��A��;*
+
+train/clip_range��L>!���*       ����	/�W�l��A��;*
+
+train/entropy_loss╌�T�w�0       ���_	/�W�l��A��;*!
+
+train/explained_variance   ���AR+       ��K	/�W�l��A��;*
+
+train/learning_rateRI�9}9�_"       x=�	/�W�l��A��;*
+
+
+train/lossfMq��2       $V�	/�W�l��A��;*#
+!
+train/policy_gradient_lossN�����\$       B+�M	/�W�l��A��;*
+
+train/reward%u�D;
+��(       �pJ	/�W�l��A��;*
+
+train/value_loss�4�M\6�        )�P	1$~�l��A��;*
+
+time/fps @KD�ߤ'       ��F	1$~�l��A��;*
+
+train/approx_kl��3g��b+       ��K	1$~�l��A��;*
+
+train/clip_fraction    �r(       �pJ	1$~�l��A��;*
+
+train/clip_range��L>Me߻*       ����	1$~�l��A��;*
+
+train/entropy_lossi������=0       ���_	1$~�l��A��;*!
+
+train/explained_variance    �:E+       ��K	1$~�l��A��;*
+
+train/learning_rateRI�9{�F�"       x=�	1$~�l��A��;*
+
+
+train/lossu]AM����2       $V�	1$~�l��A��;*#
+!
+train/policy_gradient_loss}���D0$       B+�M	1$~�l��A��;*
+
+train/reward���D)D�&(       �pJ	1$~�l��A��;*
+
+train/value_loss���Mpہ        )�P	.1��l��A��<*
+
+time/fps @KDf�٤'       ��F	.1��l��A��<*
+
+train/approx_klQ�3����+       ��K	�9��l��A��<*
+
+train/clip_fraction    �r70(       �pJ	�9��l��A��<*
+
+train/clip_range��L>�l*       ����	�9��l��A��<*
+
+train/entropy_lossƖ���E>0       ���_	�9��l��A��<*!
+
+train/explained_variance   �O��w+       ��K	�9��l��A��<*
+
+train/learning_rateRI�9�Y�"       x=�	�9��l��A��<*
+
+
+train/loss5q4M�"ݶ2       $V�	�9��l��A��<*#
+!
+train/policy_gradient_loss}��x*d$       B+�M	�9��l��A��<*
+
+train/reward�YD����(       �pJ	�9��l��A��<*
+
+train/value_lossQ��M�Q �        )�P	T�͍l��A��<*
+
+time/fps @KDX��8'       ��F	T�͍l��A��<*
+
+train/approx_klKG,3ׁ�5+       ��K	T�͍l��A��<*
+
+train/clip_fraction    >��(       �pJ	T�͍l��A��<*
+
+train/clip_range��L>N� *       ����	T�͍l��A��<*
+
+train/entropy_lossʖ���`X0       ���_	T�͍l��A��<*!
+
+train/explained_variance   ��x�+       ��K	T�͍l��A��<*
+
+train/learning_rateRI�9C܌"       x=�	T�͍l��A��<*
+
+
+train/loss�yM�^u2       $V�	T�͍l��A��<*#
+!
+train/policy_gradient_loss����!J,$       B+�M	T�͍l��A��<*
+
+train/reward4$�Dq@��(       �pJ	T�͍l��A��<*
+
+train/value_loss�%�M�7��        )�P	8[��l��A��=*
+
+time/fps @KD�`1'       ��F	8[��l��A��=*
+
+train/approx_klx��2P�3W+       ��K	8[��l��A��=*
+
+train/clip_fraction    ���(       �pJ	8[��l��A��=*
+
+train/clip_range��L>�~�a*       ����	8[��l��A��=*
+
+train/entropy_lossꖌ�*d�)0       ���_	8[��l��A��=*!
+
+train/explained_variance  ��� +       ��K	8[��l��A��=*
+
+train/learning_rateRI�96��"       x=�	8[��l��A��=*
+
+
+train/loss���L C��2       $V�	8[��l��A��=*#
+!
+train/policy_gradient_loss_�L4�5�f$       B+�M	8[��l��A��=*
+
+train/rewardP�0D���(       �pJ	8[��l��A��=*
+
+train/value_loss �{M��Ȧ        )�P	s� �l��A��>*
+
+time/fps @KD����'       ��F	s� �l��A��>*
+
+train/approx_kl�B4�;b�+       ��K	s� �l��A��>*
+
+train/clip_fraction    ���(       �pJ	s� �l��A��>*
+
+train/clip_range��L>[%*       ����	s� �l��A��>*
+
+train/entropy_loss������$L0       ���_	s� �l��A��>*!
+
+train/explained_variance   ��0�+       ��K	s� �l��A��>*
+
+train/learning_rateRI�9�US"       x=�	s� �l��A��>*
+
+
+train/loss���L��H�2       $V�	s� �l��A��>*#
+!
+train/policy_gradient_losso��m�g$       B+�M	s� �l��A��>*
+
+train/reward�t#D�	�?(       �pJ	s� �l��A��>*
+
+train/value_lossF�jMuy}        )�P	�"S�l��A��>*
+
+time/fps  KD"���'       ��F	�"S�l��A��>*
+
+train/approx_kl-n	5r_�+       ��K	�"S�l��A��>*
+
+train/clip_fraction    `�|�(       �pJ	�"S�l��A��>*
+
+train/clip_range��L>^�*       ����	�"S�l��A��>*
+
+train/entropy_lossU������0       ���_	�"S�l��A��>*!
+
+train/explained_variance    s}+�+       ��K	�"S�l��A��>*
+
+train/learning_rateRI�9]`X
+"       x=�	�"S�l��A��>*
+
+
+train/loss�ŋL���2       $V�	�"S�l��A��>*#
+!
+train/policy_gradient_loss�ƶ��l$       B+�M	�"S�l��A��>*
+
+train/rewardIHDk���(       �pJ	�"S�l��A��>*
+
+train/value_loss�M*�97        )�P	]���l��A��?*
+
+time/fps  KD~x��'       ��F	]���l��A��?*
+
+train/approx_kl��"4�A�+       ��K	]���l��A��?*
+
+train/clip_fraction     a��(       �pJ	]���l��A��?*
+
+train/clip_range��L>��t*       ����	]���l��A��?*
+
+train/entropy_loss����o?0       ���_	]���l��A��?*!
+
+train/explained_variance    $�C6+       ��K	]���l��A��?*
+
+train/learning_rateRI�9��m"       x=�	]���l��A��?*
+
+
+train/loss��TLf�ȫ2       $V�	]���l��A��?*#
+!
+train/policy_gradient_loss�*l��*�$       B+�M	]���l��A��?*
+
+train/reward� D��Q(       �pJ	]���l��A��?*
+
+train/value_lossΪ�L�_�X        )�P	�;��l��A��?*
+
+time/fps  KDBǇ'       ��F	�;��l��A��?*
+
+train/approx_kl JT52?�+       ��K	�;��l��A��?*
+
+train/clip_fraction    ,���(       �pJ	�;��l��A��?*
+
+train/clip_range��L>*^�w*       ����	�;��l��A��?*
+
+train/entropy_loss����L0       ���_	�;��l��A��?*!
+
+train/explained_variance    �SL+       ��K	�;��l��A��?*
+
+train/learning_rateRI�91G�"       x=�	�;��l��A��?*
+
+
+train/loss�
+OLGc
+2       $V�	�;��l��A��?*#
+!
+train/policy_gradient_loss~��ܫ	$       B+�M	�;��l��A��?*
+
+train/rewardj,DJ���(       �pJ	�;��l��A��?*
+
+train/value_lossx&�Lk�7�        )�P	�ޠl��A��@*
+
+time/fps  KD1��'       ��F	�ޠl��A��@*
+
+train/approx_klx5K4f5.k+       ��K	�ޠl��A��@*
+
+train/clip_fraction    ��*(       �pJ	�ޠl��A��@*
+
+train/clip_range��L>e��*       ����	�ޠl��A��@*
+
+train/entropy_loss8����]x�0       ���_	�ޠl��A��@*!
+
+train/explained_variance    .(�,+       ��K	�ޠl��A��@*
+
+train/learning_rateRI�9�ݻ�"       x=�	�ޠl��A��@*
+
+
+train/loss]@�L_o��2       $V�	�ޠl��A��@*#
+!
+train/policy_gradient_loss��ԵY2�$       B+�M	�ޠl��A��@*
+
+train/reward�T<D�܁!(       �pJ	�ޠl��A��@*
+
+train/value_lossbrM��         )�P	���l��A��A*
+
+time/fps  KDn�gD'       ��F	���l��A��A*
+
+train/approx_kln��5ߋ	�+       ��K	���l��A��A*
+
+train/clip_fraction    �a((       �pJ	���l��A��A*
+
+train/clip_range��L>Ϡz�*       ����	���l��A��A*
+
+train/entropy_loss䖌����0       ���_	���l��A��A*!
+
+train/explained_variance    �#��+       ��K	���l��A��A*
+
+train/learning_rateRI�9��0�"       x=�	���l��A��A*
+
+
+train/loss�yLg�2       $V�	���l��A��A*#
+!
+train/policy_gradient_loss�Z���$       B+�M	���l��A��A*
+
+train/reward��eDt���(       �pJ	���l��A��A*
+
+train/value_loss_� M"o�        )�P	��6�l��A��A*
+
+time/fps  KD�=�'       ��F	��6�l��A��A*
+
+train/approx_kl��5��G+       ��K	��6�l��A��A*
+
+train/clip_fraction    x>ʁ(       �pJ	��6�l��A��A*
+
+train/clip_range��L>c %T*       ����	��6�l��A��A*
+
+train/entropy_lossۖ��^�T�0       ���_	��6�l��A��A*!
+
+train/explained_variance    ��Y+       ��K	��6�l��A��A*
+
+train/learning_rateRI�9����"       x=�	��6�l��A��A*
+
+
+train/loss�j�L�Eb�2       $V�	��6�l��A��A*#
+!
+train/policy_gradient_loss{������$       B+�M	��6�l��A��A*
+
+train/rewardb�D�5\�(       �pJ	��6�l��A��A*
+
+train/value_lossN7NM����        )�P	G�c�l��A��B*
+
+time/fps �JD֗,'       ��F	G�c�l��A��B*
+
+train/approx_kl�J�3����+       ��K	G�c�l��A��B*
+
+train/clip_fraction    j9�l(       �pJ	G�c�l��A��B*
+
+train/clip_range��L>݉t+*       ����	G�c�l��A��B*
+
+train/entropy_loss����g z0       ���_	G�c�l��A��B*!
+
+train/explained_variance  `��\�+       ��K	G�c�l��A��B*
+
+train/learning_rateRI�9w���"       x=�	G�c�l��A��B*
+
+
+train/loss��Mei��2       $V�	G�c�l��A��B*#
+!
+train/policy_gradient_loss�����E��$       B+�M	G�c�l��A��B*
+
+train/rewardwPTD�ŕ(       �pJ	G�c�l��A��B*
+
+train/value_losset�M k�        )�P	����l��A��C*
+
+time/fps �JDz1��'       ��F	����l��A��C*
+
+train/approx_kl�	�3�P�+       ��K	����l��A��C*
+
+train/clip_fraction    �ڄ�(       �pJ	����l��A��C*
+
+train/clip_range��L>X��I*       ����	����l��A��C*
+
+train/entropy_lossT�����k
+0       ���_	����l��A��C*!
+
+train/explained_variance   ���Sa+       ��K	����l��A��C*
+
+train/learning_rateRI�9y�*"       x=�	����l��A��C*
+
+
+train/loss�7�L޶U2       $V�	����l��A��C*#
+!
+train/policy_gradient_loss�O=���n�$       B+�M	����l��A��C*
+
+train/reward�D�|P�(       �pJ	����l��A��C*
+
+train/value_loss��hM��S�        )�P	�b��l��A��C*
+
+time/fps �JD�L�3'       ��F	�b��l��A��C*
+
+train/approx_kl    Ú��+       ��K	�b��l��A��C*
+
+train/clip_fraction    EB5\(       �pJ	�b��l��A��C*
+
+train/clip_range��L>�KXS*       ����	�b��l��A��C*
+
+train/entropy_loss0�����L<0       ���_	�b��l��A��C*!
+
+train/explained_variance  �4��̇+       ��K	�b��l��A��C*
+
+train/learning_rateRI�98�!�"       x=�	�b��l��A��C*
+
+
+train/lossuM���2       $V�	�b��l��A��C*#
+!
+train/policy_gradient_losspy�4�Ryj$       B+�M	�b��l��A��C*
+
+train/reward�� EYY3(       �pJ	�b��l��A��C*
+
+train/value_loss{�M��a        )�P	�߳l��A��D*
+
+time/fps �JD�0M'       ��F	�߳l��A��D*
+
+train/approx_kl'L04��5�+       ��K	�߳l��A��D*
+
+train/clip_fraction    5l�(       �pJ	�߳l��A��D*
+
+train/clip_range��L>C<p�*       ����	�߳l��A��D*
+
+train/entropy_loss����]��0       ���_	�߳l��A��D*!
+
+train/explained_variance   ��?l�+       ��K	�߳l��A��D*
+
+train/learning_rateRI�9(�HF"       x=�	�߳l��A��D*
+
+
+train/loss�z�M���)2       $V�	�߳l��A��D*#
+!
+train/policy_gradient_loss�����?��$       B+�M	�߳l��A��D*
+
+train/reward��D�a�(       �pJ	�߳l��A��D*
+
+train/value_lossz�|N
+ԩ�        )�P	�l��A��D*
+
+time/fps �JD�f�6'       ��F	�l��A��D*
+
+train/approx_kl[��4	�+       ��K	�l��A��D*
+
+train/clip_fraction    ���U(       �pJ	�l��A��D*
+
+train/clip_range��L>�]-=*       ����	�l��A��D*
+
+train/entropy_lossL���x�0       ���_	�l��A��D*!
+
+train/explained_variance  `��\/c+       ��K	�l��A��D*
+
+train/learning_rateRI�9����"       x=�	�l��A��D*
+
+
+train/loss<N���2       $V�	�l��A��D*#
+!
+train/policy_gradient_loss��	���Z$       B+�M	�l��A��D*
+
+train/reward%�E��v(       �pJ	�l��A��D*
+
+train/value_loss:��N/�U�        )�P	!��l��A��E*
+
+time/fps �JD;ơ�'       ��F	!��l��A��E*
+
+train/approx_kl��}3��ü+       ��K	!��l��A��E*
+
+train/clip_fraction    
+#(       �pJ	!��l��A��E*
+
+train/clip_range��L>ڍ��*       ����	!��l��A��E*
+
+train/entropy_loss���{�p�0       ���_	!��l��A��E*!
+
+train/explained_variance  @4:���+       ��K	!��l��A��E*
+
+train/learning_rateRI�9�,}"       x=�	!��l��A��E*
+
+
+train/loss�s,N���2       $V�	!��l��A��E*#
+!
+train/policy_gradient_loss���y�$       B+�M	!��l��A��E*
+
+train/reward�1 E`�(       �pJ	!��l��A��E*
+
+train/value_loss�&�N��/�        )�P	�nD�l��A��F*
+
+time/fps �JD���'       ��F	=D�l��A��F*
+
+train/approx_kl    �l��+       ��K	=D�l��A��F*
+
+train/clip_fraction    ��/�(       �pJ	=D�l��A��F*
+
+train/clip_range��L>��*       ����	=D�l��A��F*
+
+train/entropy_loss/������x0       ���_	=D�l��A��F*!
+
+train/explained_variance  @4O�++       ��K	=D�l��A��F*
+
+train/learning_rateRI�9�%"       x=�	=D�l��A��F*
+
+
+train/loss�%ON۝�'2       $V�	=D�l��A��F*#
+!
+train/policy_gradient_lossσ�3t%�.$       B+�M	=D�l��A��F*
+
+train/rewardIE��&�(       �pJ	=D�l��A��F*
+
+train/value_lossx �Nb^:l        )�P	.f�l��A��F*
+
+time/fps �JD1Q��'       ��F	�6f�l��A��F*
+
+train/approx_kl4�3@Z7�+       ��K	�6f�l��A��F*
+
+train/clip_fraction    P��
+(       �pJ	�6f�l��A��F*
+
+train/clip_range��L>��Pm*       ����	�6f�l��A��F*
+
+train/entropy_loss���WT �0       ���_	�6f�l��A��F*!
+
+train/explained_variance   4(aw+       ��K	�6f�l��A��F*
+
+train/learning_rateRI�9M+��"       x=�	�6f�l��A��F*
+
+
+train/loss�PNH���2       $V�	�6f�l��A��F*#
+!
+train/policy_gradient_loss^��i��\$       B+�M	�6f�l��A��F*
+
+train/reward��5E}��$(       �pJ	�6f�l��A��F*
+
+train/value_lossw�NK�!        )�P	�:��l��A��G*
+
+time/fps �JDs��g'       ��F	�:��l��A��G*
+
+train/approx_klq�3���^+       ��K	�:��l��A��G*
+
+train/clip_fraction    dJH(       �pJ	�:��l��A��G*
+
+train/clip_range��L>�W�*       ����	�:��l��A��G*
+
+train/entropy_loss����8_0       ���_	�:��l��A��G*!
+
+train/explained_variance    �)�+       ��K	�:��l��A��G*
+
+train/learning_rateRI�9%{n�"       x=�	�:��l��A��G*
+
+
+train/loss�!zN���2       $V�	�:��l��A��G*#
+!
+train/policy_gradient_loss�����D�$       B+�M	�:��l��A��G*
+
+train/rewardeG7E�g��(       �pJ	�:��l��A��G*
+
+train/value_lossݐ�N*6|�        )�P	Н��l��A��G*
+
+time/fps �JD���)'       ��F	Н��l��A��G*
+
+train/approx_kl��t4j��S+       ��K	Н��l��A��G*
+
+train/clip_fraction    ���(       �pJ	Н��l��A��G*
+
+train/clip_range��L>�0g*       ����	Н��l��A��G*
+
+train/entropy_loss��������0       ���_	Н��l��A��G*!
+
+train/explained_variance    �
+͂+       ��K	Н��l��A��G*
+
+train/learning_rateRI�9D�"       x=�	Н��l��A��G*
+
+
+train/lossN��N�5z�2       $V�	Н��l��A��G*#
+!
+train/policy_gradient_lossbL7��Y�/$       B+�M	Н��l��A��G*
+
+train/reward1�WE��E�(       �pJ	Н��l��A��G*
+
+train/value_loss��MO��Z        )�P	SO��l��A��H*
+
+time/fps �JD���'       ��F	SO��l��A��H*
+
+train/approx_kl�4��Z+       ��K	SO��l��A��H*
+
+train/clip_fraction    ���(       �pJ	SO��l��A��H*
+
+train/clip_range��L>��rW*       ����	SO��l��A��H*
+
+train/entropy_lossN���Q=]�0       ���_	SO��l��A��H*!
+
+train/explained_variance    &W�+       ��K	SO��l��A��H*
+
+train/learning_rateRI�9Э�t"       x=�	SO��l��A��H*
+
+
+train/lossV��N�x^�2       $V�	SO��l��A��H*#
+!
+train/policy_gradient_loss����`�	�$       B+�M	SO��l��A��H*
+
+train/reward�;E
+��(       �pJ	SO��l��A��H*
+
+train/value_losst0:OL��
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_2/events.out.tfevents.1724746909.Trading.49200.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_2/events.out.tfevents.1724746909.Trading.49200.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_2/events.out.tfevents.1724746909.Trading.49200.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_2/events.out.tfevents.1724746909.Trading.49200.0	
@@ -0,0 +1,3174 @@
+H       ��H�	�OQ'c��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer��7o       QKD	�ì'c��A�*
+
+time/fps ��D�E�       QKD	GU (c��A�'*
+
+time/fps @�D��O&       sO� 	GU (c��A�'*
+
+train/approx_kl��q;i��*       ����	GU (c��A�'*
+
+train/clip_fraction    �c�'       ��F	GU (c��A�'*
+
+train/clip_range��L>엤�)       7�_ 	GU (c��A�'*
+
+train/entropy_loss�y��m���/       m]P	GU (c��A�'*!
+
+train/explained_variance H�9��I�*       ����	GU (c��A�'*
+
+train/learning_rateRI�9�z��!       {��	GU (c��A�'*
+
+
+train/loss��aG`P��1       ����	GU (c��A�'*#
+!
+train/policy_gradient_loss6ۺ�[O'       ��F	GU (c��A�'*
+
+train/value_loss��Gb�+0       QKD	��(c��A�;*
+
+time/fps  �D����&       sO� 	��(c��A�;*
+
+train/approx_klz��;�g��*       ����	��(c��A�;*
+
+train/clip_fraction    ����'       ��F	��(c��A�;*
+
+train/clip_range��L>��Ym)       7�_ 	��(c��A�;*
+
+train/entropy_loss�Z��̠�x/       m]P	��(c��A�;*!
+
+train/explained_variance p�9�ֿ
+*       ����	��(c��A�;*
+
+train/learning_rateRI�9�7�M!       {��	��(c��A�;*
+
+
+train/loss��G^�A1       ����	��(c��A�;*#
+!
+train/policy_gradient_loss]���T�i'       ��F	��(c��A�;*
+
+train/value_loss��,H��Z1       QKD	�"�(c��A�O*
+
+time/fps @�DOq��&       sO� 	�"�(c��A�O*
+
+train/approx_kl:ɠ:e �*       ����	�"�(c��A�O*
+
+train/clip_fraction    ۳yB'       ��F	�"�(c��A�O*
+
+train/clip_range��L>��q)       7�_ 	�"�(c��A�O*
+
+train/entropy_lossx���EH�/       m]P	�"�(c��A�O*!
+
+train/explained_variance  D:�W�5*       ����	�"�(c��A�O*
+
+train/learning_rateRI�9�
+]�!       {��	�"�(c��A�O*
+
+
+train/loss�ҠH��X-1       ����	�"�(c��A�O*#
+!
+train/policy_gradient_loss �����'       ��F	�"�(c��A�O*
+
+train/value_loss�(IP`       QKD	��o)c��A�c*
+
+time/fps `�D�A4&       sO� 	��o)c��A�c*
+
+train/approx_kl�`j8��5�*       ����	��o)c��A�c*
+
+train/clip_fraction    Ug��'       ��F	��o)c��A�c*
+
+train/clip_range��L>��)       7�_ 	��o)c��A�c*
+
+train/entropy_lossE���5��/       m]P	��o)c��A�c*!
+
+train/explained_variance ����J��*       ����	��o)c��A�c*
+
+train/learning_rateRI�9H�Uo!       {��	��o)c��A�c*
+
+
+train/loss~#�H�SW#1       ����	��o)c��A�c*#
+!
+train/policy_gradient_loss�0�,�R'       ��F	��o)c��A�c*
+
+train/value_loss�I�
+��       QKD	�)�)c��A�w*
+
+time/fps `�D�O[!&       sO� 	�)�)c��A�w*
+
+train/approx_kl�Q:�t*       ����	�)�)c��A�w*
+
+train/clip_fraction    ƒ�'       ��F	�)�)c��A�w*
+
+train/clip_range��L>�L�r)       7�_ 	�)�)c��A�w*
+
+train/entropy_loss���f�۹/       m]P	�)�)c��A�w*!
+
+train/explained_variance �ڸB��*       ����	�)�)c��A�w*
+
+train/learning_rateRI�9"�!       {��	�)�)c��A�w*
+
+
+train/loss�I�H�!k1       ����	�)�)c��A�w*#
+!
+train/policy_gradient_loss��!��cI$'       ��F	�)�)c��A�w*
+
+train/value_loss�pI����        )�P	�GQ*c��A��*
+
+time/fps `�D�T�'       ��F	�GQ*c��A��*
+
+train/approx_kl��u;�ً�+       ��K	�GQ*c��A��*
+
+train/clip_fraction    ��/�(       �pJ	�GQ*c��A��*
+
+train/clip_range��L>�s۸*       ����	�GQ*c��A��*
+
+train/entropy_loss������Tk0       ���_	�GQ*c��A��*!
+
+train/explained_variance  ̷��+       ��K	�GQ*c��A��*
+
+train/learning_rateRI�9{("       x=�	�GQ*c��A��*
+
+
+train/loss���G��#2       $V�	�GQ*c��A��*#
+!
+train/policy_gradient_loss�w��-�(       �pJ	�GQ*c��A��*
+
+train/value_loss:�jH{�J        )�P	���*c��A��*
+
+time/fps  �D�'       ��F	���*c��A��*
+
+train/approx_kl��;P�ҟ+       ��K	���*c��A��*
+
+train/clip_fraction    �t�(       �pJ	���*c��A��*
+
+train/clip_range��L>����*       ����	���*c��A��*
+
+train/entropy_loss�u�����O0       ���_	���*c��A��*!
+
+train/explained_variance  �6lc�+       ��K	���*c��A��*
+
+train/learning_rateRI�9��aY"       x=�	���*c��A��*
+
+
+train/loss�9G`2       $V�	���*c��A��*#
+!
+train/policy_gradient_lossw�뺰�*�(       �pJ	���*c��A��*
+
+train/value_loss��Gw+�p        )�P	��1+c��A�*
+
+time/fps ��D0�r'       ��F	��1+c��A�*
+
+train/approx_kl8��9SoP+       ��K	��1+c��A�*
+
+train/clip_fraction    �A��(       �pJ	��1+c��A�*
+
+train/clip_range��L>p�vu*       ����	��1+c��A�*
+
+train/entropy_loss 2���I��0       ���_	��1+c��A�*!
+
+train/explained_variance  �6�td�+       ��K	��1+c��A�*
+
+train/learning_rateRI�9˪�
+"       x=�	��1+c��A�*
+
+
+train/lossU�G�<�2       $V�	��1+c��A�*#
+!
+train/policy_gradient_loss�������(       �pJ	��1+c��A�*
+
+train/value_loss��HDP!        )�P	��+c��A��*
+
+time/fps `�D��'       ��F	��+c��A��*
+
+train/approx_kl�x�9���+       ��K	��+c��A��*
+
+train/clip_fraction    �X�(       �pJ	��+c��A��*
+
+train/clip_range��L>,�"*       ����	��+c��A��*
+
+train/entropy_loss�-�����[0       ���_	��+c��A��*!
+
+train/explained_variance  @6��
++       ��K	��+c��A��*
+
+train/learning_rateRI�9į|@"       x=�	��+c��A��*
+
+
+train/loss}
+H�"�k2       $V�	��+c��A��*#
+!
+train/policy_gradient_loss�<6��9��(       �pJ	��+c��A��*
+
+train/value_loss5��Hd8L�        )�P	a ,c��A��*
+
+time/fps  �D�b�'       ��F	a ,c��A��*
+
+train/approx_kl��<m6B�+       ��K	a ,c��A��*
+
+train/clip_fractionnYg=�e�R(       �pJ	a ,c��A��*
+
+train/clip_range��L>fK0*       ����	a ,c��A��*
+
+train/entropy_loss�����*w$0       ���_	a ,c��A��*!
+
+train/explained_variance  ��^;�~+       ��K	a ,c��A��*
+
+train/learning_rateRI�9����"       x=�	a ,c��A��*
+
+
+train/lossn�6G���2       $V�	a ,c��A��*#
+!
+train/policy_gradient_loss����e��(       �pJ	a ,c��A��*
+
+train/value_lossF�G�~�        )�P	Ђ,c��A��*
+
+time/fps ��De(�W'       ��F	Ђ,c��A��*
+
+train/approx_kl��9%��+       ��K	Ђ,c��A��*
+
+train/clip_fraction    <픉(       �pJ	Ђ,c��A��*
+
+train/clip_range��L>�dX*       ����	Ђ,c��A��*
+
+train/entropy_loss�O����0       ���_	Ђ,c��A��*!
+
+train/explained_variance  �5�d�+       ��K	Ђ,c��A��*
+
+train/learning_rateRI�9A��"       x=�	Ђ,c��A��*
+
+
+train/lossC�F���2       $V�	Ђ,c��A��*#
+!
+train/policy_gradient_loss%�޹�:.�(       �pJ	Ђ,c��A��*
+
+train/value_lossV
+5G��        )�P	l��,c��A��*
+
+time/fps `�D�$��'       ��F	l��,c��A��*
+
+train/approx_kl��
+8"h�+       ��K	l��,c��A��*
+
+train/clip_fraction    W��%(       �pJ	l��,c��A��*
+
+train/clip_range��L>{��*       ����	l��,c��A��*
+
+train/entropy_lossVa��8$ �0       ���_	l��,c��A��*!
+
+train/explained_variance  �6~I�+       ��K	l��,c��A��*
+
+train/learning_rateRI�9J���"       x=�	l��,c��A��*
+
+
+train/loss�_�G�~"d2       $V�	l��,c��A��*#
+!
+train/policy_gradient_loss�W��pJ(       �pJ	l��,c��A��*
+
+train/value_loss,�$HYlf        )�P	քc-c��A��*
+
+time/fps @�D�(bV'       ��F	քc-c��A��*
+
+train/approx_kl�;6+	+       ��K	քc-c��A��*
+
+train/clip_fraction    �8a�(       �pJ	քc-c��A��*
+
+train/clip_range��L>z�&$*       ����	քc-c��A��*
+
+train/entropy_loss���jު0       ���_	քc-c��A��*!
+
+train/explained_variance �C�sv�Y+       ��K	քc-c��A��*
+
+train/learning_rateRI�9ؑu"       x=�	քc-c��A��*
+
+
+train/loss]�Iu=u62       $V�	քc-c��A��*#
+!
+train/policy_gradient_loss�k���W#�(       �pJ	քc-c��A��*
+
+train/value_loss��J$�        )�P	��-c��A��*
+
+time/fps �D��ǧ'       ��F	��-c��A��*
+
+train/approx_kl�d68P�+       ��K	��-c��A��*
+
+train/clip_fraction    �a�(       �pJ	��-c��A��*
+
+train/clip_range��L>���*       ����	��-c��A��*
+
+train/entropy_loss�����ǁ0       ���_	��-c��A��*!
+
+train/explained_variance ���A�+       ��K	��-c��A��*
+
+train/learning_rateRI�9
+"u1"       x=�	��-c��A��*
+
+
+train/loss��I�22       $V�	��-c��A��*#
+!
+train/policy_gradient_loss=J�qL�(       �pJ	��-c��A��*
+
+train/value_loss��Jjۓo        )�P	<lD.c��A��*
+
+time/fps  �DK��d'       ��F	<lD.c��A��*
+
+train/approx_klBC�8�^�+       ��K	<lD.c��A��*
+
+train/clip_fraction    G�|=(       �pJ	<lD.c��A��*
+
+train/clip_range��L>1F��*       ����	<lD.c��A��*
+
+train/entropy_lossዋ��a�y0       ���_	<lD.c��A��*!
+
+train/explained_variance ��8�,�+       ��K	<lD.c��A��*
+
+train/learning_rateRI�9:�Y�"       x=�	<lD.c��A��*
+
+
+train/lossrM�Hg}$�2       $V�	<lD.c��A��*#
+!
+train/policy_gradient_loss,+�� ���(       �pJ	<lD.c��A��*
+
+train/value_lossȾbI�v�        )�P	�~�.c��A��*
+
+time/fps ��D曵'       ��F	�~�.c��A��*
+
+train/approx_kl'j�:���+       ��K	�~�.c��A��*
+
+train/clip_fraction    �W��(       �pJ	�~�.c��A��*
+
+train/clip_range��L>���a*       ����	�~�.c��A��*
+
+train/entropy_loss�b��hI�20       ���_	�~�.c��A��*!
+
+train/explained_variance �;��+       ��K	�~�.c��A��*
+
+train/learning_rateRI�9�p�"       x=�	�~�.c��A��*
+
+
+train/loss�FK`�2       $V�	�~�.c��A��*#
+!
+train/policy_gradient_loss��"f�(       �pJ	�~�.c��A��*
+
+train/value_loss+�FG�z        )�P	b*/c��A��*
+
+time/fps @�D5��'       ��F	b*/c��A��*
+
+train/approx_kl�I�:;/�+       ��K	b*/c��A��*
+
+train/clip_fraction    �I?�(       �pJ	b*/c��A��*
+
+train/clip_range��L>CO�E*       ����	b*/c��A��*
+
+train/entropy_loss�_��䫝�0       ���_	b*/c��A��*!
+
+train/explained_variance ^���o#�+       ��K	b*/c��A��*
+
+train/learning_rateRI�9��"       x=�	b*/c��A��*
+
+
+train/loss�YGF֊`�2       $V�	b*/c��A��*#
+!
+train/policy_gradient_loss��ºc��(       �pJ	b*/c��A��*
+
+train/value_loss���F��H�        )�P	�N�/c��A��*
+
+time/fps @�D>�߇'       ��F	�N�/c��A��*
+
+train/approx_kl�p8=�+       ��K	�N�/c��A��*
+
+train/clip_fraction    G�t�(       �pJ	�N�/c��A��*
+
+train/clip_range��L>Tډ�*       ����	�N�/c��A��*
+
+train/entropy_loss�_��U��o0       ���_	�N�/c��A��*!
+
+train/explained_variance  l6���+       ��K	�N�/c��A��*
+
+train/learning_rateRI�9:)`f"       x=�	�N�/c��A��*
+
+
+train/loss~c2H��)�2       $V�	�N�/c��A��*#
+!
+train/policy_gradient_loss�y��6(       �pJ	�N�/c��A��*
+
+train/value_loss!�H�p�        )�P	��0c��A��*
+
+time/fps  �D�/K'       ��F	��0c��A��*
+
+train/approx_kl��5G�B�+       ��K	��0c��A��*
+
+train/clip_fraction    �tlF(       �pJ	��0c��A��*
+
+train/clip_range��L>�v��*       ����	��0c��A��*
+
+train/entropy_loss�E���c��0       ���_	��0c��A��*!
+
+train/explained_variance @����ݒ+       ��K	��0c��A��*
+
+train/learning_rateRI�9�KH�"       x=�	��0c��A��*
+
+
+train/loss�I��u�2       $V�	��0c��A��*#
+!
+train/policy_gradient_loss������(       �pJ	��0c��A��*
+
+train/value_lossV��ITd�        )�P	X�|0c��A��*
+
+time/fps  �D|�nS'       ��F	X�|0c��A��*
+
+train/approx_kl�2�5��[�+       ��K	X�|0c��A��*
+
+train/clip_fraction    ���(       �pJ	X�|0c��A��*
+
+train/clip_range��L>��v�*       ����	X�|0c��A��*
+
+train/entropy_loss�F��"��0       ���_	X�|0c��A��*!
+
+train/explained_variance  `��z�+       ��K	X�|0c��A��*
+
+train/learning_rateRI�9���<"       x=�	X�|0c��A��*
+
+
+train/loss�I��{2       $V�	X�|0c��A��*#
+!
+train/policy_gradient_lossqlɷ�ܮ(       �pJ	X�|0c��A��*
+
+train/value_loss`�IjN�}        )�P	���0c��A��*
+
+time/fps  �D�H,'       ��F	���0c��A��*
+
+train/approx_klA��6��+       ��K	�0c��A��*
+
+train/clip_fraction    �Ưx(       �pJ	�0c��A��*
+
+train/clip_range��L>Tvq�*       ����	�0c��A��*
+
+train/entropy_lossn���}Ɩ0       ���_	�0c��A��*!
+
+train/explained_variance �
+�e�.�+       ��K	�0c��A��*
+
+train/learning_rateRI�9���"       x=�	�0c��A��*
+
+
+train/loss�^�H�	R2       $V�	�0c��A��*#
+!
+train/policy_gradient_lossy5��a��l(       �pJ	�0c��A��*
+
+train/value_loss� uI��=,        )�P	�
+\1c��A��*
+
+time/fps  �D��Ū'       ��F	�
+\1c��A��*
+
+train/approx_kl��]8����+       ��K	�
+\1c��A��*
+
+train/clip_fraction    !c�(       �pJ	�
+\1c��A��*
+
+train/clip_range��L>7�D�*       ����	�
+\1c��A��*
+
+train/entropy_loss�O���Qݚ0       ���_	�
+\1c��A��*!
+
+train/explained_variance  8��X�+       ��K	�
+\1c��A��*
+
+train/learning_rateRI�9|��"       x=�	�
+\1c��A��*
+
+
+train/loss$XH�qv2       $V�	�
+\1c��A��*#
+!
+train/policy_gradient_loss@���y1�(       �pJ	�
+\1c��A��*
+
+train/value_loss[y�H��bT        )�P	E��1c��A��*
+
+time/fps ��D\�w�'       ��F	E��1c��A��*
+
+train/approx_kl&��8wJ[x+       ��K	E��1c��A��*
+
+train/clip_fraction    �W[�(       �pJ	E��1c��A��*
+
+train/clip_range��L>4�-9*       ����	E��1c��A��*
+
+train/entropy_lossSC���c�0       ���_	E��1c��A��*!
+
+train/explained_variance  7Kq��+       ��K	E��1c��A��*
+
+train/learning_rateRI�9�pw_"       x=�	E��1c��A��*
+
+
+train/loss�_QH��2       $V�	E��1c��A��*#
+!
+train/policy_gradient_loss�����@2(       �pJ	E��1c��A��*
+
+train/value_lossa��HC{�        )�P	�jB2c��A��*
+
+time/fps ��D|�/'       ��F	�jB2c��A��*
+
+train/approx_kl�|�5a&�+       ��K	�jB2c��A��*
+
+train/clip_fraction    f4�((       �pJ	�jB2c��A��*
+
+train/clip_range��L>�A,D*       ����	�jB2c��A��*
+
+train/entropy_loss�����uU0       ���_	�rB2c��A��*!
+
+train/explained_variance  �5�.�+       ��K	�rB2c��A��*
+
+train/learning_rateRI�9S��Q"       x=�	�rB2c��A��*
+
+
+train/loss=dJn�2       $V�	�rB2c��A��*#
+!
+train/policy_gradient_loss.*���(       �pJ	�rB2c��A��*
+
+train/value_loss?�J�'�        )�P	���2c��A��*
+
+time/fps  �D����'       ��F	���2c��A��*
+
+train/approx_kl̈́Q5���+       ��K	���2c��A��*
+
+train/clip_fraction    �+��(       �pJ	���2c��A��*
+
+train/clip_range��L>��39*       ����	���2c��A��*
+
+train/entropy_loss�"��2�df0       ���_	���2c��A��*!
+
+train/explained_variance  ��B�+       ��K	���2c��A��*
+
+train/learning_rateRI�9ǔ�"       x=�	���2c��A��*
+
+
+train/loss�wK�|��2       $V�	���2c��A��*#
+!
+train/policy_gradient_loss� �e^j(       �pJ	���2c��A��*
+
+train/value_loss�Y�K=�Z�        )�P	�*3c��AИ*
+
+time/fps  �D���'       ��F	�*3c��AИ*
+
+train/approx_kljMg6UQQ�+       ��K	�*3c��AИ*
+
+train/clip_fraction    �8�O(       �pJ	�*3c��AИ*
+
+train/clip_range��L>#9��*       ����	�*3c��AИ*
+
+train/entropy_loss������0       ���_	�*3c��AИ*!
+
+train/explained_variance  *�0X	+       ��K	�*3c��AИ*
+
+train/learning_rateRI�9ɏ��"       x=�	�*3c��AИ*
+
+
+train/lossNM�Jm��2       $V�	�*3c��AИ*#
+!
+train/policy_gradient_loss���-�[�(       �pJ	�*3c��AИ*
+
+train/value_loss��[KB6��        )�P	!�3c��A��*
+
+time/fps @�D滵'       ��F	!�3c��A��*
+
+train/approx_kl>�6�>�M+       ��K	!�3c��A��*
+
+train/clip_fraction    ~o�(       �pJ	!�3c��A��*
+
+train/clip_range��L>ru�F*       ����	!�3c��A��*
+
+train/entropy_loss�'�� l�0       ���_	!�3c��A��*!
+
+train/explained_variance  �4��[I+       ��K	!�3c��A��*
+
+train/learning_rateRI�9�XSu"       x=�	!�3c��A��*
+
+
+train/losst�JW"	2       $V�	!�3c��A��*#
+!
+train/policy_gradient_lossy*���"�6(       �pJ	!�3c��A��*
+
+train/value_lossb<�JY(        )�P	+�4c��A��*
+
+time/fps @�D��O'       ��F	+�4c��A��*
+
+train/approx_kl�j�7�L_+       ��K	+�4c��A��*
+
+train/clip_fraction    �P�A(       �pJ	+�4c��A��*
+
+train/clip_range��L>DU�%*       ����	+�4c��A��*
+
+train/entropy_loss"���AY�0       ���_	+�4c��A��*!
+
+train/explained_variance  �6���Y+       ��K	+�4c��A��*
+
+train/learning_rateRI�9�y޽"       x=�	+�4c��A��*
+
+
+train/loss2'UI@
+��2       $V�	+�4c��A��*#
+!
+train/policy_gradient_loss� 6�����(       �pJ	+�4c��A��*
+
+train/value_loss�|�I����        )�P	�,w4c��A��*
+
+time/fps `�D��a�'       ��F	�,w4c��A��*
+
+train/approx_klN�6��Bd+       ��K	�,w4c��A��*
+
+train/clip_fraction    m&(       �pJ	�,w4c��A��*
+
+train/clip_range��L>�0+�*       ����	�,w4c��A��*
+
+train/entropy_lossr-����0       ���_	�,w4c��A��*!
+
+train/explained_variance  `5�)�+       ��K	�,w4c��A��*
+
+train/learning_rateRI�9p��"       x=�	�,w4c��A��*
+
+
+train/losstP�I��:2       $V�	�,w4c��A��*#
+!
+train/policy_gradient_lossD��I
+v((       �pJ	�,w4c��A��*
+
+train/value_loss�icJ�$t        )�P	˕�4c��A��*
+
+time/fps `�Da ('       ��F	˕�4c��A��*
+
+train/approx_kl��7N��+       ��K	˕�4c��A��*
+
+train/clip_fraction    G
+D�(       �pJ	˕�4c��A��*
+
+train/clip_range��L>T� �*       ����	˕�4c��A��*
+
+train/entropy_loss����ɂ�.0       ���_	˕�4c��A��*!
+
+train/explained_variance  ��
+� +       ��K	˕�4c��A��*
+
+train/learning_rateRI�9:�pt"       x=�	˕�4c��A��*
+
+
+train/lossk�VH��Q�2       $V�	˕�4c��A��*#
+!
+train/policy_gradient_loss^�߸�b*6(       �pJ	˕�4c��A��*
+
+train/value_loss��H�P�~        )�P	�Y5c��A��*
+
+time/fps @�D��os'       ��F	�Y5c��A��*
+
+train/approx_kl9Q9=�j+       ��K	�Y5c��A��*
+
+train/clip_fraction    �R@(       �pJ	�Y5c��A��*
+
+train/clip_range��L>���p*       ����	�Y5c��A��*
+
+train/entropy_loss�����0       ���_	�Y5c��A��*!
+
+train/explained_variance  ��ݓ�/+       ��K	�Y5c��A��*
+
+train/learning_rateRI�9�y;�"       x=�	�Y5c��A��*
+
+
+train/loss[H�?�&2       $V�	�Y5c��A��*#
+!
+train/policy_gradient_loss8��(�
+`(       �pJ	�Y5c��A��*
+
+train/value_loss�0pH�yk3        )�P	��5c��A��*
+
+time/fps @�D�#o['       ��F	��5c��A��*
+
+train/approx_kl�7�&5+       ��K	��5c��A��*
+
+train/clip_fraction    �O�}(       �pJ	��5c��A��*
+
+train/clip_range��L>`º�*       ����	��5c��A��*
+
+train/entropy_loss�6���Y0       ���_	��5c��A��*!
+
+train/explained_variance  �4�g�+       ��K	��5c��A��*
+
+train/learning_rateRI�9�x�"       x=�	��5c��A��*
+
+
+train/loss0��H*=�2       $V�	��5c��A��*#
+!
+train/policy_gradient_loss�θ�Na(       �pJ	��5c��A��*
+
+train/value_loss�I*I����        )�P	�/96c��A�*
+
+time/fps @�D&�ߜ'       ��F	5896c��A�*
+
+train/approx_kl�o�6��V�+       ��K	5896c��A�*
+
+train/clip_fraction    �!�(       �pJ	5896c��A�*
+
+train/clip_range��L>k�{~*       ����	5896c��A�*
+
+train/entropy_loss^;����+0       ���_	5896c��A�*!
+
+train/explained_variance  62���+       ��K	5896c��A�*
+
+train/learning_rateRI�9�ʅ�"       x=�	5896c��A�*
+
+
+train/loss}alJxO�12       $V�	5896c��A�*#
+!
+train/policy_gradient_loss���� J(       �pJ	5896c��A�*
+
+train/value_loss���J���D        )�P	���6c��Aз*
+
+time/fps @�Du�R�'       ��F	���6c��Aз*
+
+train/approx_klΚ5T+؝+       ��K	���6c��Aз*
+
+train/clip_fraction    �C��(       �pJ	���6c��Aз*
+
+train/clip_range��L>���6*       ����	���6c��Aз*
+
+train/entropy_loss�Ȋ�.�0�0       ���_	���6c��Aз*!
+
+train/explained_variance  b��ˬ+       ��K	���6c��Aз*
+
+train/learning_rateRI�9���D"       x=�	���6c��Aз*
+
+
+train/loss&��JҚ�2       $V�	���6c��Aз*#
+!
+train/policy_gradient_lossp����6um(       �pJ	���6c��Aз*
+
+train/value_losssrMK3 �*        )�P	u�7c��A��*
+
+time/fps @�D���'       ��F	u�7c��A��*
+
+train/approx_kl�K4����+       ��K	u�7c��A��*
+
+train/clip_fraction    A3%(       �pJ	u�7c��A��*
+
+train/clip_range��L>�h
+�*       ����	u�7c��A��*
+
+train/entropy_loss���f�T0       ���_	u�7c��A��*!
+
+train/explained_variance  ���ʪ�+       ��K	u�7c��A��*
+
+train/learning_rateRI�9���"       x=�	u�7c��A��*
+
+
+train/loss�o�J-��2       $V�	u�7c��A��*#
+!
+train/policy_gradient_loss�{>��C(       �pJ	u�7c��A��*
+
+train/value_loss��=K�ֺ,        )�P	�7�7c��A��*
+
+time/fps @�DO {'       ��F	�7�7c��A��*
+
+train/approx_kl0�f6��W+       ��K	�7�7c��A��*
+
+train/clip_fraction    Xօ(       �pJ	�7�7c��A��*
+
+train/clip_range��L>;�s�*       ����	�7�7c��A��*
+
+train/entropy_lossG�����O0       ���_	�7�7c��A��*!
+
+train/explained_variance `�8i�d+       ��K	�7�7c��A��*
+
+train/learning_rateRI�9e�
+"       x=�	�7�7c��A��*
+
+
+train/loss���J/�`�2       $V�	�7�7c��A��*#
+!
+train/policy_gradient_lossNa͸1�F�(       �pJ	�7�7c��A��*
+
+train/value_loss-�K�e��        )�P	0��7c��A��*
+
+time/fps @�Dhq%�'       ��F	0��7c��A��*
+
+train/approx_kl��p6f�&�+       ��K	0��7c��A��*
+
+train/clip_fraction    ��i(       �pJ	0��7c��A��*
+
+train/clip_range��L>��kU*       ����	0��7c��A��*
+
+train/entropy_loss,���ȃ`�0       ���_	0��7c��A��*!
+
+train/explained_variance  �8x�~)+       ��K	0��7c��A��*
+
+train/learning_rateRI�9�%��"       x=�	0��7c��A��*
+
+
+train/loss�o�JU>i�2       $V�	0��7c��A��*#
+!
+train/policy_gradient_loss∸7���(       �pJ	0��7c��A��*
+
+train/value_loss�EK���        )�P	��h8c��A��*
+
+time/fps `�D����'       ��F	��h8c��A��*
+
+train/approx_kl �6�L�+       ��K	��h8c��A��*
+
+train/clip_fraction    �#�X(       �pJ	��h8c��A��*
+
+train/clip_range��L>#�B�*       ����	��h8c��A��*
+
+train/entropy_lossp������S0       ���_	��h8c��A��*!
+
+train/explained_variance ��8��'+       ��K	��h8c��A��*
+
+train/learning_rateRI�9����"       x=�	��h8c��A��*
+
+
+train/loss��fJ��s2       $V�	��h8c��A��*#
+!
+train/policy_gradient_loss��3�[��S(       �pJ	��h8c��A��*
+
+train/value_loss���JN,Z�        )�P	T �8c��A��*
+
+time/fps `�D�s��'       ��F	T �8c��A��*
+
+train/approx_kl���6��]�+       ��K	T �8c��A��*
+
+train/clip_fraction    ��~�(       �pJ	T �8c��A��*
+
+train/clip_range��L>U�j�*       ����	T �8c��A��*
+
+train/entropy_loss󯊿�|50       ���_	T �8c��A��*!
+
+train/explained_variance  �5 IU>+       ��K	T �8c��A��*
+
+train/learning_rateRI�9Z"       x=�	T �8c��A��*
+
+
+train/loss��Jhg52       $V�	T �8c��A��*#
+!
+train/policy_gradient_loss��ɸa`ӿ(       �pJ	T �8c��A��*
+
+train/value_loss�5�J5c�        )�P	Q\H9c��A�*
+
+time/fps `�D@O
+�'       ��F	Q\H9c��A�*
+
+train/approx_kl��7�#��+       ��K	Q\H9c��A�*
+
+train/clip_fraction    2C�(       �pJ	Q\H9c��A�*
+
+train/clip_range��L>�T�*       ����	Q\H9c��A�*
+
+train/entropy_loss�ˊ�,W��0       ���_	Q\H9c��A�*!
+
+train/explained_variance  �4�xԊ+       ��K	Q\H9c��A�*
+
+train/learning_rateRI�9/�oD"       x=�	Q\H9c��A�*
+
+
+train/loss7,J���2       $V�	Q\H9c��A�*#
+!
+train/policy_gradient_loss�9�{S#�(       �pJ	Q\H9c��A�*
+
+train/value_loss	Q�J��X�        )�P	5*�9c��A��*
+
+time/fps ��D�0'       ��F	5*�9c��A��*
+
+train/approx_kl��6S��o+       ��K	5*�9c��A��*
+
+train/clip_fraction    D�(       �pJ	5*�9c��A��*
+
+train/clip_range��L>���*       ����	5*�9c��A��*
+
+train/entropy_loss�኿P²�0       ���_	5*�9c��A��*!
+
+train/explained_variance  6�4v+       ��K	5*�9c��A��*
+
+train/learning_rateRI�9"�p"       x=�	5*�9c��A��*
+
+
+train/loss\|JJ�a�2       $V�	5*�9c��A��*#
+!
+train/policy_gradient_lossG*��]�\(       �pJ	5*�9c��A��*
+
+train/value_loss!�J�^`        )�P	_$':c��A��*
+
+time/fps `�Du�'       ��F	�4':c��A��*
+
+train/approx_kl�^D8R�+       ��K	�4':c��A��*
+
+train/clip_fraction    ݱE(       �pJ	�4':c��A��*
+
+train/clip_range��L>�柕*       ����	�4':c��A��*
+
+train/entropy_loss�����0       ���_	�4':c��A��*!
+
+train/explained_variance  ��]�-�+       ��K	�4':c��A��*
+
+train/learning_rateRI�9�q�"       x=�	�4':c��A��*
+
+
+train/loss��I
+��2       $V�	�4':c��A��*#
+!
+train/policy_gradient_losst
+Z�6�HW(       �pJ	�4':c��A��*
+
+train/value_loss��In�,�        )�P	���:c��A��*
+
+time/fps `�D�̉�'       ��F	���:c��A��*
+
+train/approx_klc 8Կ%�+       ��K	���:c��A��*
+
+train/clip_fraction    S��L(       �pJ	���:c��A��*
+
+train/clip_range��L>d2:Y*       ����	���:c��A��*
+
+train/entropy_loss���}4��0       ���_	���:c��A��*!
+
+train/explained_variance   ����+       ��K	���:c��A��*
+
+train/learning_rateRI�9N��"       x=�	���:c��A��*
+
+
+train/lossI�I��r�2       $V�	���:c��A��*#
+!
+train/policy_gradient_loss�c3�r1iK(       �pJ	���:c��A��*
+
+train/value_loss�/QJ��U�        )�P	{�;c��A��*
+
+time/fps ��DiCP'       ��F	{�;c��A��*
+
+train/approx_klV�7�\j,+       ��K	{�;c��A��*
+
+train/clip_fraction    <rW(       �pJ	{�;c��A��*
+
+train/clip_range��L>n�ae*       ����	{�;c��A��*
+
+train/entropy_lossf����v:0       ���_	{�;c��A��*!
+
+train/explained_variance    32�+       ��K	{�;c��A��*
+
+train/learning_rateRI�9AY$�"       x=�	{�;c��A��*
+
+
+train/loss�IJn��s2       $V�	{�;c��A��*#
+!
+train/policy_gradient_loss�Y����l(       �pJ	{�;c��A��*
+
+train/value_loss���J5��        )�P	a��;c��A��*
+
+time/fps �D���'       ��F	a��;c��A��*
+
+train/approx_kl�t8��x+       ��K	Ϣ�;c��A��*
+
+train/clip_fraction    ��D{(       �pJ	Ϣ�;c��A��*
+
+train/clip_range��L><*ń*       ����	Ϣ�;c��A��*
+
+train/entropy_loss�Ɋ��o�$0       ���_	Ϣ�;c��A��*!
+
+train/explained_variance  �6S~�+       ��K	Ϣ�;c��A��*
+
+train/learning_rateRI�9�Ip�"       x=�	Ϣ�;c��A��*
+
+
+train/lossm"_IkQ�92       $V�	Ϣ�;c��A��*#
+!
+train/policy_gradient_loss���<NT�(       �pJ	Ϣ�;c��A��*
+
+train/value_loss���IM�V        )�P	$-�;c��A��*
+
+time/fps �D��@�'       ��F	$-�;c��A��*
+
+train/approx_kl���7!���+       ��K	$-�;c��A��*
+
+train/clip_fraction    ���(       �pJ	$-�;c��A��*
+
+train/clip_range��L>�a�=*       ����	$-�;c��A��*
+
+train/entropy_lossۊ���=�0       ���_	$-�;c��A��*!
+
+train/explained_variance  �5���+       ��K	$-�;c��A��*
+
+train/learning_rateRI�9���W"       x=�	$-�;c��A��*
+
+
+train/loss
+7�ITɢ�2       $V�	$-�;c��A��*#
+!
+train/policy_gradient_lossc�4���+�(       �pJ	$-�;c��A��*
+
+train/value_lossg�QJ����        )�P	��g<c��A��*
+
+time/fps �D���f'       ��F	��g<c��A��*
+
+train/approx_kl��I7��+       ��K	��g<c��A��*
+
+train/clip_fraction    we�(       �pJ	��g<c��A��*
+
+train/clip_range��L>	Xm�*       ����	��g<c��A��*
+
+train/entropy_lossP����0       ���_	��g<c��A��*!
+
+train/explained_variance  �4/��+       ��K	��g<c��A��*
+
+train/learning_rateRI�9j~/)"       x=�	��g<c��A��*
+
+
+train/lossqaJ*��2       $V�	��g<c��A��*#
+!
+train/policy_gradient_losst0���C(       �pJ	��g<c��A��*
+
+train/value_loss%5�J��·        )�P	 ��<c��A��*
+
+time/fps �DAqS'       ��F	 ��<c��A��*
+
+train/approx_kl~�7��G�+       ��K	 ��<c��A��*
+
+train/clip_fraction    ���(       �pJ	 ��<c��A��*
+
+train/clip_range��L>O�U
+*       ����	 ��<c��A��*
+
+train/entropy_loss����~;�0       ���_	 ��<c��A��*!
+
+train/explained_variance  O���$=+       ��K	 ��<c��A��*
+
+train/learning_rateRI�9����"       x=�	 ��<c��A��*
+
+
+train/loss
+��I��L2       $V�	 ��<c��A��*#
+!
+train/policy_gradient_loss��K�Z��V(       �pJ	 ��<c��A��*
+
+train/value_loss��FJP{*        )�P	xlH=c��A��*
+
+time/fps �D�^��'       ��F	xlH=c��A��*
+
+train/approx_kl�AA7���+       ��K	xlH=c��A��*
+
+train/clip_fraction    ��((       �pJ	xlH=c��A��*
+
+train/clip_range��L>�,&*       ����	xlH=c��A��*
+
+train/entropy_loss�����]��0       ���_	xlH=c��A��*!
+
+train/explained_variance  �6��˖+       ��K	xlH=c��A��*
+
+train/learning_rateRI�9'̤"       x=�	xlH=c��A��*
+
+
+train/loss,�I.�82       $V�	xlH=c��A��*#
+!
+train/policy_gradient_loss�p��6��,(       �pJ	xlH=c��A��*
+
+train/value_loss_p%J�	GD        )�P	K��=c��A��*
+
+time/fps �D�e�'       ��F	K��=c��A��*
+
+train/approx_klvB�7��Q+       ��K	K��=c��A��*
+
+train/clip_fraction    ��Y(       �pJ	K��=c��A��*
+
+train/clip_range��L>a
+�*       ����	K��=c��A��*
+
+train/entropy_loss���gN�\0       ���_	K��=c��A��*!
+
+train/explained_variance �E�ei��+       ��K	K��=c��A��*
+
+train/learning_rateRI�9�Ј�"       x=�	K��=c��A��*
+
+
+train/loss�U�IĆ:�2       $V�	K��=c��A��*#
+!
+train/policy_gradient_lossD�иSJ�(       �pJ	K��=c��A��*
+
+train/value_lossNYJǻ�        )�P	l�*>c��A��*
+
+time/fps �D�+l'       ��F	l�*>c��A��*
+
+train/approx_klo�9�_ t+       ��K	l�*>c��A��*
+
+train/clip_fraction    -O�6(       �pJ	l�*>c��A��*
+
+train/clip_range��L>4y��*       ����	l�*>c��A��*
+
+train/entropy_loss�����0       ���_	l�*>c��A��*!
+
+train/explained_variance  (6��=�+       ��K	l�*>c��A��*
+
+train/learning_rateRI�90xŲ"       x=�	l�*>c��A��*
+
+
+train/lossU�H]�2       $V�	l�*>c��A��*#
+!
+train/policy_gradient_loss�Q�ؗI�(       �pJ	l�*>c��A��*
+
+train/value_lossEi"I|�^�        )�P	Q��>c��A��*
+
+time/fps �D���	'       ��F	Q��>c��A��*
+
+train/approx_klo��:����+       ��K	Q��>c��A��*
+
+train/clip_fraction    �[�(       �pJ	Q��>c��A��*
+
+train/clip_range��L>X�l*       ����	Q��>c��A��*
+
+train/entropy_lossA!��$D80       ���_	Q��>c��A��*!
+
+train/explained_variance  �6?�+       ��K	Q��>c��A��*
+
+train/learning_rateRI�9��v#"       x=�	Q��>c��A��*
+
+
+train/lossq�Gy��62       $V�	Q��>c��A��*#
+!
+train/policy_gradient_losssƺ=��1(       �pJ	Q��>c��A��*
+
+train/value_lossj$�G�"�        )�P	J�
+?c��A��*
+
+time/fps �D��s'       ��F	J�
+?c��A��*
+
+train/approx_kl��8��`#+       ��K	J�
+?c��A��*
+
+train/clip_fraction    N���(       �pJ	J�
+?c��A��*
+
+train/clip_range��L>�
+��*       ����	J�
+?c��A��*
+
+train/entropy_lossh���,<0       ���_	J�
+?c��A��*!
+
+train/explained_variance ���m�6+       ��K	J�
+?c��A��*
+
+train/learning_rateRI�9S7�s"       x=�	J�
+?c��A��*
+
+
+train/loss�'IS��_2       $V�	J�
+?c��A��*#
+!
+train/policy_gradient_loss�te�&�Bp(       �pJ	J�
+?c��A��*
+
+train/value_lossZޤI���        )�P	.�z?c��A��*
+
+time/fps �Dn6S�'       ��F	.�z?c��A��*
+
+train/approx_kl�w8!av+       ��K	.�z?c��A��*
+
+train/clip_fraction    �L'�(       �pJ	.�z?c��A��*
+
+train/clip_range��L>����*       ����	.�z?c��A��*
+
+train/entropy_loss?늿
+��0       ���_	.�z?c��A��*!
+
+train/explained_variance ��77���+       ��K	.�z?c��A��*
+
+train/learning_rateRI�9�sa"       x=�	.�z?c��A��*
+
+
+train/loss�H�I��02       $V�	.�z?c��A��*#
+!
+train/policy_gradient_lossDT��:�(       �pJ	.�z?c��A��*
+
+train/value_lossGFVJ�]a        )�P	��?c��A��*
+
+time/fps �D^�'       ��F	��?c��A��*
+
+train/approx_klߛ8�.�+       ��K	��?c��A��*
+
+train/clip_fraction    ݑW (       �pJ	��?c��A��*
+
+train/clip_range��L>
+�
+�*       ����	��?c��A��*
+
+train/entropy_loss�;���H��0       ���_	��?c��A��*!
+
+train/explained_variance  (�w��+       ��K	��?c��A��*
+
+train/learning_rateRI�9�8{�"       x=�	��?c��A��*
+
+
+train/losso%�I�~s2       $V�	��?c��A��*#
+!
+train/policy_gradient_loss��:�D�(       �pJ	��?c��A��*
+
+train/value_loss��J�ZS        )�P	�[@c��A��*
+
+time/fps �D��(�'       ��F	�[@c��A��*
+
+train/approx_kl� 9�F��+       ��K	�[@c��A��*
+
+train/clip_fraction    Yx$(       �pJ	�[@c��A��*
+
+train/clip_range��L>���*       ����	�[@c��A��*
+
+train/entropy_loss� ��\�:�0       ���_	�[@c��A��*!
+
+train/explained_variance   �2�+       ��K	�[@c��A��*
+
+train/learning_rateRI�9d�\�"       x=�	�[@c��A��*
+
+
+train/lossY�I��j'2       $V�	�[@c��A��*#
+!
+train/policy_gradient_loss�b.��	��(       �pJ	�[@c��A��*
+
+train/value_loss�VqJ�+
+�        )�P	W8�@c��A��	*
+
+time/fps �DDA�'       ��F	W8�@c��A��	*
+
+train/approx_klM+�8t9 h+       ��K	W8�@c��A��	*
+
+train/clip_fraction    �n�x(       �pJ	W8�@c��A��	*
+
+train/clip_range��L>�}G�*       ����	W8�@c��A��	*
+
+train/entropy_loss�;��ʑ	q0       ���_	W8�@c��A��	*!
+
+train/explained_variance  ���HgK+       ��K	W8�@c��A��	*
+
+train/learning_rateRI�9�U��"       x=�	W8�@c��A��	*
+
+
+train/loss��J���w2       $V�	W8�@c��A��	*#
+!
+train/policy_gradient_loss?���o�>�(       �pJ	W8�@c��A��	*
+
+train/value_loss���J��        )�P	��=Ac��AД	*
+
+time/fps �D����'       ��F	��=Ac��AД	*
+
+train/approx_klʺ�7�j�p+       ��K	��=Ac��AД	*
+
+train/clip_fraction    �|sY(       �pJ	��=Ac��AД	*
+
+train/clip_range��L>���k*       ����	��=Ac��AД	*
+
+train/entropy_loss�����ϭu0       ���_	��=Ac��AД	*!
+
+train/explained_variance  ,6L���+       ��K	��=Ac��AД	*
+
+train/learning_rateRI�9�c_�"       x=�	��=Ac��AД	*
+
+
+train/loss
+��J�
+�2       $V�	��=Ac��AД	*#
+!
+train/policy_gradient_lossM|B��]
+V(       �pJ	��=Ac��AД	*
+
+train/value_loss�1K��a        )�P	��Ac��A��	*
+
+time/fps �D�v_'       ��F	��Ac��A��	*
+
+train/approx_klZ�z7I�s�+       ��K	��Ac��A��	*
+
+train/clip_fraction     +M�(       �pJ	��Ac��A��	*
+
+train/clip_range��L>�Z-�*       ����	��Ac��A��	*
+
+train/entropy_loss�F��ʦ�B0       ���_	K%�Ac��A��	*!
+
+train/explained_variance  �։Du+       ��K	K%�Ac��A��	*
+
+train/learning_rateRI�9��}�"       x=�	K%�Ac��A��	*
+
+
+train/lossf�J&�,%2       $V�	K%�Ac��A��	*#
+!
+train/policy_gradient_loss�B��><�(       �pJ	K%�Ac��A��	*
+
+train/value_lossL�Kt�]�        )�P	�FBc��A��	*
+
+time/fps �D0��'       ��F	�FBc��A��	*
+
+train/approx_kl-VT7[}��+       ��K	�FBc��A��	*
+
+train/clip_fraction    ]�Th(       �pJ	�FBc��A��	*
+
+train/clip_range��L>�Z�s*       ����	�FBc��A��	*
+
+train/entropy_loss�_��X_"@0       ���_	�FBc��A��	*!
+
+train/explained_variance   �,]]K+       ��K	�FBc��A��	*
+
+train/learning_rateRI�9`1��"       x=�	�FBc��A��	*
+
+
+train/loss���J����2       $V�	�FBc��A��	*#
+!
+train/policy_gradient_lossG������$(       �pJ	�FBc��A��	*
+
+train/value_loss��K���{        )�P	aO�Bc��A��	*
+
+time/fps �D
+�'       ��F	aO�Bc��A��	*
+
+train/approx_kl>87��(�+       ��K	aO�Bc��A��	*
+
+train/clip_fraction    6�l�(       �pJ	aO�Bc��A��	*
+
+train/clip_range��L>F��*       ����	aO�Bc��A��	*
+
+train/entropy_lossd���?0       ���_	aO�Bc��A��	*!
+
+train/explained_variance   ��
+��+       ��K	aO�Bc��A��	*
+
+train/learning_rateRI�9+@H"       x=�	aO�Bc��A��	*
+
+
+train/loss�k�J�p�>2       $V�	aO�Bc��A��	*#
+!
+train/policy_gradient_lossO�|�C(       �pJ	aO�Bc��A��	*
+
+train/value_lossD2K�f��        )�P	���Bc��A��	*
+
+time/fps �D�&8*'       ��F	���Bc��A��	*
+
+train/approx_kl�8���M+       ��K	���Bc��A��	*
+
+train/clip_fraction    ���(       �pJ	���Bc��A��	*
+
+train/clip_range��L>UgJ�*       ����	���Bc��A��	*
+
+train/entropy_loss"����r_0       ���_	���Bc��A��	*!
+
+train/explained_variance  @4��c+       ��K	���Bc��A��	*
+
+train/learning_rateRI�9 ��U"       x=�	���Bc��A��	*
+
+
+train/loss���J�P,2       $V�	���Bc��A��	*#
+!
+train/policy_gradient_loss�z��g�(       �pJ	���Bc��A��	*
+
+train/value_loss��HKn_!�        )�P	�ukCc��A��	*
+
+time/fps  �D���H'       ��F	�ukCc��A��	*
+
+train/approx_kl��38��+       ��K	�ukCc��A��	*
+
+train/clip_fraction    ��(       �pJ	�ukCc��A��	*
+
+train/clip_range��L>2�م*       ����	�ukCc��A��	*
+
+train/entropy_lossA���u�D0       ���_	�ukCc��A��	*!
+
+train/explained_variance  �����+       ��K	�ukCc��A��	*
+
+train/learning_rateRI�9�$3@"       x=�	�ukCc��A��	*
+
+
+train/loss�8�J��F2       $V�	�ukCc��A��	*#
+!
+train/policy_gradient_loss�~r��<�(       �pJ	�ukCc��A��	*
+
+train/value_lossMK�3        )�P	O�Cc��A��
+*
+
+time/fps  �D�<68'       ��F	O�Cc��A��
+*
+
+train/approx_klx0�6E��+       ��K	O�Cc��A��
+*
+
+train/clip_fraction    �Y
+�(       �pJ	O�Cc��A��
+*
+
+train/clip_range��L>Aj *       ����	O�Cc��A��
+*
+
+train/entropy_lossZd�����0       ���_	O�Cc��A��
+*!
+
+train/explained_variance  ����>+       ��K	O�Cc��A��
+*
+
+train/learning_rateRI�9�p."       x=�	O�Cc��A��
+*
+
+
+train/loss���J����2       $V�	O�Cc��A��
+*#
+!
+train/policy_gradient_loss���E+t�(       �pJ	O�Cc��A��
+*
+
+train/value_loss;2CK��        )�P	�JDc��A��
+*
+
+time/fps  �D�[��'       ��F	�JDc��A��
+*
+
+train/approx_kl"�7KT�+       ��K	�JDc��A��
+*
+
+train/clip_fraction    �d
+�(       �pJ	�JDc��A��
+*
+
+train/clip_range��L>q�g�*       ����	�JDc��A��
+*
+
+train/entropy_loss�����0       ���_	�JDc��A��
+*!
+
+train/explained_variance  �5����+       ��K	�JDc��A��
+*
+
+train/learning_rateRI�9�})%"       x=�	�JDc��A��
+*
+
+
+train/loss7x�J-u:2       $V�	�JDc��A��
+*#
+!
+train/policy_gradient_lossý ��A�W(       �pJ	�JDc��A��
+*
+
+train/value_loss�'\K�g��        )�P	0�Dc��Aг
+*
+
+time/fps  �D����'       ��F	0�Dc��Aг
+*
+
+train/approx_klR()7�\�+       ��K	�%�Dc��Aг
+*
+
+train/clip_fraction    �(       �pJ	�%�Dc��Aг
+*
+
+train/clip_range��L>��4*       ����	�%�Dc��Aг
+*
+
+train/entropy_loss����/uc0       ���_	�%�Dc��Aг
+*!
+
+train/explained_variance  ��88�+       ��K	�%�Dc��Aг
+*
+
+train/learning_rateRI�9%3�"       x=�	�%�Dc��Aг
+*
+
+
+train/loss�J�I��2       $V�	�%�Dc��Aг
+*#
+!
+train/policy_gradient_lossJڢ�W�d)(       �pJ	�%�Dc��Aг
+*
+
+train/value_loss�kMK&�z        )�P	E�*Ec��A��
+*
+
+time/fps  �D�\1a'       ��F	E�*Ec��A��
+*
+
+train/approx_kl��	6�Sb+       ��K	E�*Ec��A��
+*
+
+train/clip_fraction    �(       �pJ	E�*Ec��A��
+*
+
+train/clip_range��L>��*       ����	E�*Ec��A��
+*
+
+train/entropy_loss[z��3��0       ���_	E�*Ec��A��
+*!
+
+train/explained_variance   5����+       ��K	E�*Ec��A��
+*
+
+train/learning_rateRI�9��(�"       x=�	E�*Ec��A��
+*
+
+
+train/loss;�J��#2       $V�	E�*Ec��A��
+*#
+!
+train/policy_gradient_loss��ַ���(       �pJ	E�*Ec��A��
+*
+
+train/value_loss��zK���U        )�P	#=�Ec��A��
+*
+
+time/fps  �D3��'       ��F	#=�Ec��A��
+*
+
+train/approx_kl��6���+       ��K	#=�Ec��A��
+*
+
+train/clip_fraction    �]l(       �pJ	#=�Ec��A��
+*
+
+train/clip_range��L>�g��*       ����	#=�Ec��A��
+*
+
+train/entropy_loss�c���&lC0       ���_	#=�Ec��A��
+*!
+
+train/explained_variance   �M	�+       ��K	#=�Ec��A��
+*
+
+train/learning_rateRI�9ބH�"       x=�	#=�Ec��A��
+*
+
+
+train/lossY��JI"M2       $V�	#=�Ec��A��
+*#
+!
+train/policy_gradient_loss�]�&��(       �pJ	#=�Ec��A��
+*
+
+train/value_lossh�bK]�o5        )�P	��Fc��A��
+*
+
+time/fps  �DW�_�'       ��F	��Fc��A��
+*
+
+train/approx_klvX�77u��+       ��K	��Fc��A��
+*
+
+train/clip_fraction    ���(       �pJ	��Fc��A��
+*
+
+train/clip_range��L>�A��*       ����	��Fc��A��
+*
+
+train/entropy_loss���p��)0       ���_	��Fc��A��
+*!
+
+train/explained_variance  �4$�l�+       ��K	��Fc��A��
+*
+
+train/learning_rateRI�9%F�\"       x=�	��Fc��A��
+*
+
+
+train/loss��J���2       $V�	��Fc��A��
+*#
+!
+train/policy_gradient_loss���>��(       �pJ	��Fc��A��
+*
+
+train/value_lossD\KvZ�        )�P	a{|Fc��A��*
+
+time/fps  �D�y��'       ��F	a{|Fc��A��*
+
+train/approx_kl杩7�k+       ��K	a{|Fc��A��*
+
+train/clip_fraction    �%`�(       �pJ	a{|Fc��A��*
+
+train/clip_range��L>�~R�*       ����	a{|Fc��A��*
+
+train/entropy_loss�4���.�0       ���_	a{|Fc��A��*!
+
+train/explained_variance   ��=�+       ��K	a{|Fc��A��*
+
+train/learning_rateRI�9ӾtQ"       x=�	a{|Fc��A��*
+
+
+train/loss���J�}e�2       $V�	a{|Fc��A��*#
+!
+train/policy_gradient_loss�'�^??}(       �pJ	a{|Fc��A��*
+
+train/value_loss��sKwPaQ        )�P	Z��Fc��A��*
+
+time/fps  �D /ng'       ��F	Z��Fc��A��*
+
+train/approx_klD�79m�V+       ��K	Z��Fc��A��*
+
+train/clip_fraction    �rHf(       �pJ	Z��Fc��A��*
+
+train/clip_range��L>ڧ*       ����	Z��Fc��A��*
+
+train/entropy_lossR�����0       ���_	Z��Fc��A��*!
+
+train/explained_variance  �5�:>�+       ��K	Z��Fc��A��*
+
+train/learning_rateRI�9Ll�"       x=�	Z��Fc��A��*
+
+
+train/loss��K�L�2       $V�	Z��Fc��A��*#
+!
+train/policy_gradient_lossN���Z�(       �pJ	Z��Fc��A��*
+
+train/value_loss��K=4�L        )�P	4�ZGc��A�*
+
+time/fps  �D�V�}'       ��F	4�ZGc��A�*
+
+train/approx_kl���7X�
+L+       ��K	4�ZGc��A�*
+
+train/clip_fraction    NY�(       �pJ	4�ZGc��A�*
+
+train/clip_range��L>�9��*       ����	4�ZGc��A�*
+
+train/entropy_loss����!��0       ���_	4�ZGc��A�*!
+
+train/explained_variance  �46�XI+       ��K	4�ZGc��A�*
+
+train/learning_rateRI�9Sr�8"       x=�	4�ZGc��A�*
+
+
+train/loss��Jf��2       $V�	4�ZGc��A�*#
+!
+train/policy_gradient_losscvQ�&id^(       �pJ	4�ZGc��A�*
+
+train/value_loss0xOK�a�        )�P	���Gc��A�*
+
+time/fps  �Dw�8�'       ��F	���Gc��A�*
+
+train/approx_kl���7��+       ��K	���Gc��A�*
+
+train/clip_fraction    ("�P(       �pJ	���Gc��A�*
+
+train/clip_range��L>����*       ����	���Gc��A�*
+
+train/entropy_loss����.�4&0       ���_	��Gc��A�*!
+
+train/explained_variance  ��W�+       ��K	��Gc��A�*
+
+train/learning_rateRI�9�ղ�"       x=�	��Gc��A�*
+
+
+train/loss���J�
+!)2       $V�	��Gc��A�*#
+!
+train/policy_gradient_loss���;���(       �pJ	��Gc��A�*
+
+train/value_lossOLK^�3        )�P	�:Hc��A��*
+
+time/fps  �D�{�'       ��F	�:Hc��A��*
+
+train/approx_kl��{80�$�+       ��K	�:Hc��A��*
+
+train/clip_fraction    ��c�(       �pJ	�:Hc��A��*
+
+train/clip_range��L>0�*       ����	�:Hc��A��*
+
+train/entropy_loss�	��S0       ���_	�:Hc��A��*!
+
+train/explained_variance  �5��X+       ��K	�:Hc��A��*
+
+train/learning_rateRI�9O8"       x=�	�:Hc��A��*
+
+
+train/lossG�J��2       $V�	�:Hc��A��*#
+!
+train/policy_gradient_lossU~��I��(       �pJ	�:Hc��A��*
+
+train/value_loss�
+K)�+�        )�P	�"�Hc��A��*
+
+time/fps  �Di���'       ��F	�"�Hc��A��*
+
+train/approx_kl�
+D9�#�#+       ��K	�"�Hc��A��*
+
+train/clip_fraction    ��d�(       �pJ	�"�Hc��A��*
+
+train/clip_range��L>�e$*       ����	�"�Hc��A��*
+
+train/entropy_loss7�����(0       ���_	�"�Hc��A��*!
+
+train/explained_variance  �5z-SA+       ��K	�"�Hc��A��*
+
+train/learning_rateRI�9!PA"       x=�	�"�Hc��A��*
+
+
+train/loss��TJ͋5�2       $V�	�"�Hc��A��*#
+!
+train/policy_gradient_loss>��~}
+�(       �pJ	�"�Hc��A��*
+
+train/value_loss�,�J�$��        )�P	?}Ic��A��*
+
+time/fps  �DʕJ�'       ��F	?}Ic��A��*
+
+train/approx_klN��7^aPp+       ��K	?}Ic��A��*
+
+train/clip_fraction    � ��(       �pJ	?}Ic��A��*
+
+train/clip_range��L>�E=*       ����	?}Ic��A��*
+
+train/entropy_loss�L���9�T0       ���_	?}Ic��A��*!
+
+train/explained_variance  �5�jH�+       ��K	?}Ic��A��*
+
+train/learning_rateRI�9�Ǜp"       x=�	?}Ic��A��*
+
+
+train/loss�I]J�t�D2       $V�	?}Ic��A��*#
+!
+train/policy_gradient_loss��4o��(       �pJ	?}Ic��A��*
+
+train/value_loss�h�J�x�        )�P	5ߊIc��A��*
+
+time/fps  �Dϥ�''       ��F	5ߊIc��A��*
+
+train/approx_kl��/8_��+       ��K	5ߊIc��A��*
+
+train/clip_fraction    ���(       �pJ	5ߊIc��A��*
+
+train/clip_range��L>�F��*       ����	5ߊIc��A��*
+
+train/entropy_loss>��V���0       ���_	5ߊIc��A��*!
+
+train/explained_variance   ��Z�+       ��K	5ߊIc��A��*
+
+train/learning_rateRI�9���"       x=�	5ߊIc��A��*
+
+
+train/loss��CJ����2       $V�	5ߊIc��A��*#
+!
+train/policy_gradient_loss�nB��O��(       �pJ	5ߊIc��A��*
+
+train/value_lossQ�J/.��        )�P	��Ic��A��*
+
+time/fps  �DM0 �'       ��F	��Ic��A��*
+
+train/approx_klb�9		P+       ��K	��Ic��A��*
+
+train/clip_fraction    �،�(       �pJ	��Ic��A��*
+
+train/clip_range��L>]�c*       ����	��Ic��A��*
+
+train/entropy_loss�:��=��0       ���_	��Ic��A��*!
+
+train/explained_variance  �5���+       ��K	��Ic��A��*
+
+train/learning_rateRI�9��k"       x=�	��Ic��A��*
+
+
+train/lossEvJD�v22       $V�	��Ic��A��*#
+!
+train/policy_gradient_loss�bֹ�w�F(       �pJ	��Ic��A��*
+
+train/value_loss�@�JjX��        )�P	slJc��A��*
+
+time/fps  �D�~!'       ��F	slJc��A��*
+
+train/approx_kl�m9ߐ��+       ��K	slJc��A��*
+
+train/clip_fraction    ��E(       �pJ	slJc��A��*
+
+train/clip_range��L>���*       ����	slJc��A��*
+
+train/entropy_loss�O��h7�-0       ���_	slJc��A��*!
+
+train/explained_variance  ����B�+       ��K	slJc��A��*
+
+train/learning_rateRI�9�)q�"       x=�	slJc��A��*
+
+
+train/loss�m�IYJO2       $V�	slJc��A��*#
+!
+train/policy_gradient_loss\��X2W(       �pJ	slJc��A��*
+
+train/value_loss{dnJi�u�        )�P	��Jc��A��*
+
+time/fps  �D�)�'       ��F	��Jc��A��*
+
+train/approx_kl)�9�q R+       ��K	��Jc��A��*
+
+train/clip_fraction    �^�/(       �pJ	��Jc��A��*
+
+train/clip_range��L>����*       ����	��Jc��A��*
+
+train/entropy_loss�}���K�\0       ���_	��Jc��A��*!
+
+train/explained_variance  @�dJ�e+       ��K	��Jc��A��*
+
+train/learning_rateRI�9���"       x=�	��Jc��A��*
+
+
+train/loss�`�Igs�2       $V�	��Jc��A��*#
+!
+train/policy_gradient_lossW�5���[(       �pJ	��Jc��A��*
+
+train/value_lossSh"J
+���        )�P	iqMKc��A��*
+
+time/fps  �D;�D'       ��F	iqMKc��A��*
+
+train/approx_klI�C9�84+       ��K	iqMKc��A��*
+
+train/clip_fraction    ��G>(       �pJ	iqMKc��A��*
+
+train/clip_range��L>Z���*       ����	iqMKc��A��*
+
+train/entropy_loss̞��� �r0       ���_	iqMKc��A��*!
+
+train/explained_variance  05��+       ��K	iqMKc��A��*
+
+train/learning_rateRI�9��j�"       x=�	iqMKc��A��*
+
+
+train/loss"�J�7��2       $V�	iqMKc��A��*#
+!
+train/policy_gradient_lossw9�?S
+(       �pJ	iqMKc��A��*
+
+train/value_lossܺ�J��:�        )�P	n�Kc��A��*
+
+time/fps  �DŖ�'       ��F	n�Kc��A��*
+
+train/approx_kl�Z9���+       ��K	n�Kc��A��*
+
+train/clip_fraction    ����(       �pJ	��Kc��A��*
+
+train/clip_range��L>%��T*       ����	��Kc��A��*
+
+train/entropy_lossN������/0       ���_	��Kc��A��*!
+
+train/explained_variance   �}V�+       ��K	��Kc��A��*
+
+train/learning_rateRI�9X �"       x=�	��Kc��A��*
+
+
+train/loss\�?II�2       $V�	��Kc��A��*#
+!
+train/policy_gradient_loss*䧹��(       �pJ	��Kc��A��*
+
+train/value_losst��I8�N        )�P	�.Lc��A��
+*
+
+time/fps  �D&&f'       ��F	�.Lc��A��
+*
+
+train/approx_kl�;���+       ��K	�.Lc��A��
+*
+
+train/clip_fraction`,�;0%�i(       �pJ	�.Lc��A��
+*
+
+train/clip_range��L>w���*       ����	�.Lc��A��
+*
+
+train/entropy_loss�Ƌ�2��0       ���_	�.Lc��A��
+*!
+
+train/explained_variance    nDj�+       ��K	�.Lc��A��
+*
+
+train/learning_rateRI�97���"       x=�	�.Lc��A��
+*
+
+
+train/loss��jG�Qx2       $V�	�.Lc��A��
+*#
+!
+train/policy_gradient_loss��lb��(       �pJ	�.Lc��A��
+*
+
+train/value_lossm'�G�4�-        )�P	�ܨLc��A��
+*
+
+time/fps �D,��'       ��F	�ܨLc��A��
+*
+
+train/approx_klL�;��PL+       ��K	�ܨLc��A��
+*
+
+train/clip_fraction    j���(       �pJ	�ܨLc��A��
+*
+
+train/clip_range��L>Uk3�*       ����	�ܨLc��A��
+*
+
+train/entropy_loss�e��W}NH0       ���_	�ܨLc��A��
+*!
+
+train/explained_variance   �Ƽxv+       ��K	�ܨLc��A��
+*
+
+train/learning_rateRI�9w?�3"       x=�	�ܨLc��A��
+*
+
+
+train/lossXXGGXءT2       $V�	�ܨLc��A��
+*#
+!
+train/policy_gradient_loss�ĺ�^](       �pJ	�ܨLc��A��
+*
+
+train/value_loss�"�G�G�
+        )�P	JMc��A��
+*
+
+time/fps �D��U�'       ��F	JMc��A��
+*
+
+train/approx_kl=��;5f��+       ��K	JMc��A��
+*
+
+train/clip_fraction��H<B��(       �pJ	JMc��A��
+*
+
+train/clip_range��L>Y	M�*       ����	JMc��A��
+*
+
+train/entropy_loss!����ے�0       ���_	JMc��A��
+*!
+
+train/explained_variance    ~@�l+       ��K	JMc��A��
+*
+
+train/learning_rateRI�9f���"       x=�	JMc��A��
+*
+
+
+train/lossyPG�=E�2       $V�	JMc��A��
+*#
+!
+train/policy_gradient_loss@�Q��+�(       �pJ	JMc��A��
+*
+
+train/value_loss�j�GH�z         )�P	�_�Mc��A��
+*
+
+time/fps �D�v'       ��F	�_�Mc��A��
+*
+
+train/approx_klz�[;{��+       ��K	�_�Mc��A��
+*
+
+train/clip_fraction    /*��(       �pJ	�_�Mc��A��
+*
+
+train/clip_range��L>��f�*       ����	�_�Mc��A��
+*
+
+train/entropy_loss��������0       ���_	�_�Mc��A��
+*!
+
+train/explained_variance   �ߌz�+       ��K	�_�Mc��A��
+*
+
+train/learning_rateRI�92��0"       x=�	�_�Mc��A��
+*
+
+
+train/loss_��G�4��2       $V�	�_�Mc��A��
+*#
+!
+train/policy_gradient_loss����&7��(       �pJ	�_�Mc��A��
+*
+
+train/value_lossbe]H�O�         )�P	x�Mc��A��
+*
+
+time/fps �D{��'       ��F	x�Mc��A��
+*
+
+train/approx_kl@*:	��/+       ��K	x�Mc��A��
+*
+
+train/clip_fraction    �2z�(       �pJ	x�Mc��A��
+*
+
+train/clip_range��L>.��K*       ����	x�Mc��A��
+*
+
+train/entropy_lossݱ���d0       ���_	x�Mc��A��
+*!
+
+train/explained_variance   ��~��+       ��K	x�Mc��A��
+*
+
+train/learning_rateRI�9��^G"       x=�	x�Mc��A��
+*
+
+
+train/loss��aJB�!�2       $V�	x�Mc��A��
+*#
+!
+train/policy_gradient_loss�����(       �pJ	x�Mc��A��
+*
+
+train/value_loss��J_j��        )�P	� fNc��A��
+*
+
+time/fps  �D0i'       ��F	� fNc��A��
+*
+
+train/approx_kl�9��l�+       ��K	� fNc��A��
+*
+
+train/clip_fraction    p��s(       �pJ	� fNc��A��
+*
+
+train/clip_range��L>tŪ�*       ����	� fNc��A��
+*
+
+train/entropy_loss�s����&�0       ���_	� fNc��A��
+*!
+
+train/explained_variance  �4Q�ӂ+       ��K	� fNc��A��
+*
+
+train/learning_rateRI�9m1��"       x=�	� fNc��A��
+*
+
+
+train/loss�:aK|£2       $V�	� fNc��A��
+*#
+!
+train/policy_gradient_loss�ι1(       �pJ	� fNc��A��
+*
+
+train/value_loss���K�i�        )�P	1��Nc��A��
+*
+
+time/fps  �DV3_�'       ��F	1��Nc��A��
+*
+
+train/approx_kl}��7v�u1+       ��K	1��Nc��A��
+*
+
+train/clip_fraction    H���(       �pJ	1��Nc��A��
+*
+
+train/clip_range��L>�H��*       ����	1��Nc��A��
+*
+
+train/entropy_lossV��t/0       ���_	1��Nc��A��
+*!
+
+train/explained_variance  �4��?+       ��K	1��Nc��A��
+*
+
+train/learning_rateRI�9U\�J"       x=�	1��Nc��A��
+*
+
+
+train/lossa��K�' 2       $V�	1��Nc��A��
+*#
+!
+train/policy_gradient_lossu"��޹r(       �pJ	1��Nc��A��
+*
+
+train/value_loss'qL>!w�        )�P	{�FOc��AА*
+
+time/fps  �DUO�'       ��F	{�FOc��AА*
+
+train/approx_klm�@8��i�+       ��K	{�FOc��AА*
+
+train/clip_fraction    �}dS(       �pJ	{�FOc��AА*
+
+train/clip_range��L>D/�	*       ����	{�FOc��AА*
+
+train/entropy_loss�a����+0       ���_	{�FOc��AА*!
+
+train/explained_variance  �4\�v+       ��K	{�FOc��AА*
+
+train/learning_rateRI�9eP�"       x=�	{�FOc��AА*
+
+
+train/loss*�KC�hF2       $V�	{�FOc��AА*#
+!
+train/policy_gradient_lossU���f��(       �pJ	{�FOc��AА*
+
+train/value_loss�aL_���        )�P	��Oc��A��*
+
+time/fps  �D4��i'       ��F	��Oc��A��*
+
+train/approx_kl�+8�	wx+       ��K	��Oc��A��*
+
+train/clip_fraction    -&��(       �pJ	��Oc��A��*
+
+train/clip_range��L>��[*       ����	��Oc��A��*
+
+train/entropy_loss�S���}0       ���_	��Oc��A��*!
+
+train/explained_variance  ���4(�+       ��K	��Oc��A��*
+
+train/learning_rateRI�90��Q"       x=�	��Oc��A��*
+
+
+train/lossJ�L=e�2       $V�	��Oc��A��*#
+!
+train/policy_gradient_loss� \�o� �(       �pJ	��Oc��A��*
+
+train/value_loss��Ls��z        )�P	��&Pc��A��*
+
+time/fps  �DB�'       ��F	��&Pc��A��*
+
+train/approx_kl5�7���_+       ��K	��&Pc��A��*
+
+train/clip_fraction    "	>(       �pJ	��&Pc��A��*
+
+train/clip_range��L>IY��*       ����	��&Pc��A��*
+
+train/entropy_loss�S�����0       ���_	��&Pc��A��*!
+
+train/explained_variance  @�-=+       ��K	��&Pc��A��*
+
+train/learning_rateRI�9�
+� "       x=�	��&Pc��A��*
+
+
+train/loss�{�K�h�2       $V�	��&Pc��A��*#
+!
+train/policy_gradient_loss���8���(       �pJ	��&Pc��A��*
+
+train/value_loss�`L��b	        )�P	t�Pc��A��*
+
+time/fps  �Da���'       ��F	t�Pc��A��*
+
+train/approx_kl��7b� �+       ��K	t�Pc��A��*
+
+train/clip_fraction    ��X�(       �pJ	t�Pc��A��*
+
+train/clip_range��L>�jjb*       ����	t�Pc��A��*
+
+train/entropy_losspW�����0       ���_	��Pc��A��*!
+
+train/explained_variance  �4{�t)+       ��K	��Pc��A��*
+
+train/learning_rateRI�9��"       x=�	��Pc��A��*
+
+
+train/loss��)L�$��2       $V�	��Pc��A��*#
+!
+train/policy_gradient_lossm�߸s�Q(       �pJ	��Pc��A��*
+
+train/value_loss4;�L� �(        )�P	��Qc��A��*
+
+time/fps  �Dt��'       ��F	��Qc��A��*
+
+train/approx_kl	Q8�b��+       ��K	��Qc��A��*
+
+train/clip_fraction    � n�(       �pJ	��Qc��A��*
+
+train/clip_range��L>�1l�*       ����	��Qc��A��*
+
+train/entropy_loss�Ǌ�w0�0       ���_	��Qc��A��*!
+
+train/explained_variance  @4+#u2+       ��K	��Qc��A��*
+
+train/learning_rateRI�9��JG"       x=�	��Qc��A��*
+
+
+train/loss�VLC��*2       $V�	��Qc��A��*#
+!
+train/policy_gradient_loss�iE��Y5(       �pJ	��Qc��A��*
+
+train/value_loss��L��U�        )�P	d*wQc��A��*
+
+time/fps  �D�o'       ��F	d*wQc��A��*
+
+train/approx_kl�j�7e��+       ��K	d*wQc��A��*
+
+train/clip_fraction    �!(       �pJ	d*wQc��A��*
+
+train/clip_range��L>O��*       ����	d*wQc��A��*
+
+train/entropy_loss(�����!�0       ���_	d*wQc��A��*!
+
+train/explained_variance  ��N��h+       ��K	d*wQc��A��*
+
+train/learning_rateRI�9'�ʝ"       x=�	d*wQc��A��*
+
+
+train/lossb��L4T�Z2       $V�	d*wQc��A��*#
+!
+train/policy_gradient_loss��K��@��(       �pJ	d*wQc��A��*
+
+train/value_loss��M�,�o        )�P	Qr�Qc��A��*
+
+time/fps �D��1%'       ��F	Qr�Qc��A��*
+
+train/approx_kl��564�
+a+       ��K	Qr�Qc��A��*
+
+train/clip_fraction    �~K�(       �pJ	Qr�Qc��A��*
+
+train/clip_range��L>=�Q�*       ����	Qr�Qc��A��*
+
+train/entropy_loss�e��$�q0       ���_	Qr�Qc��A��*!
+
+train/explained_variance H�9	��+       ��K	Qr�Qc��A��*
+
+train/learning_rateRI�9�eg7"       x=�	Qr�Qc��A��*
+
+
+train/loss�̕LrA��2       $V�	Qr�Qc��A��*#
+!
+train/policy_gradient_lossr���&u��(       �pJ	Qr�Qc��A��*
+
+train/value_loss�bMm��	        )�P	��]Rc��A��*
+
+time/fps �D�\G�'       ��F	L�]Rc��A��*
+
+train/approx_klt�B7�?�+       ��K	L�]Rc��A��*
+
+train/clip_fraction    ^T`(       �pJ	L�]Rc��A��*
+
+train/clip_range��L>{3�*       ����	L�]Rc��A��*
+
+train/entropy_loss���.�ɉ0       ���_	L�]Rc��A��*!
+
+train/explained_variance  h�сrf+       ��K	L�]Rc��A��*
+
+train/learning_rateRI�9%���"       x=�	L�]Rc��A��*
+
+
+train/loss���L�-(2       $V�	L�]Rc��A��*#
+!
+train/policy_gradient_loss>.��?RL(       �pJ	L�]Rc��A��*
+
+train/value_loss�D-M$��        )�P	��Rc��AЯ*
+
+time/fps �D�h�{'       ��F	��Rc��AЯ*
+
+train/approx_kl�[7sQЩ+       ��K	��Rc��AЯ*
+
+train/clip_fraction    (>�(       �pJ	��Rc��AЯ*
+
+train/clip_range��L>xm(*       ����	 �Rc��AЯ*
+
+train/entropy_loss$(���g@�0       ���_	 �Rc��AЯ*!
+
+train/explained_variance  �6��bm+       ��K	 �Rc��AЯ*
+
+train/learning_rateRI�9\�b�"       x=�	 �Rc��AЯ*
+
+
+train/loss?m^L)��2       $V�	 �Rc��AЯ*#
+!
+train/policy_gradient_loss����gJ�(       �pJ	 �Rc��AЯ*
+
+train/value_loss 2�LDl�        )�P	VsCSc��A��*
+
+time/fps �D�K'       ��F	VsCSc��A��*
+
+train/approx_kl�N�7,�[+       ��K	VsCSc��A��*
+
+train/clip_fraction    �+_(       �pJ	VsCSc��A��*
+
+train/clip_range��L>s7�*       ����	VsCSc��A��*
+
+train/entropy_loss4��Ьz�0       ���_	VsCSc��A��*!
+
+train/explained_variance  �m.�p+       ��K	VsCSc��A��*
+
+train/learning_rateRI�9��s�"       x=�	VsCSc��A��*
+
+
+train/losskG_L�y92       $V�	VsCSc��A��*#
+!
+train/policy_gradient_loss�C��-(       �pJ	��CSc��A��*
+
+train/value_lossd�LV�        )�P	�Sc��A��*
+
+time/fps �D-)�
+'       ��F	w�Sc��A��*
+
+train/approx_klE�i6�,X�+       ��K	w�Sc��A��*
+
+train/clip_fraction    �z2~(       �pJ	w�Sc��A��*
+
+train/clip_range��L>�$�*       ����	w�Sc��A��*
+
+train/entropy_loss�F����NM0       ���_	w�Sc��A��*!
+
+train/explained_variance   5
+
+�+       ��K	w�Sc��A��*
+
+train/learning_rateRI�9�Q"       x=�	w�Sc��A��*
+
+
+train/loss�tL��Ƌ2       $V�	w�Sc��A��*#
+!
+train/policy_gradient_loss]b'�MLv�(       �pJ	w�Sc��A��*
+
+train/value_loss�Z�L��t�        )�P	FB#Tc��A��*
+
+time/fps �D�xu'       ��F	FB#Tc��A��*
+
+train/approx_kl��D9�c+       ��K	FB#Tc��A��*
+
+train/clip_fraction    0�(       �pJ	FB#Tc��A��*
+
+train/clip_range��L>��8�*       ����	FB#Tc��A��*
+
+train/entropy_loss̈���씍0       ���_	FB#Tc��A��*!
+
+train/explained_variance   4�F�z+       ��K	FB#Tc��A��*
+
+train/learning_rateRI�9-��?"       x=�	FB#Tc��A��*
+
+
+train/loss��dL�9�2       $V�	FB#Tc��A��*#
+!
+train/policy_gradient_loss�Z�4��(       �pJ	FB#Tc��A��*
+
+train/value_loss�/�L���        )�P	���Tc��A��*
+
+time/fps �D/D2'       ��F	���Tc��A��*
+
+train/approx_klw<(9��O+       ��K	���Tc��A��*
+
+train/clip_fraction    8�y (       �pJ	���Tc��A��*
+
+train/clip_range��L>��{*       ����	���Tc��A��*
+
+train/entropy_loss-��c0       ���_	���Tc��A��*!
+
+train/explained_variance  ��t��&+       ��K	���Tc��A��*
+
+train/learning_rateRI�9E�^�"       x=�	���Tc��A��*
+
+
+train/lossLQdL����2       $V�	���Tc��A��*#
+!
+train/policy_gradient_loss�1ϹSm"(       �pJ	���Tc��A��*
+
+train/value_loss	��L��É        )�P	IjUc��A��*
+
+time/fps �D�c�y'       ��F	IjUc��A��*
+
+train/approx_kl>9���+       ��K	IjUc��A��*
+
+train/clip_fraction    �'^#(       �pJ	IjUc��A��*
+
+train/clip_range��L>E��L*       ����	IjUc��A��*
+
+train/entropy_loss΋��Gp0       ���_	IjUc��A��*!
+
+train/explained_variance   ��u��+       ��K	IjUc��A��*
+
+train/learning_rateRI�9��z�"       x=�	IjUc��A��*
+
+
+train/loss��K\G�2       $V�	IjUc��A��*#
+!
+train/policy_gradient_loss�q
+��ʹ�(       �pJ	IjUc��A��*
+
+train/value_loss�}L`���        )�P	�sUc��A�*
+
+time/fps �D����'       ��F	�sUc��A�*
+
+train/approx_klB�9�$q+       ��K	�sUc��A�*
+
+train/clip_fraction    �rc�(       �pJ	�sUc��A�*
+
+train/clip_range��L>�2�}*       ����	�sUc��A�*
+
+train/entropy_loss������j0       ���_	�sUc��A�*!
+
+train/explained_variance    ��o�+       ��K	�sUc��A�*
+
+train/learning_rateRI�9�YO
+"       x=�	�sUc��A�*
+
+
+train/lossB��K!z2       $V�	�sUc��A�*#
+!
+train/policy_gradient_loss-��'{�g(       �pJ	�sUc��A�*
+
+train/value_loss
+�ULZhs�        )�P	���Uc��A�*
+
+time/fps �D�x�'       ��F	���Uc��A�*
+
+train/approx_klv̦8'S1+       ��K	���Uc��A�*
+
+train/clip_fraction    "u(       �pJ	���Uc��A�*
+
+train/clip_range��L>�J��*       ����	���Uc��A�*
+
+train/entropy_loss�����e0       ���_	���Uc��A�*!
+
+train/explained_variance  ���Xu+       ��K	���Uc��A�*
+
+train/learning_rateRI�9%���"       x=�	���Uc��A�*
+
+
+train/loss��L/��2       $V�	���Uc��A�*#
+!
+train/policy_gradient_lossT�6�1'��(       �pJ	���Uc��A�*
+
+train/value_loss8Y�L�ˌ[        )�P	~PVc��A��*
+
+time/fps �D�'       ��F	~PVc��A��*
+
+train/approx_kl�d�8X+:	+       ��K	~PVc��A��*
+
+train/clip_fraction    ��_(       �pJ	~PVc��A��*
+
+train/clip_range��L>��*       ����	~PVc��A��*
+
+train/entropy_loss^��:X޼0       ���_	~PVc��A��*!
+
+train/explained_variance   �a�f�+       ��K	~PVc��A��*
+
+train/learning_rateRI�9����"       x=�	~PVc��A��*
+
+
+train/loss�9LX��R2       $V�	~PVc��A��*#
+!
+train/policy_gradient_lossb����(       �pJ	~PVc��A��*
+
+train/value_lossRݎLB]�        )�P	 ��Vc��A��*
+
+time/fps  �D:�T'       ��F	 ��Vc��A��*
+
+train/approx_kl�Q�7�c�M+       ��K	 ��Vc��A��*
+
+train/clip_fraction    �F.-(       �pJ	 ��Vc��A��*
+
+train/clip_range��L>��4�*       ����	 ��Vc��A��*
+
+train/entropy_lossn����XI0       ���_	 ��Vc��A��*!
+
+train/explained_variance   5>ɭ.+       ��K	 ��Vc��A��*
+
+train/learning_rateRI�9��
+�"       x=�	 ��Vc��A��*
+
+
+train/lossA�L�I�2       $V�	 ��Vc��A��*#
+!
+train/policy_gradient_loss�����.(       �pJ	 ��Vc��A��*
+
+train/value_lossx M��.+        )�P	r�1Wc��A��*
+
+time/fps �Dv|��'       ��F	r�1Wc��A��*
+
+train/approx_kl{��6��8�+       ��K	r�1Wc��A��*
+
+train/clip_fraction    p��(       �pJ	r�1Wc��A��*
+
+train/clip_range��L>���*       ����	r�1Wc��A��*
+
+train/entropy_lossg����wώ0       ���_	r�1Wc��A��*!
+
+train/explained_variance  �5��qE+       ��K	��1Wc��A��*
+
+train/learning_rateRI�9�J�"       x=�	��1Wc��A��*
+
+
+train/loss��L�$��2       $V�	��1Wc��A��*#
+!
+train/policy_gradient_loss����.�(       �pJ	��1Wc��A��*
+
+train/value_loss�<M���f        )�P	+ޡWc��A��*
+
+time/fps �DmA��'       ��F	+ޡWc��A��*
+
+train/approx_klm�i60z,+       ��K	+ޡWc��A��*
+
+train/clip_fraction     Y.�(       �pJ	+ޡWc��A��*
+
+train/clip_range��L>rf��*       ����	+ޡWc��A��*
+
+train/entropy_loss����|��0       ���_	+ޡWc��A��*!
+
+train/explained_variance   ���+       ��K	+ޡWc��A��*
+
+train/learning_rateRI�9r
+
+"       x=�	+ޡWc��A��*
+
+
+train/lossP��L��nm2       $V�	+ޡWc��A��*#
+!
+train/policy_gradient_loss�����yY(       �pJ	+ޡWc��A��*
+
+train/value_loss�	MM3��        )�P	��Xc��A��*
+
+time/fps �D�u��'       ��F	��Xc��A��*
+
+train/approx_kl��C7²��+       ��K	��Xc��A��*
+
+train/clip_fraction    -F(       �pJ	��Xc��A��*
+
+train/clip_range��L>0"��*       ����	��Xc��A��*
+
+train/entropy_lossK=���\S60       ���_	��Xc��A��*!
+
+train/explained_variance  ���+       ��K	��Xc��A��*
+
+train/learning_rateRI�90�1�"       x=�	��Xc��A��*
+
+
+train/loss�v M�E�02       $V�	��Xc��A��*#
+!
+train/policy_gradient_loss}��)�x(       �pJ	��Xc��A��*
+
+train/value_loss�T�M��8        )�P	�H�Xc��A��*
+
+time/fps �DV��y'       ��F	�H�Xc��A��*
+
+train/approx_klU
+q5�b��+       ��K	�H�Xc��A��*
+
+train/clip_fraction    =V)�(       �pJ	�H�Xc��A��*
+
+train/clip_range��L>�2X*       ����	�H�Xc��A��*
+
+train/entropy_loss�q��K V0       ���_	�H�Xc��A��*!
+
+train/explained_variance  ��3��t+       ��K	�H�Xc��A��*
+
+train/learning_rateRI�9@m
+"       x=�	�H�Xc��A��*
+
+
+train/losstMS��22       $V�	�H�Xc��A��*#
+!
+train/policy_gradient_lossb᧷�+�[(       �pJ	�H�Xc��A��*
+
+train/value_loss�^�M(�5        )�P	7�Xc��A��*
+
+time/fps  �Ds<1Z'       ��F	7�Xc��A��*
+
+train/approx_kl~�852��+       ��K	7�Xc��A��*
+
+train/clip_fraction    w_P(       �pJ	7�Xc��A��*
+
+train/clip_range��L>�N�a*       ����	7�Xc��A��*
+
+train/entropy_lossfʋ�����0       ���_	7�Xc��A��*!
+
+train/explained_variance    '�)+       ��K	7�Xc��A��*
+
+train/learning_rateRI�9%Ps�"       x=�	7�Xc��A��*
+
+
+train/loss7�QM ��.2       $V�	7�Xc��A��*#
+!
+train/policy_gradient_loss�����_>(       �pJ	7�Xc��A��*
+
+train/value_loss*��M���        )�P	�	aYc��A��*
+
+time/fps  �D�,�j'       ��F	�	aYc��A��*
+
+train/approx_kl
+k�5�<�+       ��K	�	aYc��A��*
+
+train/clip_fraction    ��q�(       �pJ	�	aYc��A��*
+
+train/clip_range��L>���#*       ����	�	aYc��A��*
+
+train/entropy_lossw"��O60       ���_	�	aYc��A��*!
+
+train/explained_variance    �v[�+       ��K	�	aYc��A��*
+
+train/learning_rateRI�9��F#"       x=�	�	aYc��A��*
+
+
+train/losssEsM���L2       $V�	�	aYc��A��*#
+!
+train/policy_gradient_lossR�^��rOg(       �pJ	�	aYc��A��*
+
+train/value_loss�e�M�[.         )�P	���Yc��A��*
+
+time/fps  �D�d��'       ��F	���Yc��A��*
+
+train/approx_kl:��5!��+       ��K	���Yc��A��*
+
+train/clip_fraction    A���(       �pJ	���Yc��A��*
+
+train/clip_range��L>m$F*       ����	���Yc��A��*
+
+train/entropy_loss$1��4�N!0       ���_	���Yc��A��*!
+
+train/explained_variance    ����+       ��K	���Yc��A��*
+
+train/learning_rateRI�9<<�S"       x=�	���Yc��A��*
+
+
+train/loss�klM#�1*2       $V�	���Yc��A��*#
+!
+train/policy_gradient_loss�@&�DL��(       �pJ	���Yc��A��*
+
+train/value_lossR��M��        )�P	^ABZc��A��*
+
+time/fps  �D�*X'       ��F	^ABZc��A��*
+
+train/approx_kltB5�G�+       ��K	^ABZc��A��*
+
+train/clip_fraction    ���T(       �pJ	^ABZc��A��*
+
+train/clip_range��L>��yw*       ����	^ABZc��A��*
+
+train/entropy_loss߯��rM�0       ���_	^ABZc��A��*!
+
+train/explained_variance  @4
+Q�+       ��K	^ABZc��A��*
+
+train/learning_rateRI�9���"       x=�	^ABZc��A��*
+
+
+train/lossg�oMN ��2       $V�	^ABZc��A��*#
+!
+train/policy_gradient_lossD=w��,��(       �pJ	^ABZc��A��*
+
+train/value_loss��M��%        )�P	��Zc��A��*
+
+time/fps  �D���J'       ��F	��Zc��A��*
+
+train/approx_kl�q5�+��+       ��K	��Zc��A��*
+
+train/clip_fraction    �ϸ�(       �pJ	��Zc��A��*
+
+train/clip_range��L>����*       ����	��Zc��A��*
+
+train/entropy_loss����0��x0       ���_	��Zc��A��*!
+
+train/explained_variance    �	'+       ��K	��Zc��A��*
+
+train/learning_rateRI�9���y"       x=�	��Zc��A��*
+
+
+train/loss�4�M�V�Y2       $V�	��Zc��A��*#
+!
+train/policy_gradient_loss��Ʒ�
+�'(       �pJ	��Zc��A��*
+
+train/value_loss� N��        )�P	- [c��A��*
+
+time/fps  �D{"O�'       ��F	- [c��A��*
+
+train/approx_kl3߰5��#�+       ��K	- [c��A��*
+
+train/clip_fraction    <��(       �pJ	- [c��A��*
+
+train/clip_range��L>����*       ����	- [c��A��*
+
+train/entropy_loss�a��j*��0       ���_	- [c��A��*!
+
+train/explained_variance    ��qA+       ��K	- [c��A��*
+
+train/learning_rateRI�9A
+�"       x=�	- [c��A��*
+
+
+train/loss{M7B�a2       $V�	- [c��A��*#
+!
+train/policy_gradient_loss�Aﷺ�9;(       �pJ	- [c��A��*
+
+train/value_loss%��M@!�        )�P	���[c��A��*
+
+time/fps  �D��'       ��F	���[c��A��*
+
+train/approx_kl��6#
+O*+       ��K	���[c��A��*
+
+train/clip_fraction    d)(       �pJ	���[c��A��*
+
+train/clip_range��L>kɷ^*       ����	���[c��A��*
+
+train/entropy_loss"q��	�0       ���_	���[c��A��*!
+
+train/explained_variance    j ��+       ��K	���[c��A��*
+
+train/learning_rateRI�9��P�"       x=�	���[c��A��*
+
+
+train/lossl2yMB��}2       $V�	���[c��A��*#
+!
+train/policy_gradient_loss`�q�&��(       �pJ	���[c��A��*
+
+train/value_loss�<�M�C8�        )�P	� \c��A��*
+
+time/fps  �D��}�'       ��F	� \c��A��*
+
+train/approx_kl��E5���+       ��K	� \c��A��*
+
+train/clip_fraction    ���(       �pJ	� \c��A��*
+
+train/clip_range��L>�3�*       ����	� \c��A��*
+
+train/entropy_loss���Oݽ40       ���_	� \c��A��*!
+
+train/explained_variance   �KJ�5+       ��K	� \c��A��*
+
+train/learning_rateRI�9~��<"       x=�	� \c��A��*
+
+
+train/loss�YM��*2       $V�	� \c��A��*#
+!
+train/policy_gradient_loss}��V��(       �pJ	� \c��A��*
+
+train/value_lossjt�M`P�        )�P	Tr\c��A��*
+
+time/fps  �D�&l�'       ��F	Tr\c��A��*
+
+train/approx_kl)�6H��+       ��K	Tr\c��A��*
+
+train/clip_fraction    M4W1(       �pJ	Tr\c��A��*
+
+train/clip_range��L>m�t*       ����	Tr\c��A��*
+
+train/entropy_loss6���7uW0       ���_	Tr\c��A��*!
+
+train/explained_variance   �1_"|+       ��K	Tr\c��A��*
+
+train/learning_rateRI�9P�{�"       x=�	Tr\c��A��*
+
+
+train/loss��zM"2�2       $V�	Tr\c��A��*#
+!
+train/policy_gradient_loss�2_��1͸(       �pJ	Tr\c��A��*
+
+train/value_loss�t�M�-^.        )�P	��\c��A��*
+
+time/fps  �D��:'       ��F	��\c��A��*
+
+train/approx_kl�x�6}�,+       ��K	��\c��A��*
+
+train/clip_fraction    捷�(       �pJ	��\c��A��*
+
+train/clip_range��L>�lB�*       ����	��\c��A��*
+
+train/entropy_loss]芿��U�0       ���_	��\c��A��*!
+
+train/explained_variance   4
+�x�+       ��K	��\c��A��*
+
+train/learning_rateRI�9�4�6"       x=�	��\c��A��*
+
+
+train/loss��M�x�2       $V�	��\c��A��*#
+!
+train/policy_gradient_losso���t��(       �pJ	��\c��A��*
+
+train/value_lossna�M/1W        )�P	.�S]c��AЌ*
+
+time/fps  �D��'       ��F	.�S]c��AЌ*
+
+train/approx_klڑ�6���+       ��K	.�S]c��AЌ*
+
+train/clip_fraction    a(       �pJ	.�S]c��AЌ*
+
+train/clip_range��L>�JE*       ����	.�S]c��AЌ*
+
+train/entropy_loss�2��hT[0       ���_	.�S]c��AЌ*!
+
+train/explained_variance    pAVJ+       ��K	.�S]c��AЌ*
+
+train/learning_rateRI�9!�:�"       x=�	.�S]c��AЌ*
+
+
+train/loss�5�M�M2       $V�	.�S]c��AЌ*#
+!
+train/policy_gradient_loss�ָ���s(       �pJ	.�S]c��AЌ*
+
+train/value_losso?N���        )�P	=�]c��A��*
+
+time/fps  �D�~��'       ��F	=�]c��A��*
+
+train/approx_kl}�a7��	
++       ��K	=�]c��A��*
+
+train/clip_fraction    ���(       �pJ	=�]c��A��*
+
+train/clip_range��L>[T=�*       ����	=�]c��A��*
+
+train/entropy_lossq�����d�0       ���_	=�]c��A��*!
+
+train/explained_variance    '!P�+       ��K	=�]c��A��*
+
+train/learning_rateRI�9��;["       x=�	=�]c��A��*
+
+
+train/loss&OuMZ���2       $V�	=�]c��A��*#
+!
+train/policy_gradient_lossY�-��ٺV(       �pJ	=�]c��A��*
+
+train/value_loss���MO��        )�P	�M4^c��A��*
+
+time/fps  �DEߙ�'       ��F	�M4^c��A��*
+
+train/approx_kl5% 8|A��+       ��K	�M4^c��A��*
+
+train/clip_fraction    
+$��(       �pJ	�M4^c��A��*
+
+train/clip_range��L> ��*       ����	�M4^c��A��*
+
+train/entropy_loss+�:� 0       ���_	�M4^c��A��*!
+
+train/explained_variance   4/I�+       ��K	�M4^c��A��*
+
+train/learning_rateRI�9��a"       x=�	�M4^c��A��*
+
+
+train/loss��gM��B2       $V�	�M4^c��A��*#
+!
+train/policy_gradient_loss�9m�:��(       �pJ	�M4^c��A��*
+
+train/value_loss�_�Mz:%        )�P	�>�^c��A��*
+
+time/fps  �D��/�'       ��F	�>�^c��A��*
+
+train/approx_kl�C6�(�+       ��K	�>�^c��A��*
+
+train/clip_fraction    '�|(       �pJ	�>�^c��A��*
+
+train/clip_range��L>��*       ����	�>�^c��A��*
+
+train/entropy_lossD
+��8
+)�0       ���_	�>�^c��A��*!
+
+train/explained_variance    ? q�+       ��K	�>�^c��A��*
+
+train/learning_rateRI�91X�"       x=�	�>�^c��A��*
+
+
+train/loss�|�M�A�N2       $V�	�>�^c��A��*#
+!
+train/policy_gradient_loss�=�=Vm�(       �pJ	�>�^c��A��*
+
+train/value_loss��NB�9�        )�P	�R_c��A��*
+
+time/fps  �D���'       ��F	�R_c��A��*
+
+train/approx_kl~"6ʓ�2+       ��K	�R_c��A��*
+
+train/clip_fraction    �n�(       �pJ	�R_c��A��*
+
+train/clip_range��L>N��*       ����	�R_c��A��*
+
+train/entropy_loss�����)�0       ���_	�R_c��A��*!
+
+train/explained_variance    �\�+       ��K	�R_c��A��*
+
+train/learning_rateRI�9��Je"       x=�	�R_c��A��*
+
+
+train/loss��xMB7��2       $V�	�R_c��A��*#
+!
+train/policy_gradient_loss��3.�(       �pJ	�R_c��A��*
+
+train/value_loss���M��U�        )�P	q��_c��A��*
+
+time/fps  �D�� '       ��F	q��_c��A��*
+
+train/approx_kl��+6����+       ��K	q��_c��A��*
+
+train/clip_fraction    �tKV(       �pJ	q��_c��A��*
+
+train/clip_range��L>��A
+*       ����	q��_c��A��*
+
+train/entropy_lossu}��f�y~0       ���_	q��_c��A��*!
+
+train/explained_variance    ���P+       ��K	q��_c��A��*
+
+train/learning_rateRI�9�Kg�"       x=�	q��_c��A��*
+
+
+train/loss��lM,0�2       $V�	q��_c��A��*#
+!
+train/policy_gradient_loss�2���:�!(       �pJ	q��_c��A��*
+
+train/value_loss|��MB��        )�P	��_c��A��*
+
+time/fps  �D���'       ��F	��_c��A��*
+
+train/approx_kl*^)7��+       ��K	��_c��A��*
+
+train/clip_fraction    �DL�(       �pJ	��_c��A��*
+
+train/clip_range��L>�N״*       ����	��_c��A��*
+
+train/entropy_loss�����P�q0       ���_	��_c��A��*!
+
+train/explained_variance  @4̙E;+       ��K	��_c��A��*
+
+train/learning_rateRI�9�h="       x=�	��_c��A��*
+
+
+train/loss^��M�kQ�2       $V�	��_c��A��*#
+!
+train/policy_gradient_loss��Ƹչ�(       �pJ	��_c��A��*
+
+train/value_loss��N���w        )�P	��d`c��A��*
+
+time/fps  �Dzr�'       ��F	��d`c��A��*
+
+train/approx_kl���6����+       ��K	��d`c��A��*
+
+train/clip_fraction    QG�(       �pJ	��d`c��A��*
+
+train/clip_range��L>����*       ����	��d`c��A��*
+
+train/entropy_loss�񊿠�ц0       ���_	��d`c��A��*!
+
+train/explained_variance    J2�+       ��K	��d`c��A��*
+
+train/learning_rateRI�9L�ߊ"       x=�	��d`c��A��*
+
+
+train/loss�EtM��!2       $V�	��d`c��A��*#
+!
+train/policy_gradient_loss.���9��(       �pJ	��d`c��A��*
+
+train/value_lossz/�M�9��        )�P	�k�`c��AЫ*
+
+time/fps  �D� c '       ��F	�k�`c��AЫ*
+
+train/approx_kl��57ZW}+       ��K	�k�`c��AЫ*
+
+train/clip_fraction    |��-(       �pJ	�k�`c��AЫ*
+
+train/clip_range��L>�\]*       ����	�k�`c��AЫ*
+
+train/entropy_loss:늿����0       ���_	�k�`c��AЫ*!
+
+train/explained_variance   4m?��+       ��K	�k�`c��AЫ*
+
+train/learning_rateRI�9����"       x=�	�k�`c��AЫ*
+
+
+train/loss��1M0��92       $V�	�k�`c��AЫ*#
+!
+train/policy_gradient_loss�����(       �pJ	�k�`c��AЫ*
+
+train/value_lossɑ�M�ǂ        )�P	��Cac��A��*
+
+time/fps  �DWF�Y'       ��F	��Cac��A��*
+
+train/approx_kl��6�W�p+       ��K	��Cac��A��*
+
+train/clip_fraction    V\L,(       �pJ	��Cac��A��*
+
+train/clip_range��L> �p�*       ����	��Cac��A��*
+
+train/entropy_loss9Ȋ��qS0       ���_	��Cac��A��*!
+
+train/explained_variance    �u+       ��K	��Cac��A��*
+
+train/learning_rateRI�9K�h�"       x=�	��Cac��A��*
+
+
+train/loss֭3M��H�2       $V�	��Cac��A��*#
+!
+train/policy_gradient_loss�ฒZ.�(       �pJ	��Cac��A��*
+
+train/value_loss�´M�S��        )�P	hg�ac��A��*
+
+time/fps  �D��Q�'       ��F	hg�ac��A��*
+
+train/approx_kl&�8��2�+       ��K	hg�ac��A��*
+
+train/clip_fraction    H��(       �pJ	hg�ac��A��*
+
+train/clip_range��L>u�3*       ����	hg�ac��A��*
+
+train/entropy_lossK���<:�0       ���_	hg�ac��A��*!
+
+train/explained_variance    �P�+       ��K	hg�ac��A��*
+
+train/learning_rateRI�9UV�"       x=�	hg�ac��A��*
+
+
+train/lossCx@M��2       $V�	hg�ac��A��*#
+!
+train/policy_gradient_loss����h��&(       �pJ	hg�ac��A��*
+
+train/value_loss��M���D        )�P	�
+%bc��A��*
+
+time/fps  �D����'       ��F	�
+%bc��A��*
+
+train/approx_kl��8l�+       ��K	�
+%bc��A��*
+
+train/clip_fraction    "d(       �pJ	�
+%bc��A��*
+
+train/clip_range��L>���*       ����	�
+%bc��A��*
+
+train/entropy_lossೊ�10t0       ���_	�
+%bc��A��*!
+
+train/explained_variance    +��+       ��K	�
+%bc��A��*
+
+train/learning_rateRI�9�:�"       x=�	�
+%bc��A��*
+
+
+train/loss��M��Z2       $V�	�
+%bc��A��*#
+!
+train/policy_gradient_loss�� ���(       �pJ	�
+%bc��A��*
+
+train/value_loss\�Nȴ-        )�P	�D�bc��A��*
+
+time/fps  �Dџ6T'       ��F	�D�bc��A��*
+
+train/approx_kl ��8:`�+       ��K	�D�bc��A��*
+
+train/clip_fraction    �Y??(       �pJ	�D�bc��A��*
+
+train/clip_range��L>�_�*       ����	�D�bc��A��*
+
+train/entropy_lossj���9�0       ���_	�D�bc��A��*!
+
+train/explained_variance    Kp��+       ��K	�D�bc��A��*
+
+train/learning_rateRI�9�r�"       x=�	�D�bc��A��*
+
+
+train/loss;M�j#2       $V�	�D�bc��A��*#
+!
+train/policy_gradient_loss+�D�6��j(       �pJ	�D�bc��A��*
+
+train/value_loss�a�M�f�        )�P	��cc��A��*
+
+time/fps  �D'��'       ��F	��cc��A��*
+
+train/approx_kl���8�V�R+       ��K	��cc��A��*
+
+train/clip_fraction    	k�_(       �pJ	��cc��A��*
+
+train/clip_range��L>z��*       ����	��cc��A��*
+
+train/entropy_loss�f��l9�e0       ���_	��cc��A��*!
+
+train/explained_variance    �I,+       ��K	��cc��A��*
+
+train/learning_rateRI�9T��"       x=�	��cc��A��*
+
+
+train/lossz��Lף��2       $V�	��cc��A��*#
+!
+train/policy_gradient_loss1��,(       �pJ	��cc��A��*
+
+train/value_losskwM/"�y        )�P	��tcc��A�*
+
+time/fps  �D�ƙ'       ��F	��tcc��A�*
+
+train/approx_kl�8�7�i�+       ��K	��tcc��A�*
+
+train/clip_fraction    |���(       �pJ	��tcc��A�*
+
+train/clip_range��L>��	*       ����	��tcc��A�*
+
+train/entropy_lossE��]s�A0       ���_	��tcc��A�*!
+
+train/explained_variance    ����+       ��K	��tcc��A�*
+
+train/learning_rateRI�9�$�"       x=�	��tcc��A�*
+
+
+train/loss޾�L�̈́|2       $V�	��tcc��A�*#
+!
+train/policy_gradient_loss���X�$@(       �pJ	��tcc��A�*
+
+train/value_loss��hM�G�        )�P	^U�cc��A�*
+
+time/fps  �DV-:'       ��F	^U�cc��A�*
+
+train/approx_kl�8QsV�+       ��K	^U�cc��A�*
+
+train/clip_fraction    p4>(       �pJ	^U�cc��A�*
+
+train/clip_range��L>���*       ����	^U�cc��A�*
+
+train/entropy_loss���m0       ���_	^U�cc��A�*!
+
+train/explained_variance   4=�\f+       ��K	^U�cc��A�*
+
+train/learning_rateRI�9m��"       x=�	^U�cc��A�*
+
+
+train/loss�*�LN��r2       $V�	^U�cc��A�*#
+!
+train/policy_gradient_lossH���EaH(       �pJ	^U�cc��A�*
+
+train/value_lossYDMNƊ7        )�P	�Sdc��A��*
+
+time/fps  �D_��'       ��F	�Sdc��A��*
+
+train/approx_klv��9�k�I+       ��K	�Sdc��A��*
+
+train/clip_fraction    ��(       �pJ	�Sdc��A��*
+
+train/clip_range��L>VX"F*       ����	�Sdc��A��*
+
+train/entropy_lossU݊���:�0       ���_	�Sdc��A��*!
+
+train/explained_variance    ���b+       ��K	�Sdc��A��*
+
+train/learning_rateRI�9A'-"       x=�	�Sdc��A��*
+
+
+train/loss�PL9A	2       $V�	�Sdc��A��*#
+!
+train/policy_gradient_loss:��|q�(       �pJ	�Sdc��A��*
+
+train/value_lossT��L��k        )�P	00�dc��A��*
+
+time/fps  �D��,'       ��F	00�dc��A��*
+
+train/approx_kl"589�Vk�+       ��K	00�dc��A��*
+
+train/clip_fraction    ^a��(       �pJ	00�dc��A��*
+
+train/clip_range��L>}AՁ*       ����	�8�dc��A��*
+
+train/entropy_loss.���s�W0       ���_	�8�dc��A��*!
+
+train/explained_variance   ��kr�+       ��K	�8�dc��A��*
+
+train/learning_rateRI�9��.�"       x=�	�8�dc��A��*
+
+
+train/loss3gYL�'�2       $V�	�8�dc��A��*#
+!
+train/policy_gradient_loss���ꅢ<(       �pJ	�8�dc��A��*
+
+train/value_lossu�L2��K        )�P	�<:ec��A��*
+
+time/fps  �D�Z]-'       ��F	�<:ec��A��*
+
+train/approx_kl�
+�8��'+       ��K	�<:ec��A��*
+
+train/clip_fraction    ��'(       �pJ	�<:ec��A��*
+
+train/clip_range��L>���A*       ����	�<:ec��A��*
+
+train/entropy_lossI	����0       ���_	�<:ec��A��*!
+
+train/explained_variance    �*�+       ��K	�<:ec��A��*
+
+train/learning_rateRI�9�I1�"       x=�	�<:ec��A��*
+
+
+train/loss��XLWbc2       $V�	�<:ec��A��*#
+!
+train/policy_gradient_lossX�m��(       �pJ	�<:ec��A��*
+
+train/value_loss���L�`�
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_18/events.out.tfevents.1724754769.Trading.100052.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_18/events.out.tfevents.1724754769.Trading.100052.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_18/events.out.tfevents.1724754769.Trading.100052.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_18/events.out.tfevents.1724754769.Trading.100052.0	
@@ -0,0 +1,355 @@
+H       ��H�	?a�j��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writerr&�
+       QKD	3���j��A�c*
+
+time/fps `�D�#       ��wC	3���j��A�c*
+
+train/reward �¸Z�}        )�P	�k�j��A��*
+
+time/fps ��Dj���'       ��F	�k�j��A��*
+
+train/approx_kl��8E��+       ��K	�k�j��A��*
+
+train/clip_fraction    ���(       �pJ	�k�j��A��*
+
+train/clip_range��L>��n�*       ����	�k�j��A��*
+
+train/entropy_loss음�y��G0       ���_	�k�j��A��*!
+
+train/explained_variance  ���[��+       ��K	�k�j��A��*
+
+train/learning_rateRI�9��y"       x=�	�k�j��A��*
+
+
+train/loss�MI��X2       $V�	�k�j��A��*#
+!
+train/policy_gradient_loss�Kշm�$       B+�M	�k�j��A��*
+
+train/reward�W�Bu�d8(       �pJ	�k�j��A��*
+
+train/value_loss�o�I����        )�P	��B�j��A��*
+
+time/fps @qD���'       ��F	��B�j��A��*
+
+train/approx_klB�$:_��U+       ��K	��B�j��A��*
+
+train/clip_fraction    �C��(       �pJ	��B�j��A��*
+
+train/clip_range��L>˄x0*       ����	��B�j��A��*
+
+train/entropy_loss?����.��0       ���_	��B�j��A��*!
+
+train/explained_variance  �7��I;+       ��K	��B�j��A��*
+
+train/learning_rateRI�9~��="       x=�	��B�j��A��*
+
+
+train/lossXl�Ir�2       $V�	��B�j��A��*#
+!
+train/policy_gradient_loss�&��z߭�$       B+�M	��B�j��A��*
+
+train/reward���BW�/(       �pJ	��B�j��A��*
+
+train/value_loss�%JZ��a        )�P	�A�j��Ač*
+
+time/fps @hD�;E'       ��F	�A�j��Ač*
+
+train/approx_kl.��:4�+       ��K	�A�j��Ač*
+
+train/clip_fraction    ����(       �pJ	�A�j��Ač*
+
+train/clip_range��L>H��*       ����	�A�j��Ač*
+
+train/entropy_lossȑ�����0       ���_	�A�j��Ač*!
+
+train/explained_variance   4��+       ��K	�A�j��Ač*
+
+train/learning_rateRI�9|�d"       x=�	�A�j��Ač*
+
+
+train/lossY�J��ib2       $V�	�A�j��Ač*#
+!
+train/policy_gradient_loss�]%��Xu$       B+�M	�A�j��Ač*
+
+train/reward1����jn(       �pJ	�A�j��Ač*
+
+train/value_loss=)�Jvi�        )�P	i���j��A��*
+
+time/fps @dDX5K'       ��F	i���j��A��*
+
+train/approx_kl�y�;ۆ\@+       ��K	i���j��A��*
+
+train/clip_fraction    �|� (       �pJ	i���j��A��*
+
+train/clip_range��L>�^��*       ����	i���j��A��*
+
+train/entropy_loss	����|�Z0       ���_	i���j��A��*!
+
+train/explained_variance  �4$!��+       ��K	i���j��A��*
+
+train/learning_rateRI�9�c�|"       x=�	i���j��A��*
+
+
+train/lossK Ie!&s2       $V�	i���j��A��*#
+!
+train/policy_gradient_lossrv��QjP<$       B+�M	i���j��A��*
+
+train/rewardS�Cu 9v(       �pJ	i���j��A��*
+
+train/value_loss �Ih�-E        )�P	����j��A��*
+
+time/fps �_D�'       ��F	����j��A��*
+
+train/approx_klSL:q�{�+       ��K	����j��A��*
+
+train/clip_fraction    �a�(       �pJ	����j��A��*
+
+train/clip_range��L>y'��*       ����	����j��A��*
+
+train/entropy_loss��dW��0       ���_	����j��A��*!
+
+train/explained_variance  @4�_+       ��K	����j��A��*
+
+train/learning_rateRI�9�uX"       x=�	����j��A��*
+
+
+train/lossPJk�.2       $V�	����j��A��*#
+!
+train/policy_gradient_lossz������$       B+�M	����j��A��*
+
+train/reward����! +�(       �pJ	����j��A��*
+
+train/value_lossx�J��@        )�P	�Zs�j��A׷*
+
+time/fps �]Dcj�'       ��F	�Zs�j��A׷*
+
+train/approx_klɕ�9�˼+       ��K	�Zs�j��A׷*
+
+train/clip_fraction    %��8(       �pJ	�Zs�j��A׷*
+
+train/clip_range��L>�O��*       ����	�Zs�j��A׷*
+
+train/entropy_loss+ڋ�����0       ���_	�Zs�j��A׷*!
+
+train/explained_variance  �3�_�+       ��K	�Zs�j��A׷*
+
+train/learning_rateRI�94��"       x=�	�Zs�j��A׷*
+
+
+train/loss�W�I�!��2       $V�	�Zs�j��A׷*#
+!
+train/policy_gradient_loss,״)'��$       B+�M	�Zs�j��A׷*
+
+train/reward�f��N1(       �pJ	�Zs�j��A׷*
+
+train/value_lossg�J0�
+        )�P	�&'�j��A��*
+
+time/fps  ]D�*V�'       ��F	�&'�j��A��*
+
+train/approx_klD�:��d�+       ��K	�&'�j��A��*
+
+train/clip_fraction{+}:�S(       �pJ	�&'�j��A��*
+
+train/clip_range��L>Trw�*       ����	�&'�j��A��*
+
+train/entropy_loss�!���`��0       ���_	�&'�j��A��*!
+
+train/explained_variance  �4�0��+       ��K	�&'�j��A��*
+
+train/learning_rateRI�9u*��"       x=�	�&'�j��A��*
+
+
+train/loss�C5JܭN#2       $V�	�&'�j��A��*#
+!
+train/policy_gradient_loss�ظ<��$       B+�M	�&'�j��A��*
+
+train/reward��Ö��z(       �pJ	�&'�j��A��*
+
+train/value_lossz�JC�l        )�P	M&��j��A��*
+
+time/fps �[D��p'       ��F	M&��j��A��*
+
+train/approx_kl�C$7[;j�+       ��K	M&��j��A��*
+
+train/clip_fraction    h�p�(       �pJ	M&��j��A��*
+
+train/clip_range��L>���G*       ����	M&��j��A��*
+
+train/entropy_lossK����ԝ0       ���_	M&��j��A��*!
+
+train/explained_variance �����+       ��K	M&��j��A��*
+
+train/learning_rateRI�9uDt"       x=�	M&��j��A��*
+
+
+train/loss���Kz�-2       $V�	M&��j��A��*#
+!
+train/policy_gradient_loss�(�h�a$       B+�M	M&��j��A��*
+
+train/reward6(���X�o(       �pJ	M&��j��A��*
+
+train/value_loss�MLl3�k        )�P	:���j��A��*
+
+time/fps  [D@�1�'       ��F	:���j��A��*
+
+train/approx_kl���3�]n�+       ��K	:���j��A��*
+
+train/clip_fraction    R�g�(       �pJ	:���j��A��*
+
+train/clip_range��L>��8�*       ����	:���j��A��*
+
+train/entropy_loss����:�
+�0       ���_	:���j��A��*!
+
+train/explained_variance @����Ę+       ��K	:���j��A��*
+
+train/learning_rateRI�9O1K"       x=�	:���j��A��*
+
+
+train/loss9��K�#��2       $V�	:���j��A��*#
+!
+train/policy_gradient_loss��5g�-$       B+�M	:���j��A��*
+
+train/rewardvj��9�(       �pJ	:���j��A��*
+
+train/value_loss�J~L�}�        )�P	�5`�j��A��*
+
+time/fps �ZD�ӕ�'       ��F	�5`�j��A��*
+
+train/approx_kl���5;�+       ��K	�5`�j��A��*
+
+train/clip_fraction    ���(       �pJ	�5`�j��A��*
+
+train/clip_range��L>�L+�*       ����	�5`�j��A��*
+
+train/entropy_loss����Q��z0       ���_	�5`�j��A��*!
+
+train/explained_variance  @��k�+       ��K	�5`�j��A��*
+
+train/learning_rateRI�9 ���"       x=�	�5`�j��A��*
+
+
+train/loss��LY �2       $V�	�5`�j��A��*#
+!
+train/policy_gradient_loss8l'��
+��$       B+�M	�5`�j��A��*
+
+train/reward�p���,(       �pJ	�5`�j��A��*
+
+train/value_loss�8�L���        )�P	�� k��A̨	*
+
+time/fps @ZD/`$�'       ��F	�� k��A̨	*
+
+train/approx_kl��4�PC+       ��K	�� k��A̨	*
+
+train/clip_fraction    #� �(       �pJ	�� k��A̨	*
+
+train/clip_range��L>n/*       ����	�� k��A̨	*
+
+train/entropy_loss�������0       ���_	�� k��A̨	*!
+
+train/explained_variance  @����+       ��K	�� k��A̨	*
+
+train/learning_rateRI�9*4""       x=�	�� k��A̨	*
+
+
+train/lossNxL��(2       $V�	�� k��A̨	*#
+!
+train/policy_gradient_lossE��bMc$       B+�M	�� k��A̨	*
+
+train/reward��Í�o�(       �pJ	�� k��A̨	*
+
+train/value_loss�z�Lܒ|g        )�P	���k��A��
+*
+
+time/fps  ZD�!��'       ��F	���k��A��
+*
+
+train/approx_klN]a4�|g'+       ��K	���k��A��
+*
+
+train/clip_fraction    �!*(       �pJ	���k��A��
+*
+
+train/clip_range��L>��)*       ����	���k��A��
+*
+
+train/entropy_loss��������0       ���_	���k��A��
+*!
+
+train/explained_variance   �i�q+       ��K	���k��A��
+*
+
+train/learning_rateRI�9�6�"       x=�	���k��A��
+*
+
+
+train/loss/�LM��2       $V�	���k��A��
+*#
+!
+train/policy_gradient_loss�Q��1�PV$       B+�M	���k��A��
+*
+
+train/reward"��/(       �pJ	���k��A��
+*
+
+train/value_loss��LBz6)        )�P	��k��A��
+*
+
+time/fps @YD#i�'       ��F	��k��A��
+*
+
+train/approx_kl��51��E+       ��K	��k��A��
+*
+
+train/clip_fraction    ��4�(       �pJ	��k��A��
+*
+
+train/clip_range��L>��ϔ*       ����	��k��A��
+*
+
+train/entropy_loss �����>�0       ���_	��k��A��
+*!
+
+train/explained_variance   �n���+       ��K	��k��A��
+*
+
+train/learning_rateRI�9� s"       x=�	��k��A��
+*
+
+
+train/loss���K��2       $V�	��k��A��
+*#
+!
+train/policy_gradient_lossY���U}$       B+�M	��k��A��
+*
+
+train/rewardwM�F2(       �pJ	��k��A��
+*
+
+train/value_lossvyL4��        )�P	�Uk��A��*
+
+time/fps  YD��mJ'       ��F	�Uk��A��*
+
+train/approx_kl[uZ3�d�+       ��K	�Uk��A��*
+
+train/clip_fraction    ��(       �pJ	�Uk��A��*
+
+train/clip_range��L>� y.*       ����	�Uk��A��*
+
+train/entropy_loss/����i��0       ���_	�Uk��A��*!
+
+train/explained_variance   42W��+       ��K	�Uk��A��*
+
+train/learning_rateRI�9zȮ�"       x=�	�Uk��A��*
+
+
+train/lossY�LEY��2       $V�	�Uk��A��*#
+!
+train/policy_gradient_loss��5r��V$       B+�M	�Uk��A��*
+
+train/reward�j�Ûm��(       �pJ	�Uk��A��*
+
+train/value_lossp�L�
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_17/events.out.tfevents.1724754503.Trading.104472.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_17/events.out.tfevents.1724754503.Trading.104472.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_17/events.out.tfevents.1724754503.Trading.104472.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_17/events.out.tfevents.1724754503.Trading.104472.0	
@@ -0,0 +1,31 @@
+H       ��H�	�gΑj��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer�=֧       QKD	vm�j��A�c*
+
+time/fps  �B�G�#       ��wC	vm�j��A�c*
+
+train/reward �µ;F        )�P	硖�j��A��*
+
+time/fps  �B_��'       ��F	硖�j��A��*
+
+train/approx_kl��8ԭ+       ��K	硖�j��A��*
+
+train/clip_fraction    ����(       �pJ	硖�j��A��*
+
+train/clip_range��L>@��*       ����	硖�j��A��*
+
+train/entropy_loss음�
+0       ���_	硖�j��A��*!
+
+train/explained_variance  ��%;S�+       ��K	硖�j��A��*
+
+train/learning_rateRI�9�^�j"       x=�	硖�j��A��*
+
+
+train/loss�MIG.�2       $V�	硖�j��A��*#
+!
+train/policy_gradient_loss�Kշ��]�$       B+�M	硖�j��A��*
+
+train/reward�W�B->(       �pJ	硖�j��A��*
+
+train/value_loss�o�IBS�
\ No newline at end of file
Index: reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_16/events.out.tfevents.1724752689.Trading.51052.0
===================================================================
diff --git a/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_16/events.out.tfevents.1724752689.Trading.51052.0 b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_16/events.out.tfevents.1724752689.Trading.51052.0
new file mode 100644
--- /dev/null	
+++ b/reinforcement_learning/ppo_toy_algo/ppo_crypto_trading_tensorboard/PPO_16/events.out.tfevents.1724752689.Trading.51052.0	
@@ -0,0 +1,359 @@
+H       ��H�	��g�h��A
+brain.Event:2R.
+,tensorboard.summary.writer.event_file_writer���$       QKD	j1�h��A�c*
+
+time/fps ��D��ٚ#       ��wC	j1�h��A�c*
+
+train/reward�����.n�        )�P	��W�h��A��*
+
+time/fps  �D���-'       ��F	��W�h��A��*
+
+train/approx_kl�۳;80�+       ��K	��W�h��A��*
+
+train/clip_fraction�(�:	jIR(       �pJ	��W�h��A��*
+
+train/clip_range��L>7C�*       ����	��W�h��A��*
+
+train/entropy_loss�9��L�2R0       ���_	��W�h��A��*!
+
+train/explained_variance �E�!�|
++       ��K	��W�h��A��*
+
+train/learning_rateRI�9J��"       x=�	��W�h��A��*
+
+
+train/loss�f(I���62       $V�	��W�h��A��*#
+!
+train/policy_gradient_loss�����3I$       B+�M	��W�h��A��*
+
+train/reward���B	��(       �pJ	��W�h��A��*
+
+train/value_loss:ݱI�<�        )�P	U���h��A��*
+
+time/fps @�Dhx��'       ��F	U���h��A��*
+
+train/approx_kl��;�Nb+       ��K	U���h��A��*
+
+train/clip_fraction��5;yyp^(       �pJ	U���h��A��*
+
+train/clip_range��L>�tۧ*       ����	U���h��A��*
+
+train/entropy_lossF勿9b�"0       ���_	U���h��A��*!
+
+train/explained_variance m�:�+       ��K	U���h��A��*
+
+train/learning_rateRI�9��C�"       x=�	U���h��A��*
+
+
+train/loss��I[gs�2       $V�	U���h��A��*#
+!
+train/policy_gradient_loss�IH��P&�$       B+�M	U���h��A��*
+
+train/reward�-C�s�,(       �pJ	U���h��A��*
+
+train/value_loss��bJ�+�        )�P	�$�h��Ač*
+
+time/fps `�D��Q�'       ��F	�$�h��Ač*
+
+train/approx_kl�� <�q+       ��K	�$�h��Ač*
+
+train/clip_fraction�;�\��(       �pJ	�$�h��Ač*
+
+train/clip_range��L>��>�*       ����	�$�h��Ač*
+
+train/entropy_lossև���cR0       ���_	�$�h��Ač*!
+
+train/explained_variance  ���<�S+       ��K	�$�h��Ač*
+
+train/learning_rateRI�9l�D�"       x=�	�$�h��Ač*
+
+
+train/lossʢ�J_�x2       $V�	�$�h��Ač*#
+!
+train/policy_gradient_loss;�ͺ�i�$       B+�M	�$�h��Ač*
+
+train/rewardHqB��b(       �pJ	�$�h��Ač*
+
+train/value_loss�%K����        )�P	�7h�h��A��*
+
+time/fps `�D��W'       ��F	�7h�h��A��*
+
+train/approx_kl�z;���+       ��K	�7h�h��A��*
+
+train/clip_fraction��9E��(       �pJ	�7h�h��A��*
+
+train/clip_range��L>�5*       ����	�7h�h��A��*
+
+train/entropy_loss�o����,�0       ���_	�7h�h��A��*!
+
+train/explained_variance  ���O��+       ��K	�7h�h��A��*
+
+train/learning_rateRI�9Z�h"       x=�	�7h�h��A��*
+
+
+train/loss�Q
+J1g !2       $V�	�7h�h��A��*#
+!
+train/policy_gradient_loss��5��HW$       B+�M	�7h�h��A��*
+
+train/reward{ezC|�(       �pJ	�7h�h��A��*
+
+train/value_loss��J�8o>        )�P	Dj��h��A��*
+
+time/fps ��D�Y�'       ��F	Dj��h��A��*
+
+train/approx_kl�i;����+       ��K	Dj��h��A��*
+
+train/clip_fraction�;�Fu�(       �pJ	Dj��h��A��*
+
+train/clip_range��L>���*       ����	Dj��h��A��*
+
+train/entropy_loss���X��0       ���_	Dj��h��A��*!
+
+train/explained_variance   4����+       ��K	Dj��h��A��*
+
+train/learning_rateRI�9`G_�"       x=�	Dj��h��A��*
+
+
+train/loss�+�JB!x�2       $V�	Dj��h��A��*#
+!
+train/policy_gradient_lossJ���*N�q$       B+�M	Dj��h��A��*
+
+train/reward��AQZe}(       �pJ	Dj��h��A��*
+
+train/value_losst�:K�sr�        )�P	g��h��A׷*
+
+time/fps ��DVG'       ��F	g��h��A׷*
+
+train/approx_kl�J�:�d+       ��K	mo��h��A׷*
+
+train/clip_fractionU�f9����(       �pJ	mo��h��A׷*
+
+train/clip_range��L>�\�*       ����	mo��h��A׷*
+
+train/entropy_losse��g�0       ���_	mo��h��A׷*!
+
+train/explained_variance   �x��+       ��K	mo��h��A׷*
+
+train/learning_rateRI�9�wPs"       x=�	mo��h��A׷*
+
+
+train/loss���J��2       $V�	mo��h��A׷*#
+!
+train/policy_gradient_loss�F�-L�$       B+�M	mo��h��A׷*
+
+train/reward��â��_(       �pJ	mo��h��A׷*
+
+train/value_lossVb5Kf�M        )�P	�5��h��A��*
+
+time/fps @�DBBt�'       ��F	�5��h��A��*
+
+train/approx_klHO�;��U	+       ��K	�5��h��A��*
+
+train/clip_fraction���8:nIv(       �pJ	�5��h��A��*
+
+train/clip_range��L>��3�*       ����	�5��h��A��*
+
+train/entropy_loss��VJE�0       ���_	�5��h��A��*!
+
+train/explained_variance    aP�N+       ��K	�5��h��A��*
+
+train/learning_rateRI�9Ӳ(�"       x=�	�5��h��A��*
+
+
+train/loss�v�JD���2       $V�	�5��h��A��*#
+!
+train/policy_gradient_loss� ʹ�E'$       B+�M	�5��h��A��*
+
+train/rewardS�����G(       �pJ	�5��h��A��*
+
+train/value_lossr<KK�D�        )�P	��.�h��A��*
+
+time/fps ��D�b�Y'       ��F	��.�h��A��*
+
+train/approx_kl˭�:mg'+       ��K	��.�h��A��*
+
+train/clip_fraction    ����(       �pJ	��.�h��A��*
+
+train/clip_range��L>8Bw�*       ����	��.�h��A��*
+
+train/entropy_lossI�����(g0       ���_	��.�h��A��*!
+
+train/explained_variance    '��+       ��K	��.�h��A��*
+
+train/learning_rateRI�9��j"       x=�	��.�h��A��*
+
+
+train/lossԭ�K�UX�2       $V�	��.�h��A��*#
+!
+train/policy_gradient_loss�	��B
+.$       B+�M	��.�h��A��*
+
+train/rewardF����a�(       �pJ	��.�h��A��*
+
+train/value_loss�p0L+)��        )�P	�-f�h��A��*
+
+time/fps @�D>�o�'       ��F	�-f�h��A��*
+
+train/approx_kl�9��:
++       ��K	�-f�h��A��*
+
+train/clip_fraction    ���(       �pJ	�-f�h��A��*
+
+train/clip_range��L>����*       ����	�-f�h��A��*
+
+train/entropy_loss&b����0       ���_	�-f�h��A��*!
+
+train/explained_variance  �4�=^�+       ��K	�-f�h��A��*
+
+train/learning_rateRI�9
+, i"       x=�	�-f�h��A��*
+
+
+train/loss}(L;�&2       $V�	�-f�h��A��*#
+!
+train/policy_gradient_loss>�C��K��$       B+�M	�-f�h��A��*
+
+train/rewardA �Nt�V(       �pJ	�-f�h��A��*
+
+train/value_loss��L�D^q        )�P	�Ϣ�h��A��*
+
+time/fps @D�No�'       ��F	�Ϣ�h��A��*
+
+train/approx_kl`�h:߯�f+       ��K	�Ϣ�h��A��*
+
+train/clip_fraction    W�P(       �pJ	�Ϣ�h��A��*
+
+train/clip_range��L>o���*       ����	�Ϣ�h��A��*
+
+train/entropy_loss
+������0       ���_	�Ϣ�h��A��*!
+
+train/explained_variance  �3`���+       ��K	�Ϣ�h��A��*
+
+train/learning_rateRI�9J���"       x=�	�Ϣ�h��A��*
+
+
+train/loss,�VL �H�2       $V�	�Ϣ�h��A��*#
+!
+train/policy_gradient_lossۥɹ��j�$       B+�M	�Ϣ�h��A��*
+
+train/reward�PĪ�`@(       �pJ	�Ϣ�h��A��*
+
+train/value_loss�z�LUW�        )�P	����h��A̨	*
+
+time/fps �~DsA�'       ��F	����h��A̨	*
+
+train/approx_kl��:��9�+       ��K	����h��A̨	*
+
+train/clip_fraction    f�-(       �pJ	����h��A̨	*
+
+train/clip_range��L>�1��*       ����	����h��A̨	*
+
+train/entropy_losso����!�z0       ���_	%���h��A̨	*!
+
+train/explained_variance    -���+       ��K	%���h��A̨	*
+
+train/learning_rateRI�9-ٿ�"       x=�	%���h��A̨	*
+
+
+train/loss�QL���2       $V�	%���h��A̨	*#
+!
+train/policy_gradient_loss�Y���ǲA$       B+�M	%���h��A̨	*
+
+train/reward�S�\<��(       �pJ	%���h��A̨	*
+
+train/value_lossHh�L0��d        )�P	�A�h��A��
+*
+
+time/fps  ~D�mK'       ��F	�A�h��A��
+*
+
+train/approx_kl$v�:� z�+       ��K	�A�h��A��
+*
+
+train/clip_fraction��8e��(       �pJ	�A�h��A��
+*
+
+train/clip_range��L>���*       ����	�A�h��A��
+*
+
+train/entropy_loss�֋�/
+	�0       ���_	�A�h��A��
+*!
+
+train/explained_variance    ^1��+       ��K	�A�h��A��
+*
+
+train/learning_rateRI�9�� �"       x=�	�A�h��A��
+*
+
+
+train/loss.L^�4�2       $V�	�A�h��A��
+*#
+!
+train/policy_gradient_loss���ĺLb$       B+�M	�A�h��A��
+*
+
+train/rewardM��>zA(       �pJ	�A�h��A��
+*
+
+train/value_lossiѯL0ݿ�        )�P	�C�h��A��
+*
+
+time/fps �}DЗsr'       ��F	�C�h��A��
+*
+
+train/approx_klQ́:�kx�+       ��K	�C�h��A��
+*
+
+train/clip_fraction    ��8@(       �pJ	�C�h��A��
+*
+
+train/clip_range��L>��Bx*       ����	�C�h��A��
+*
+
+train/entropy_loss�ȋ��"��0       ���_	�C�h��A��
+*!
+
+train/explained_variance    ���+       ��K	�C�h��A��
+*
+
+train/learning_rateRI�9�C�"       x=�	�C�h��A��
+*
+
+
+train/loss��5LE�<2       $V�	�C�h��A��
+*#
+!
+train/policy_gradient_loss��¹��=�$       B+�M	�C�h��A��
+*
+
+train/reward��ĄS�(       �pJ	�C�h��A��
+*
+
+train/value_loss}�LV:�        )�P	��x�h��A��*
+
+time/fps @}D�|{�'       ��F	��x�h��A��*
+
+train/approx_kl��+:���y+       ��K	��x�h��A��*
+
+train/clip_fraction    "��(       �pJ	��x�h��A��*
+
+train/clip_range��L>�W�*       ����	��x�h��A��*
+
+train/entropy_loss��V00       ���_	��x�h��A��*!
+
+train/explained_variance  �3�0�;+       ��K	��x�h��A��*
+
+train/learning_rateRI�9���"       x=�	��x�h��A��*
+
+
+train/lossǒILF�2�2       $V�	��x�h��A��*#
+!
+train/policy_gradient_loss%�Թ�
+��$       B+�M	��x�h��A��*
+
+train/reward������(       �pJ	��x�h��A��*
+
+train/value_loss��L���
\ No newline at end of file
