{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3f8fd9-28ba-4d4e-93cc-fde047a6d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    return (pl.col(col) - pl.col(col).shift(lag)).alias(f\"{col}_diff_{lag}\")\n",
    "\n",
    "def second_order_diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    # äºŒé˜¶å·®åˆ† = ä¸€é˜¶å·®åˆ†çš„å·®åˆ†\n",
    "    first_diff = pl.col(col) - pl.col(col).shift(lag)\n",
    "    second_diff = first_diff - first_diff.shift(lag)\n",
    "    return second_diff.alias(f\"{col}_second_order_diff_{lag}\")\n",
    "\n",
    "def momentum_ratio_expr(col: str, lag: int = 200) -> pl.Expr:\n",
    "    # åŠ¨é‡æ¯”ç‡ = x_t / x_{t-lag}\n",
    "    return (pl.col(col) / (pl.col(col).shift(lag) + 1e-8)).alias(f\"{col}_momentum_ratio_{lag}\")\n",
    "\n",
    "def rolling_volatility_expr(col: str, window: int) -> pl.Expr:\n",
    "    return pl.col(col).rolling_std(window).alias(f\"{col}_volatility_{window}\")\n",
    "\n",
    "def lag_exprs(col: str, lags: List[int]) -> List[pl.Expr]:\n",
    "    return [\n",
    "        pl.col(col).shift(lag).alias(f\"{col}_lag_{lag}\")\n",
    "        for lag in lags\n",
    "    ]\n",
    "\n",
    "\n",
    "def cross_comb_expr(a: str, b: str) -> pl.Expr:\n",
    "    return (pl.col(a) + (pl.col(b) + 1e-8)).alias(f\"{a}_comb_{b}\")\n",
    "\n",
    "def cols_to_transforms(\n",
    "        df: pl.DataFrame,\n",
    "        exclude_cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        cols = df.collect_schema().names()\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    cols = [\n",
    "        col for col in cols\n",
    "        if col not in exclude_cols and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_scaled')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return cols\n",
    "\n",
    "def batch_apply_single_exprs(\n",
    "        window: int,\n",
    "        lag: int,\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    single_exprs = []\n",
    "    # single features transformation\n",
    "    for col in cols:\n",
    "        single_exprs.extend([\n",
    "            diff_expr(col),\n",
    "            second_order_diff_expr(col),\n",
    "            momentum_ratio_expr(col, lag),\n",
    "            rolling_volatility_expr(col, window),\n",
    "        ])\n",
    "        single_exprs.extend(lag_exprs(col, [10, 20, 50, 100, 200]))\n",
    "\n",
    "    return single_exprs\n",
    "\n",
    "def batch_apply_multi_exprs(\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    multi_exprs = []\n",
    "\n",
    "    n = len(cols)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            a, b = cols[i], cols[j]\n",
    "            multi_exprs.extend([\n",
    "                cross_comb_expr(a, b),\n",
    "            ])\n",
    "\n",
    "    return multi_exprs\n",
    "\n",
    "def batch_apply_transforms(\n",
    "        df_to_transforms: pl.DataFrame,\n",
    "        window: int,\n",
    "        lag: int,\n",
    "        exclude_cols: List[str] = None\n",
    ") -> pl.DataFrame:\n",
    "    base_cols = cols_to_transforms(df_to_transforms, exclude_cols)\n",
    "   \n",
    "    single_exprs = batch_apply_single_exprs(window, lag, base_cols)\n",
    "    multi_exprs = batch_apply_multi_exprs(base_cols)\n",
    "\n",
    "    exprs = single_exprs + multi_exprs\n",
    "    return df_to_transforms.with_columns(single_exprs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eaa881d-4d3c-4ca2-abba-579ed9f0c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_df_by_week(\n",
    "        origin_input_df: pl.DataFrame,\n",
    "        ts_col: str = \"timestamp\"\n",
    ") -> List[pl.DataFrame]:\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(ts_col).cast(pl.Datetime).alias(f\"{ts_col}_dt\")\n",
    "    ])\n",
    "\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(f\"{ts_col}_dt\").dt.truncate(\"1w\").alias(\"week_start\")\n",
    "    ])\n",
    "\n",
    "    unique_weeks = origin_input_df.select(\"week_start\").unique().sort(\"week_start\")\n",
    "\n",
    "    weekly_dfs = [\n",
    "        origin_input_df.filter(pl.col(\"week_start\") == wk).drop(\"week_start\")\n",
    "        for wk in unique_weeks[\"week_start\"]\n",
    "    ]\n",
    "\n",
    "    return weekly_dfs\n",
    "\n",
    "\n",
    "def clean_df_drop_nulls(\n",
    "        df_to_clean: pl.DataFrame,\n",
    "        null_threshold: int = 10000,\n",
    "        verbose: bool = True\n",
    ") -> pl.DataFrame:\n",
    "    pd_df = df_to_clean.to_pandas()\n",
    "\n",
    "    null_counts = pd_df.isnull().sum()\n",
    "    cols_to_drop = null_counts[null_counts > null_threshold].index\n",
    "\n",
    "    pd_df_cleaned = pd_df.drop(columns=cols_to_drop)\n",
    "    pd_df_clean = pd_df_cleaned.dropna()\n",
    "    pl_df_clean = pl.from_pandas(pd_df_clean)\n",
    "\n",
    "    if verbose:\n",
    "        max_null_col = null_counts.idxmax()\n",
    "        max_null_count = null_counts.max()\n",
    "        print(\"å„åˆ—ç©ºå€¼æ•°é‡ï¼š\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        print(f\"åˆ é™¤ç©ºå€¼è¶…è¿‡ {null_threshold} çš„åˆ—ï¼š{list(cols_to_drop)}\")\n",
    "        print(f\"åˆ é™¤åˆ—åï¼ŒDataFrameå½¢çŠ¶ï¼š{pd_df_cleaned.shape}\")\n",
    "        print(f\"ç©ºå€¼æœ€å¤šçš„åˆ—æ˜¯ï¼š{max_null_col}ï¼Œå…±æœ‰ {max_null_count} ä¸ªç©ºå€¼\")\n",
    "        print(f\"åˆ é™¤ç©ºå€¼è¡Œåï¼ŒDataFrameå½¢çŠ¶ï¼š{pd_df_clean.shape}\")\n",
    "\n",
    "    return pl_df_clean\n",
    "\n",
    "def avg_steps_to_volatility(prices: np.ndarray, target_ratio: float) -> int:\n",
    "    n = len(prices)\n",
    "    steps_list = []\n",
    "    for i in tqdm(range(n), desc=f\"cal abs change {target_ratio*100:.2f}% avg steps\"):\n",
    "        start_price = prices[i]\n",
    "        steps = -1\n",
    "        for j in range(i + 1, n):\n",
    "            change = abs(prices[j] / start_price - 1)\n",
    "            if change >= target_ratio:\n",
    "                steps = j - i\n",
    "                break\n",
    "        if steps != -1:\n",
    "            steps_list.append(steps)\n",
    "    if len(steps_list) == 0:\n",
    "        return -1\n",
    "    return int(np.mean(steps_list))\n",
    "\n",
    "def future_return_expr(price_col: str, step: int) -> pl.Expr:\n",
    "    return ((pl.col(price_col).shift(-step) - pl.col(price_col)) / pl.col(price_col)).alias(f\"future_return_{step}\")\n",
    "\n",
    "def rolling_minmax_scaled_expr(\n",
    "        col: str,\n",
    "        min_col: str,\n",
    "        max_col: str,\n",
    "        scaled_col: str\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        ((pl.col(col) - pl.col(min_col)) / (pl.col(max_col) - pl.col(min_col) + 1e-9))\n",
    "        .clip(0.0, 1.0)\n",
    "        .fill_null(0.5)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "def rolling_minmax_normalize(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')  # scaled æ˜¯æœ€ç»ˆäº§ç‰©ï¼Œä¿ç•™\n",
    "           and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_rolling_max') or\n",
    "                col.endswith('_rolling_min')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rolling_cols = []\n",
    "    for column in columns_to_normalize:\n",
    "        rolling_cols.extend([\n",
    "            pl.col(column).rolling_max(window, min_samples=1).alias(f\"{column}_rolling_max\"),\n",
    "            pl.col(column).rolling_min(window, min_samples=1).alias(f\"{column}_rolling_min\"),\n",
    "        ])\n",
    "\n",
    "    intermediate_cols = [\n",
    "                            f\"{column}_rolling_max\" for column in columns_to_normalize\n",
    "                        ] + [\n",
    "                            f\"{column}_rolling_min\" for column in columns_to_normalize\n",
    "                        ]\n",
    "\n",
    "    return (\n",
    "        rollin_df\n",
    "        .with_columns(rolling_cols)\n",
    "        .with_columns([\n",
    "            rolling_minmax_scaled_expr(\n",
    "                col=column,\n",
    "                min_col=f\"{column}_rolling_min\",\n",
    "                max_col=f\"{column}_rolling_max\",\n",
    "                scaled_col=f\"{column}_scaled\"\n",
    "            ) for column in columns_to_normalize\n",
    "        ])\n",
    "        .drop(intermediate_cols)\n",
    "    )\n",
    "\n",
    "def rolling_mean_tanh_scaled_expr(\n",
    "        col: str,\n",
    "        scaled_col: str,\n",
    "        window: int\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(col)\n",
    "        .rolling_mean(window, min_samples=1)\n",
    "        .tanh()\n",
    "        .rolling_mean(window, min_samples=1)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "def rolling_mean_tanh_normalize(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')\n",
    "    ]\n",
    "\n",
    "    return rollin_df.with_columns([\n",
    "        rolling_mean_tanh_scaled_expr(\n",
    "            col=column,\n",
    "            scaled_col=f\"{column}_scaled\",\n",
    "            window=window\n",
    "        ) for column in columns_to_normalize\n",
    "    ])\n",
    "\n",
    "def rolling_ic_ir_icto_index(\n",
    "        df: pl.DataFrame,\n",
    "        target_col: str,\n",
    "        exclude_prefixes: list[str],\n",
    "        window_size: int,\n",
    "        step: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col.endswith(\"_scaled\") \n",
    "            and (col.startswith(\"z_\") or col.startswith(\"raw_\")) \n",
    "            and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "            and not col.startswith(\"future_return_\")\n",
    "            and col != \"px\"\n",
    "    ]\n",
    "\n",
    "    # feature_cols = [\n",
    "    #     col for col in df.columns\n",
    "    #     if col.startswith(\"z_\") \n",
    "    #         and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "    #         and not col.startswith(\"future_return_\")\n",
    "    #         and col != \"px\"\n",
    "    # ]\n",
    "\n",
    "    n = df.height\n",
    "    results = []\n",
    "    prev_ranks = {}\n",
    "\n",
    "    for start in tqdm(range(0, n - window_size + 1, step), desc=\"Rolling IC & ICTO\"):\n",
    "        end = start + window_size\n",
    "        df_win = df.slice(start, window_size)\n",
    "\n",
    "        # rank è½¬æ¢\n",
    "        df_ranked = df_win.with_columns([\n",
    "            (pl.col(c).rank(method=\"average\") / window_size).alias(c + \"_rank\") for c in feature_cols + [target_col]\n",
    "        ])\n",
    "        target_rank_col = target_col + \"_rank\"\n",
    "\n",
    "        for feat in feature_cols:\n",
    "            feat_rank_col = feat + \"_rank\"\n",
    "            ic = df_ranked.select(\n",
    "                pl.corr(pl.col(feat_rank_col), pl.col(target_rank_col)).alias(\"ic\")\n",
    "            ).to_series()[0]\n",
    "\n",
    "            turnover = None\n",
    "            if feat in prev_ranks:\n",
    "                cur_ranks = df_ranked[feat_rank_col].to_numpy()\n",
    "                prev = prev_ranks[feat]\n",
    "                if len(prev) == len(cur_ranks):\n",
    "                    turnover = np.mean(np.abs(cur_ranks - prev))\n",
    "\n",
    "            # æ›´æ–° prev_ranks\n",
    "            prev_ranks[feat] = df_ranked[feat_rank_col].to_numpy()\n",
    "\n",
    "            results.append({\n",
    "                \"window_start\": int(start),\n",
    "                \"window_end\": int(end - 1),\n",
    "                \"factor\": str(feat),\n",
    "                \"ic\": float(ic) if not np.isnan(ic) else None,\n",
    "                \"turnover\": float(turnover) if turnover is not None else None\n",
    "            })\n",
    "\n",
    "    df_result = pl.DataFrame(\n",
    "        results,\n",
    "        schema={\n",
    "            \"window_start\": pl.Int64,\n",
    "            \"window_end\": pl.Int64,\n",
    "            \"factor\": pl.Utf8,\n",
    "            \"ic\": pl.Float64,\n",
    "            \"turnover\": pl.Float64,\n",
    "        }\n",
    "    )      \n",
    "    return (\n",
    "        df_result\n",
    "        .group_by(\"factor\")\n",
    "        .agg([\n",
    "            pl.mean(\"ic\").alias(\"mean_ic\"),\n",
    "            pl.std(\"ic\").alias(\"std_ic\"),\n",
    "            pl.mean(\"turnover\").alias(\"mean_turnover\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"mean_ic\") / pl.col(\"std_ic\")).alias(\"ir\"),\n",
    "            (pl.col(\"mean_ic\") / (pl.col(\"mean_turnover\") + 1e-8)).abs().alias(\"icto\")\n",
    "        ])\n",
    "        .sort(\"icto\", descending=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dbd618d-0275-4332-8f80-fc8024a90269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (21_852, 117)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ timestamp        â”† px     â”† sum_buy_sz â”† sum_sell_sz â”† â€¦ â”† z_factor_momentum_t â”† z_factor_order_sent â”† z_factor_oi_momentu â”† z_factor_oi_moment â”‚\n",
      "â”‚ ---              â”† ---    â”† ---        â”† ---         â”†   â”† rend_confirâ€¦        â”† iment_diverâ€¦        â”† m_punch             â”† um_long_termâ€¦      â”‚\n",
      "â”‚ i64              â”† f64    â”† f64        â”† f64         â”†   â”† ---                 â”† ---                 â”† ---                 â”† ---                â”‚\n",
      "â”‚                  â”†        â”†            â”†             â”†   â”† f64                 â”† f64                 â”† f64                 â”† f64                â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 1744206774672000 â”† 106.06 â”† 20184.72   â”† 19598.83    â”† â€¦ â”† -0.198807           â”† -0.400984           â”† -0.438047           â”† -0.234447          â”‚\n",
      "â”‚ 1744206977641000 â”† 106.28 â”† 48103.24   â”† 59927.02    â”† â€¦ â”† -0.151364           â”† -0.394095           â”† -0.451051           â”† -0.177329          â”‚\n",
      "â”‚ 1744207090442000 â”† 106.06 â”† 36291.28   â”† 42417.46    â”† â€¦ â”† -0.192095           â”† -0.400792           â”† -0.496333           â”† -0.236247          â”‚\n",
      "â”‚ 1744207103597000 â”† 106.29 â”† 4552.43    â”† 3261.99     â”† â€¦ â”† -0.200416           â”† -0.401664           â”† -0.493955           â”† -0.241267          â”‚\n",
      "â”‚ 1744207135828000 â”† 106.07 â”† 9665.97    â”† 20809.94    â”† â€¦ â”† -0.246435           â”† -0.394793           â”† -0.535998           â”† -0.284338          â”‚\n",
      "â”‚ â€¦                â”† â€¦      â”† â€¦          â”† â€¦           â”† â€¦ â”† â€¦                   â”† â€¦                   â”† â€¦                   â”† â€¦                  â”‚\n",
      "â”‚ 1749597917722000 â”† 164.79 â”† 9436.54    â”† 1611.2      â”† â€¦ â”† -0.525788           â”† -0.685468           â”† 0.065212            â”† 0.113779           â”‚\n",
      "â”‚ 1749598005021000 â”† 164.46 â”† 10652.48   â”† 12300.88    â”† â€¦ â”† -0.545393           â”† -0.679936           â”† 0.05811             â”† 0.097817           â”‚\n",
      "â”‚ 1749598566727000 â”† 164.79 â”† 71148.25   â”† 53906.04    â”† â€¦ â”† -0.543268           â”† -0.636919           â”† 0.056491            â”† 0.099621           â”‚\n",
      "â”‚ 1749598880587000 â”† 165.12 â”† 54594.2    â”† 51853.36    â”† â€¦ â”† -0.526926           â”† -0.578586           â”† 0.059981            â”† 0.112122           â”‚\n",
      "â”‚ 1749599178024000 â”† 164.78 â”† 38712.76   â”† 45733.65    â”† â€¦ â”† -0.524679           â”† -0.541712           â”† 0.057174            â”† 0.111052           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "origin_df = pl.read_csv(\"C:/quant/data/binance_resampled_data/SOLUSDT_factors_threshold0.002_rolling2000.csv\")\n",
    "print(origin_df)\n",
    "origin_df = batch_apply_transforms(origin_df, 200, 10)\n",
    "origin_df = rolling_mean_tanh_normalize(origin_df, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec8522-be63-4934-9f56-84d8e128baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bf104-b1b5-4338-a9e5-f1a5fff54971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_last_n_rows_with_px(y_test, y_test_binary, y_pred_prob, px, std_array=None, n=700, m=-1, alpha=1):\n",
    "    # æˆªå–æœ€å n è¡Œ\n",
    "    y_test_slice = y_test[n:m]\n",
    "    y_test_binary_slice = y_test_binary[n:m]\n",
    "    y_pred_prob_slice = y_pred_prob[n:m]\n",
    "    px_slice = px[n:m]\n",
    "    \n",
    "    if std_array is not None:\n",
    "        std_slice = std_array[n:m]\n",
    "    else:\n",
    "        std_slice = None\n",
    "\n",
    "    time_index = np.arange(len(y_test_slice))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "    # # åŸå§‹æ”¶ç›Š\n",
    "    # ax1.plot(time_index, y_test_slice, label=\"Original Future Returns\", color='gray', alpha=0.8)\n",
    "    # ax1.set_ylabel(\"Original Future Returns\", color='gray')\n",
    "    # ax1.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "    # GMM æ ‡ç­¾\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(time_index, y_test_binary_slice, label=\"GMM Labels\", color='tab:blue', marker='o', s=20, alpha=0.7)\n",
    "    ax2.set_ylabel(\"GMM Labels (0 or 1)\", color='tab:blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax2.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    # æ¨¡å‹é¢„æµ‹æ¦‚ç‡\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))\n",
    "    ax3.plot(time_index, y_pred_prob_slice, label=\"MODEL Predicted Probability\", color='tab:green', alpha=0.4)\n",
    "    ax3.set_ylabel(\"MODEL Predicted Probability\", color='tab:green')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax3.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    # ä»·æ ¼çº¿ (ç¬¬4è½´)\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines.right.set_position((\"outward\", 120))  # å†å³ç§»ä¸€å±‚\n",
    "    ax4.plot(time_index, px_slice, label=\"Price (px)\", color='tab:blue', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # åŠ ä¸Šä¸‹è½¨çº¿\n",
    "    if std_slice is not None:\n",
    "        ax4.plot(time_index, px_slice + alpha * std_slice, label=\"Price + std\", color='tab:gray', linestyle=':', alpha=0.4)\n",
    "        ax4.plot(time_index, px_slice - alpha * std_slice, label=\"Price - std\", color='tab:gray', linestyle=':', alpha=0.4)\n",
    "\n",
    "    ax4.set_ylabel(\"Price (px)\", color='tab:red')\n",
    "    ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # xè½´\n",
    "    ax1.set_xlabel(\"Time Index\")\n",
    "    plt.title(f\"Compare Returns, Labels, Prediction & Price (Last {n} Rows)\")\n",
    "\n",
    "    # å›¾ä¾‹\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "    lines_4, labels_4 = ax4.get_legend_handles_labels()\n",
    "\n",
    "    ax1.legend(\n",
    "        lines_1 + lines_2 + lines_3 + lines_4,\n",
    "        labels_1 + labels_2 + labels_3 + labels_4,\n",
    "        loc='upper left'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcbd29-3cd0-44ae-8606-4b7b41d095ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_with_confidence(\n",
    "    y_true,\n",
    "    y_pred_proba,                      \n",
    "    model_name=\"Model\",                    \n",
    "    lower_thresh=0.2,                  \n",
    "    upper_thresh=0.8,       \n",
    "    print_report=True,\n",
    "):\n",
    "\n",
    "    # ===== åŸºç¡€è¯„ä¼° (å…¨æ ·æœ¬) =====\n",
    "    base_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    base_acc = accuracy_score(y_true, base_pred)\n",
    "    \n",
    "    # ===== ç½®ä¿¡é¢„æµ‹è¯„ä¼° =====\n",
    "    # ç”Ÿæˆäº¤æ˜“ä¿¡å· (-1: ä¸äº¤æ˜“, 0: åšç©º, 1: åšå¤š)\n",
    "    signals = np.full_like(y_pred_proba, fill_value=-1, dtype=int)\n",
    "    signals[y_pred_proba <= lower_thresh] = 0\n",
    "    signals[y_pred_proba >= upper_thresh] = 1\n",
    "    \n",
    "    # è®¡ç®—ç½®ä¿¡é¢„æµ‹æŒ‡æ ‡\n",
    "    mask = signals != -1\n",
    "    conf_acc = accuracy_score(y_true[mask], signals[mask]) if any(mask) else np.nan\n",
    "    trading_rate = mask.mean()\n",
    "    \n",
    "    # ===== ç»“æœç»„ç»‡ =====\n",
    "    eval_dict = {\n",
    "        'model': model_name,\n",
    "        'thresholds': f\"{lower_thresh}-{upper_thresh}\",\n",
    "        'base_accuracy': base_acc,\n",
    "        'confident_accuracy': conf_acc,\n",
    "        'trading_rate': trading_rate,\n",
    "        'conf_matrix': confusion_matrix(y_true[mask], signals[mask]) if any(mask) else None,\n",
    "        'class_report': classification_report(y_true[mask], signals[mask], output_dict=True) if any(mask) else None,\n",
    "        'y_test_binary': y_true, \n",
    "        'y_pred_proba': y_pred_proba, \n",
    "        'signal': signals,\n",
    "    }\n",
    "    \n",
    "    # ===== æ‰“å°è¾“å‡º =====\n",
    "    if print_report:\n",
    "        print(f\"\\n=== {model_name} è¯„ä¼°ç»“æœ ===\")\n",
    "        print(f\"ğŸ“Š å…¨æ ·æœ¬å‡†ç¡®ç‡: {base_acc:.4f}\")\n",
    "        print(f\"âœ… è‡ªä¿¡é¢„æµ‹å‡†ç¡®ç‡: {conf_acc:.4f} (é˜ˆå€¼ {lower_thresh}-{upper_thresh})\")\n",
    "        print(f\"ğŸ“ˆ å‡ºæ‰‹ç‡: {trading_rate:.2%}\")\n",
    "        \n",
    "        if any(mask):\n",
    "            print(\"\\nğŸ§® è‡ªä¿¡é¢„æµ‹æ··æ·†çŸ©é˜µ:\")\n",
    "            print(eval_dict['conf_matrix'])\n",
    "            \n",
    "            print(\"\\nğŸ“ åˆ†ç±»æŠ¥å‘Š:\")\n",
    "            print(classification_report(y_true[mask], signals[mask]))\n",
    "        else:\n",
    "            print(\"âš ï¸ æ— è‡ªä¿¡é¢„æµ‹æ ·æœ¬!\")\n",
    "    \n",
    "    return eval_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1655fec-6a5e-47c3-979e-073d316e40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "rolling_window = 300\n",
    "alpha = 4\n",
    "\n",
    "a_df = (\n",
    "    origin_df\n",
    "    .with_columns([\n",
    "        pl.col(\"px\").rolling_std(window_size=rolling_window).alias(\"rolling_std\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "def label_by_future_std(px: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    n = len(px)\n",
    "    labels = np.full(n, -1)  # åˆå§‹åŒ–ä¸º -1ï¼ˆæœªè§¦å‘ï¼‰\n",
    "\n",
    "    for i in tqdm(range(n), desc=\"Labeling by future std\"):\n",
    "        anchor_price = px[i]\n",
    "        for j in range(i + 1, n):\n",
    "            upper_bound = anchor_price + alpha * std[j]\n",
    "            lower_bound = anchor_price - alpha * std[j]\n",
    "            if px[j] >= upper_bound:\n",
    "                labels[i] = 1\n",
    "                break\n",
    "            elif px[j] <= lower_bound:\n",
    "                labels[i] = 0\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                labels[i] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "px_np = a_df[\"px\"].to_numpy()\n",
    "std_np = a_df[\"rolling_std\"].to_numpy()\n",
    "\n",
    "labels_np = label_by_future_std(px_np, std_np)\n",
    "\n",
    "a_df = a_df.with_columns(pl.Series(\"future_std_label\", labels_np))\n",
    "print(a_df)\n",
    "a_df_filtered = a_df.filter(pl.col(\"future_std_label\") >= 0)\n",
    "print(a_df_filtered)\n",
    "a_df_filtered = clean_df_drop_nulls(a_df_filtered)\n",
    "\n",
    "weekly_dataframes = split_df_by_week(a_df_filtered)\n",
    "print(\"num weekly dfs: \", len(weekly_dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6e1e6-0562-47c6-bad9-74426dfe8b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_train_weeks = 3# å¯é…ç½®\n",
    "n_val_weeks = 1    # ä¸€èˆ¬ 1 å‘¨éªŒè¯\n",
    "n_test_weeks = 1   # å 1 å‘¨åš test\n",
    "\n",
    "exclude_prefixes = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "target_col = \"future_std_label\"\n",
    "feature_cols = [\n",
    "    col for col in origin_df.columns\n",
    "    if col.endswith(\"_scaled\") \n",
    "        and (col.startswith(\"z_\")) \n",
    "        and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "        and not col.startswith(\"future_return_\")\n",
    "        and col != \"px\"\n",
    "]\n",
    "\n",
    "\n",
    "# feature_cols = [\"raw_factor_long_term_oi_trend\"]\n",
    "results = []\n",
    "all_tab_inc_test_predictions = []\n",
    "all_tabnet_test_predictions = []\n",
    "all_lgb_test_predictions = []\n",
    "\n",
    "lgb_model = None\n",
    "tab_inc = None\n",
    "tab_inc_flag = 0\n",
    "\n",
    "for i in range(len(weekly_dataframes) - n_train_weeks - n_val_weeks - n_test_weeks + 1):\n",
    "    train_dfs = weekly_dataframes[i : i + n_train_weeks]\n",
    "    val_dfs = weekly_dataframes[i + n_train_weeks : i + n_train_weeks + n_val_weeks]\n",
    "    test_dfs = weekly_dataframes[i + n_train_weeks + n_val_weeks : i + n_train_weeks + n_val_weeks + n_test_weeks]\n",
    "    \n",
    "    train_df = pl.concat(train_dfs)\n",
    "    val_df = pl.concat(val_dfs)\n",
    "    test_df = pl.concat(test_dfs)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Fold {i}: Train {i}~{i+n_train_weeks-1}, Val {i+n_train_weeks}, Test {i+n_train_weeks+1}\")\n",
    "    print(\"Train:\", train_df['timestamp_dt'][0], \"to\", train_df['timestamp_dt'][-1])\n",
    "    print(\"Val:\", val_df['timestamp_dt'][0], \"to\", val_df['timestamp_dt'][-1])\n",
    "    print(\"Test:\", test_df['timestamp_dt'][0], \"to\", test_df['timestamp_dt'][-1])\n",
    "    \n",
    "    # å¤„ç† train\n",
    "    train_df_processed = train_df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px']).to_pandas()\n",
    "    X_train = train_df_processed[feature_cols]\n",
    "    y_train = train_df_processed[target_col]\n",
    "    px_train = train_df_processed['px']\n",
    "    std_train =train_df_processed['rolling_std']\n",
    "    \n",
    "    # å¤„ç† val\n",
    "    val_df_processed = val_df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px']).to_pandas()\n",
    "    X_val = val_df_processed[feature_cols]\n",
    "    y_val = val_df_processed[target_col]\n",
    "    px_val = val_df_processed['px']\n",
    "    std_val =val_df_processed['rolling_std']\n",
    "    timestamps_val = val_df_processed['timestamp'] # æ”¶é›†æ—¶é—´æˆ³\n",
    "\n",
    "    # å¤„ç† test\n",
    "    test_df_processed = test_df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px']).to_pandas()\n",
    "    X_test = test_df_processed[feature_cols]\n",
    "    y_test = test_df_processed[target_col]\n",
    "    px_test = test_df_processed['px']\n",
    "    std_test =test_df_processed['rolling_std']\n",
    "    timestamps_test = test_df_processed['timestamp'] # æ”¶é›†æ—¶é—´æˆ³\n",
    "\n",
    "    # modellllllll\n",
    "    up_thresh = 0.2\n",
    "    lower_thresh = 0.8\n",
    "    \n",
    "    # # #LGBM\n",
    "    if lgb_model == None:\n",
    "        lgb_model = lgb.LGBMClassifier(n_estimators=5000, learning_rate=0.005, max_depth=7, verbose=-1)\n",
    "        early_stopping_callback = lgb.early_stopping(\n",
    "            stopping_rounds=500,  # è€å¿ƒå€¼ï¼šå¦‚æœéªŒè¯é›†æ€§èƒ½åœ¨è¿ç»­100è½®å†…æ²¡æœ‰æå‡ï¼Œå°±åœæ­¢è®­ç»ƒ\n",
    "            verbose=True,          # æ‰“å°æ—©åœä¿¡æ¯ï¼Œä¾‹å¦‚åœ¨ç¬¬å¤šå°‘è½®åœæ­¢ï¼Œæœ€ä½³åˆ†æ•°æ˜¯å¤šå°‘\n",
    "        )\n",
    "        lgb_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='acc',\n",
    "            # callbacks=[early_stopping_callback], # å°†æ—©åœå›è°ƒä¼ å…¥ callbacks å‚æ•°\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        lgb_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='acc',\n",
    "            # callbacks=[early_stopping_callback], # å°†æ—©åœå›è°ƒä¼ å…¥ callbacks å‚æ•°\n",
    "        )\n",
    "    \n",
    "    lgb_eval = evaluate_with_confidence(\n",
    "        y_true=y_val,\n",
    "        y_pred_proba=lgb_model.predict_proba(X_val)[:, 1],\n",
    "        model_name=\"lgb_model\",\n",
    "        lower_thresh=up_thresh,\n",
    "        upper_thresh=lower_thresh,\n",
    "        print_report=True,\n",
    "    )\n",
    "    # plot_last_n_rows_with_px(y_val, y_val, lgb_model.predict_proba(X_val)[:, 1], px_val, std_array=std_val, n=1, m=-1, alpha=alpha)\n",
    "    \n",
    "    lgb_eval = evaluate_with_confidence(\n",
    "        y_true=y_test,\n",
    "        y_pred_proba=lgb_model.predict_proba(X_test)[:, 1],\n",
    "        model_name=\"lgb_model\",\n",
    "        lower_thresh=up_thresh,\n",
    "        upper_thresh=lower_thresh,\n",
    "        print_report=True,\n",
    "    )\n",
    "    plot_last_n_rows_with_px(lgb_model.predict_proba(X_test)[:, 1], y_test, lgb_model.predict_proba(X_test)[:, 1], px_test, std_array=std_test, n=1, m=-1, alpha=alpha)\n",
    "    lgb_test_pred_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    current_window_results = pd.DataFrame({\n",
    "        'timestamp': timestamps_test,\n",
    "        'symbol': \"BTCUSDT\", # å¦‚æœæœ‰å¤šä¸ªè‚¡ç¥¨ï¼Œæ”¶é›† symbol æ˜¯å¿…è¦çš„\n",
    "        'true_label': y_test,\n",
    "        'predicted_prob': lgb_test_pred_probs,\n",
    "        'px': px_test, # æ”¶é›†ä»·æ ¼ï¼Œå›æµ‹æ—¶éœ€è¦\n",
    "        'rolling_std': std_test # æ”¶é›†æ³¢åŠ¨ç‡ï¼Œå¯èƒ½ç”¨äºç­–ç•¥æˆ–åˆ†æ\n",
    "        # æ·»åŠ ä»»ä½•ä½ å›æµ‹éœ€è¦çš„å…¶ä»–æ•°æ®\n",
    "    })\n",
    "    all_lgb_test_predictions.append(current_window_results)\n",
    "\n",
    "    # # CAT\n",
    "    # cat_model = CatBoostClassifier(iterations=2000, learning_rate=0.005, depth=9, verbose=0)\n",
    "    # cat_model.fit(\n",
    "    #     X_train, y_train_bin,\n",
    "    # )\n",
    "\n",
    "    # cat_eval = evaluate_with_confidence(\n",
    "    #     y_true=y_val_bin,\n",
    "    #     y_pred_proba=cat_model.predict_proba(X_val)[:, 1],\n",
    "    #     model_name=\"cat_model\",\n",
    "    #     lower_thresh=up_thresh,\n",
    "    #     upper_thresh=lower_thresh,\n",
    "    #     print_report=True,\n",
    "    # )\n",
    "    # plot_last_n_rows_with_px(y_val, y_val_bin, cat_model.predict_proba(X_val)[:, 1], px_val, n=6199)\n",
    "    \n",
    "    # cat_eval = evaluate_with_confidence(\n",
    "    #     y_true=y_test_bin,\n",
    "    #     y_pred_proba=cat_model.predict_proba(X_test)[:, 1],\n",
    "    #     model_name=\"cat_model\",\n",
    "    #     lower_thresh=up_thresh,\n",
    "    #     upper_thresh=lower_thresh,\n",
    "    #     print_report=True,\n",
    "    # )\n",
    "    # plot_last_n_rows_with_px(y_test, y_test_bin, cat_model.predict_proba(X_test)[:, 1], px_test, n=6199)\n",
    "\n",
    "    # # XGB\n",
    "    # xgb_model = xgb.XGBClassifier(n_estimators=2000, learning_rate=0.005, max_depth=9, verbosity=0, use_label_encoder=False)\n",
    "    # xgb_model.fit(\n",
    "    #     X_train, y_train,\n",
    "    # )\n",
    "\n",
    "    # xgb_eval = evaluate_with_confidence(\n",
    "    #     y_true=y_val,\n",
    "    #     y_pred_proba=xgb_model.predict_proba(X_val)[:, 1],\n",
    "    #     model_name=\"xgb_model\",\n",
    "    #     lower_thresh=up_thresh,\n",
    "    #     upper_thresh=lower_thresh,\n",
    "    #     print_report=True,\n",
    "    # )\n",
    "    # plot_last_n_rows_with_px(y_val, y_val, xgb_model.predict_proba(X_val)[:, 1], px_val, std_array=std_val, n=1, m=-1, alpha=alpha)\n",
    "    \n",
    "\n",
    "    # xgb_eval = evaluate_with_confidence(\n",
    "    #     y_true=y_test,\n",
    "    #     y_pred_proba=xgb_model.predict_proba(X_test)[:, 1],\n",
    "    #     model_name=\"xgb_model\",\n",
    "    #     lower_thresh=up_thresh,\n",
    "    #     upper_thresh=lower_thresh,\n",
    "    #     print_report=True,\n",
    "    # )\n",
    "    # plot_last_n_rows_with_px(y_test, y_test, xgb_model.predict_proba(X_test)[:, 1], px_test, std_array=std_test, n=1, m=-1, alpha=alpha)\n",
    "\n",
    "    \n",
    "    # Label Encode yï¼ˆå¦‚æœæ˜¯0/1å°±ä¸ç”¨ï¼‰\n",
    "    y_train_enc = y_train.astype(int)\n",
    "    y_val_enc = y_val.astype(int)\n",
    "    \n",
    "    # TabNet è®­ç»ƒ\n",
    "    tabnet = TabNetClassifier(\n",
    "        # n_d=32,\n",
    "        # n_a=32,\n",
    "        # n_steps=7,\n",
    "        device_name='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=5,\n",
    "    )\n",
    "    tabnet.fit(\n",
    "        X_train=X_train.values, y_train=y_train_enc,\n",
    "        eval_set=[(X_val.values, y_val_enc)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=200,\n",
    "        patience=20,\n",
    "        # batch_size=256,\n",
    "        # virtual_batch_size=128,\n",
    "    )\n",
    "\n",
    "    tabnet_eval = evaluate_with_confidence(\n",
    "        y_true=y_val,\n",
    "        y_pred_proba=tabnet.predict_proba(X_val.values)[:, 1],\n",
    "        model_name=\"tabnet\",\n",
    "        lower_thresh=up_thresh,\n",
    "        upper_thresh=lower_thresh,\n",
    "        print_report=True,\n",
    "    )\n",
    "    # plot_last_n_rows_with_px(y_val, y_val, tabnet.predict_proba(X_val.values)[:, 1], px_val, std_array=std_val, n=1, m=-1, alpha=alpha)\n",
    "\n",
    "\n",
    "    tabnet_eval = evaluate_with_confidence(\n",
    "        y_true=y_test,\n",
    "        y_pred_proba=tabnet.predict_proba(X_test.values)[:, 1],\n",
    "        model_name=\"tabnet\",\n",
    "        lower_thresh=up_thresh,\n",
    "        upper_thresh=lower_thresh,\n",
    "        print_report=True,\n",
    "    )\n",
    "    plot_last_n_rows_with_px(tabnet.predict_proba(X_test.values)[:, 1], y_test, tabnet.predict_proba(X_test.values)[:, 1], px_test, std_array=std_test, n=1, m=-1, alpha=alpha)\n",
    "\n",
    "    tabnet_test_pred_probs = tabnet.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "    current_window_results = pd.DataFrame({\n",
    "        'timestamp': timestamps_test,\n",
    "        'symbol': \"BTCUSDT\", # å¦‚æœæœ‰å¤šä¸ªè‚¡ç¥¨ï¼Œæ”¶é›† symbol æ˜¯å¿…è¦çš„\n",
    "        'true_label': y_test,\n",
    "        'predicted_prob': tabnet_test_pred_probs,\n",
    "        'px': px_test, # æ”¶é›†ä»·æ ¼ï¼Œå›æµ‹æ—¶éœ€è¦\n",
    "        'rolling_std': std_test # æ”¶é›†æ³¢åŠ¨ç‡ï¼Œå¯èƒ½ç”¨äºç­–ç•¥æˆ–åˆ†æ\n",
    "        # æ·»åŠ ä»»ä½•ä½ å›æµ‹éœ€è¦çš„å…¶ä»–æ•°æ®\n",
    "    })\n",
    "    all_tabnet_test_predictions.append(current_window_results)\n",
    "\n",
    "    # TabNet_incremental è®­ç»ƒ\n",
    "    if tab_inc_flag == 0:\n",
    "        tab_inc_flag = 1\n",
    "        tab_inc = TabNetClassifier(\n",
    "            # n_d=32,\n",
    "            # n_a=32,\n",
    "            # n_steps=7,\n",
    "            device_name='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            verbose=5,\n",
    "        )\n",
    "        tab_inc.fit(\n",
    "            X_train=X_train.values, y_train=y_train_enc,\n",
    "            eval_set=[(X_val.values, y_val_enc)],\n",
    "            eval_metric=['accuracy'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            # batch_size=256,\n",
    "            # virtual_batch_size=128,\n",
    "        )\n",
    "    \n",
    "        tabnet_eval = evaluate_with_confidence(\n",
    "            y_true=y_val,\n",
    "            y_pred_proba=tab_inc.predict_proba(X_val.values)[:, 1],\n",
    "            model_name=\"tab_inc\",\n",
    "            lower_thresh=up_thresh,\n",
    "            upper_thresh=lower_thresh,\n",
    "            print_report=True,\n",
    "        )\n",
    "        # plot_last_n_rows_with_px(y_val, y_val, tab_inc.predict_proba(X_val.values)[:, 1], px_val, std_array=std_val, n=1, m=-1, alpha=alpha)\n",
    "    \n",
    "    \n",
    "        tabnet_eval = evaluate_with_confidence(\n",
    "            y_true=y_test,\n",
    "            y_pred_proba=tab_inc.predict_proba(X_test.values)[:, 1],\n",
    "            model_name=\"tab_inc\",\n",
    "            lower_thresh=up_thresh,\n",
    "            upper_thresh=lower_thresh,\n",
    "            print_report=True,\n",
    "        )\n",
    "        plot_last_n_rows_with_px(tab_inc.predict_proba(X_test.values)[:, 1], y_test, tab_inc.predict_proba(X_test.values)[:, 1], px_test, std_array=std_test, n=1, m=-1, alpha=alpha)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        tab_inc.fit(\n",
    "            X_train=X_train.values, y_train=y_train_enc,\n",
    "            eval_set=[(X_val.values, y_val_enc)],\n",
    "            eval_metric=['accuracy'],\n",
    "            max_epochs=50,\n",
    "            patience=10,\n",
    "            # batch_size=256,\n",
    "            # virtual_batch_size=128,\n",
    "            warm_start=True,\n",
    "\n",
    "        )\n",
    "        \n",
    "        tabnet_eval = evaluate_with_confidence(\n",
    "            y_true=y_val,\n",
    "            y_pred_proba=tab_inc.predict_proba(X_val.values)[:, 1],\n",
    "            model_name=\"tab_inc\",\n",
    "            lower_thresh=up_thresh,\n",
    "            upper_thresh=lower_thresh,\n",
    "            print_report=True,\n",
    "        )\n",
    "        # plot_last_n_rows_with_px(y_val, y_val, tab_inc.predict_proba(X_val.values)[:, 1], px_val, std_array=std_val, n=1, m=-1, alpha=alpha)\n",
    "    \n",
    "    \n",
    "        tabnet_eval = evaluate_with_confidence(\n",
    "            y_true=y_test,\n",
    "            y_pred_proba=tab_inc.predict_proba(X_test.values)[:, 1],\n",
    "            model_name=\"tab_inc\",\n",
    "            lower_thresh=up_thresh,\n",
    "            upper_thresh=lower_thresh,\n",
    "            print_report=True,\n",
    "        )\n",
    "        plot_last_n_rows_with_px(tab_inc.predict_proba(X_test.values)[:, 1], y_test, tab_inc.predict_proba(X_test.values)[:, 1], px_test, std_array=std_test, n=1, m=-1, alpha=alpha)\n",
    "\n",
    "    tab_inc_test_pred_probs = tab_inc.predict_proba(X_test.values)[:, 1]\n",
    "    current_window_results = pd.DataFrame({\n",
    "        'timestamp': timestamps_test,\n",
    "        'symbol': \"BTCUSDT\", # å¦‚æœæœ‰å¤šä¸ªè‚¡ç¥¨ï¼Œæ”¶é›† symbol æ˜¯å¿…è¦çš„\n",
    "        'true_label': y_test,\n",
    "        'predicted_prob': tab_inc_test_pred_probs,\n",
    "        'px': px_test, # æ”¶é›†ä»·æ ¼ï¼Œå›æµ‹æ—¶éœ€è¦\n",
    "        'rolling_std': std_test # æ”¶é›†æ³¢åŠ¨ç‡ï¼Œå¯èƒ½ç”¨äºç­–ç•¥æˆ–åˆ†æ\n",
    "        # æ·»åŠ ä»»ä½•ä½ å›æµ‹éœ€è¦çš„å…¶ä»–æ•°æ®\n",
    "    })\n",
    "    all_tab_inc_test_predictions.append(current_window_results)\n",
    "\n",
    "    week_results = {\n",
    "        'train_period': f\"{train_df['timestamp_dt'][0]} to {train_df['timestamp_dt'][-1]}\",\n",
    "        'test_period': f\"{test_df['timestamp_dt'][0]} to {test_df['timestamp_dt'][-1]}\",\n",
    "        'LGBM': lgb_eval,\n",
    "        # 'CatBoost': cat_model,\n",
    "        # 'XGBoost': xgb_eval,\n",
    "        'TabNet': tabnet_eval,\n",
    "    }\n",
    "    \n",
    "    results.append(week_results)\n",
    "    print(f\"Test Week {i+n_train_weeks} Evaluation Completed\")\n",
    "\n",
    "final_predictions_df = pd.concat(all_tab_inc_test_predictions).sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Final Predictions DataFrame for Backtesting ---\")\n",
    "print(final_predictions_df.head())\n",
    "print(f\"Total rows collected: {len(final_predictions_df)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb45578-60ec-4d08-8770-a0a4f21b23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 10\n",
    "\n",
    "final_predictions_df = pd.concat(all_tab_inc_test_predictions).sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "final_predictions_df['predicted_prob_rolling_mean'] = final_predictions_df['predicted_prob'].rolling(window=rolling_window_size, min_periods=1).mean()\n",
    "\n",
    "tabnet_eval = evaluate_with_confidence(\n",
    "    y_true=final_predictions_df['true_label'],\n",
    "    y_pred_proba=final_predictions_df['predicted_prob_rolling_mean'],\n",
    "    model_name=\"model\",\n",
    "    lower_thresh=up_thresh,\n",
    "    upper_thresh=lower_thresh,\n",
    "    print_report=True,\n",
    ")\n",
    "\n",
    "plot_last_n_rows_with_px(\n",
    "    final_predictions_df['predicted_prob_rolling_mean'], \n",
    "    final_predictions_df['true_label'],      \n",
    "    final_predictions_df['predicted_prob_rolling_mean'], \n",
    "    final_predictions_df['px'],\n",
    "    std_array=final_predictions_df['rolling_std'],\n",
    "    n=1,\n",
    "    m=-1,\n",
    "    alpha=alpha,\n",
    ")\n",
    "plot_last_n_rows_with_px(\n",
    "    final_predictions_df['predicted_prob'], \n",
    "    final_predictions_df['true_label'], \n",
    "    final_predictions_df['predicted_prob'], \n",
    "    final_predictions_df['px'], \n",
    "    std_array=final_predictions_df['rolling_std'], \n",
    "    n=1,\n",
    "    m=-1, \n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "final_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c21df0-e20a-47b9-9133-152418b43505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# åŸå§‹ predict_proba åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(final_predictions_df['predicted_prob'], bins=50, alpha=0.7, color='skyblue')\n",
    "plt.title('Predicted Probability Distribution')\n",
    "plt.xlabel('Predicted Prob')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# æ»šåŠ¨å‡å€¼åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(final_predictions_df['predicted_prob_rolling_mean'], bins=50, alpha=0.7, color='lightcoral')\n",
    "plt.title('Rolling Mean of Predicted Probabilities')\n",
    "plt.xlabel('Rolling Mean Prob')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef2ded-4a7d-45dd-998b-5e05925daf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "all_predictions_df = final_predictions_df\n",
    "all_predictions_df['timestamp'] = pd.to_datetime(all_predictions_df['timestamp'], unit='us') # Assuming microseconds\n",
    "\n",
    "symbol_to_backtest = all_predictions_df['symbol'].iloc[0] # Take the first symbol for now\n",
    "df = all_predictions_df[all_predictions_df['symbol'] == symbol_to_backtest].copy()\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# ===============================================\n",
    "# Backtesting Parameters (Ratios for costs)\n",
    "# ===============================================\n",
    "initial_capital = 100000  # Initial capital\n",
    "commission_ratio = 0.0005 # Commission ratio (e.0.05%)\n",
    "slippage_ratio = 0.001   # Slippage ratio (e.g., 0.02%)\n",
    "\n",
    "trade_size_ratio = 0.9    # Percentage of current equity to allocate per trade\n",
    "\n",
    "# Strategy Thresholds (needs optimization)\n",
    "long_threshold = 0.95# Predicted probability above this to go long (buy)\n",
    "short_threshold = 0.05\n",
    "\n",
    "beta = 3\n",
    "# Dynamic Take Profit / Stop Loss (in multiples of rolling_std/px)\n",
    "long_stop_loss_multiplier = beta\n",
    "long_take_profit_multiplier = beta\n",
    "short_stop_loss_multiplier = beta\n",
    "short_take_profit_multiplier = beta\n",
    "\n",
    "# ===============================================\n",
    "# Backtesting Main Logic (No change here from previous version)\n",
    "# ===============================================\n",
    "\n",
    "# Initialize account state\n",
    "capital = initial_capital\n",
    "position = 0          # Position size (positive for long, negative for short, 0 for flat)\n",
    "entry_price = 0       # Entry price for current position\n",
    "realized_pnl = 0      # Realized PnL from closed trades\n",
    "equity_curve = [initial_capital] # Equity curve\n",
    "\n",
    "# Record trades\n",
    "trades = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    current_time = df['timestamp'].iloc[i]\n",
    "    current_px = df['px'].iloc[i]\n",
    "    predicted_prob = df['predicted_prob_rolling_mean'].iloc[i]\n",
    "    current_rolling_std = df['rolling_std'].iloc[i]\n",
    "\n",
    "    # Calculate current total equity (capital + market value of position)\n",
    "    if position > 0: # Long position\n",
    "        unrealized_pnl = (current_px - entry_price) * position\n",
    "    elif position < 0: # Short position\n",
    "        unrealized_pnl = (entry_price - current_px) * abs(position) # Profit when price falls\n",
    "    else: # Flat\n",
    "        unrealized_pnl = 0\n",
    "\n",
    "    current_equity = capital\n",
    "    equity_curve.append(current_equity)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Strategy Execution - Long-Short\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Scenario 1: Currently FLAT (position == 0)\n",
    "    if position == 0:\n",
    "        if predicted_prob >= long_threshold: # Go Long Signal\n",
    "            trade_type = 'BUY_OPEN'\n",
    "            trade_price = current_px * (1 + slippage_ratio)\n",
    "            num_shares = (current_equity * trade_size_ratio) / (trade_price * (1 + commission_ratio))\n",
    "\n",
    "            if num_shares > 0:\n",
    "                position = num_shares\n",
    "                entry_price = trade_price\n",
    "                # capital -= (position * entry_price * (1 + commission_ratio))\n",
    "\n",
    "                trades.append({\n",
    "                    'timestamp': current_time, 'type': trade_type, 'price': entry_price, 'shares': position,\n",
    "                    'capital_after_trade': capital, 'equity_after_trade': current_equity, 'predicted_prob': predicted_prob\n",
    "                })\n",
    "                # print(f\"{current_time}: {trade_type} {position:.2f} @ {entry_price:.2f} (Prob: {predicted_prob:.4f}) | Capital: {capital:.2f}\")\n",
    "\n",
    "        elif predicted_prob <= short_threshold: # Go Short Signal\n",
    "            trade_type = 'SELL_SHORT_OPEN'\n",
    "            trade_price = current_px * (1 - slippage_ratio) # Price for short is lower (sell at market)\n",
    "            num_shares = (current_equity * trade_size_ratio) / (trade_price * (1 + commission_ratio))\n",
    "\n",
    "            if num_shares > 0:\n",
    "                position = -num_shares # Negative for short position\n",
    "                entry_price = trade_price\n",
    "                # capital -= (abs(position) * entry_price * (1 + commission_ratio))\n",
    "\n",
    "                trades.append({\n",
    "                    'timestamp': current_time, 'type': trade_type, 'price': entry_price, 'shares': position,\n",
    "                    'capital_after_trade': capital, 'equity_after_trade': current_equity, 'predicted_prob': predicted_prob\n",
    "                })\n",
    "                # print(f\"{current_time}: {trade_type} {position:.2f} @ {entry_price:.2f} (Prob: {predicted_prob:.4f}) | Capital: {capital:.2f}\")\n",
    "\n",
    "    # Scenario 2: Currently LONG (position > 0)\n",
    "    elif position > 0:\n",
    "        sl_price = entry_price - long_stop_loss_multiplier * current_rolling_std\n",
    "        tp_price = entry_price + long_take_profit_multiplier * current_rolling_std\n",
    "\n",
    "        should_close = False\n",
    "        reason = \"\"\n",
    "\n",
    "        # if predicted_prob < short_threshold: # Changed from sell_threshold to short_threshold for consistent logic\n",
    "        #     should_close = True\n",
    "        #     reason = \"Prediction below short threshold (close long)\"\n",
    "        if current_px <= sl_price:\n",
    "            should_close = True\n",
    "            reason = \"Long Stop Loss Hit\"\n",
    "        elif current_px >= tp_price:\n",
    "            should_close = True\n",
    "            reason = \"Long Take Profit Hit\"\n",
    "\n",
    "        if should_close:\n",
    "            trade_type = 'SELL_CLOSE_LONG'\n",
    "            close_price = current_px * (1 - slippage_ratio)\n",
    "            gross_pnl_on_trade = (close_price - entry_price) * position\n",
    "            net_pnl_on_trade = gross_pnl_on_trade - (close_price * position * commission_ratio)\n",
    "\n",
    "            realized_pnl += net_pnl_on_trade\n",
    "            capital += net_pnl_on_trade\n",
    "\n",
    "            trades.append({\n",
    "                'timestamp': current_time, 'type': trade_type, 'price': close_price, 'shares': position,\n",
    "                'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': current_equity,\n",
    "                'reason': reason, 'predicted_prob': predicted_prob\n",
    "            })\n",
    "            # print(f\"{current_time}: {trade_type} {position:.2f} @ {close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f} | Reason: {reason}\")\n",
    "\n",
    "            position = 0\n",
    "            entry_price = 0\n",
    "\n",
    "\n",
    "    # Scenario 3: Currently SHORT (position < 0)\n",
    "    elif position < 0:\n",
    "        sl_price = entry_price + long_stop_loss_multiplier * current_rolling_std\n",
    "        tp_price = entry_price - long_take_profit_multiplier * current_rolling_std\n",
    "\n",
    "        should_close = False\n",
    "        reason = \"\"\n",
    "\n",
    "        # if predicted_prob >= long_threshold: # Changed from buy_threshold to long_threshold\n",
    "        #     should_close = True\n",
    "        #     reason = \"Prediction above long threshold (close short)\"\n",
    "        if current_px >= sl_price:\n",
    "            should_close = True\n",
    "            reason = \"Short Stop Loss Hit\"\n",
    "        elif current_px <= tp_price:\n",
    "            should_close = True\n",
    "            reason = \"Short Take Profit Hit\"\n",
    "\n",
    "        if should_close:\n",
    "            trade_type = 'BUY_TO_COVER_SHORT'\n",
    "            close_price = current_px * (1 + slippage_ratio)\n",
    "            gross_pnl_on_trade = (entry_price - close_price) * abs(position)\n",
    "            net_pnl_on_trade = gross_pnl_on_trade - (close_price * abs(position) * commission_ratio)\n",
    "\n",
    "            realized_pnl += net_pnl_on_trade\n",
    "            capital += net_pnl_on_trade\n",
    " \n",
    "            trades.append({\n",
    "                'timestamp': current_time, 'type': trade_type, 'price': close_price, 'shares': position,\n",
    "                'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': current_equity,\n",
    "                'reason': reason, 'predicted_prob': predicted_prob\n",
    "            })\n",
    "            # print(f\"{current_time}: {trade_type} {abs(position):.2f} @ {close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f} | Reason: {reason}\")\n",
    "\n",
    "            position = 0\n",
    "            entry_price = 0\n",
    "\n",
    "# Final close-out at the end of backtest if any position is open\n",
    "final_equity_append_time = df['timestamp'].iloc[-1] + pd.Timedelta(seconds=1)\n",
    "if position != 0:\n",
    "    last_px = df['px'].iloc[-1]\n",
    "    if position > 0: # Close long\n",
    "        final_close_price = last_px * (1 - slippage_ratio)\n",
    "        gross_pnl_on_trade = (final_close_price - entry_price) * position\n",
    "        net_pnl_on_trade = gross_pnl_on_trade - (final_close_price * position * commission_ratio)\n",
    "        # capital += (position * final_close_price * (1 - commission_ratio))\n",
    "        trades.append({\n",
    "            'timestamp': final_equity_append_time, 'type': 'SELL_FINAL_LONG', 'price': final_close_price, 'shares': position,\n",
    "            'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': equity_curve[-1],\n",
    "            'reason': 'End of Backtest', 'predicted_prob': df['predicted_prob'].iloc[-1]\n",
    "        })\n",
    "        # print(f\"End of Backtest: SELL_FINAL_LONG {position:.2f} @ {final_close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f}\")\n",
    "    else: # Close short\n",
    "        final_close_price = last_px * (1 + slippage_ratio)\n",
    "        gross_pnl_on_trade = (entry_price - final_close_price) * abs(position)\n",
    "        net_pnl_on_trade = gross_pnl_on_trade - (final_close_price * abs(position) * commission_ratio)\n",
    "        # capital += (abs(position) * entry_price * (1 - commission_ratio))\n",
    "        trades.append({\n",
    "            'timestamp': final_equity_append_time, 'type': 'BUY_FINAL_SHORT', 'price': final_close_price, 'shares': position,\n",
    "            'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': equity_curve[-1],\n",
    "            'reason': 'End of Backtest', 'predicted_prob': df['predicted_prob'].iloc[-1]\n",
    "        })\n",
    "        # print(f\"End of Backtest: BUY_FINAL_SHORT {abs(position):.2f} @ {final_close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f}\")\n",
    "    realized_pnl += net_pnl_on_trade\n",
    "    position = 0\n",
    "\n",
    "# Final equity curve update\n",
    "equity_curve[-1] = capital\n",
    "\n",
    "# ===============================================\n",
    "# Performance Metrics Calculation (KEY CHANGES HERE)\n",
    "# ===============================================\n",
    "equity_series = pd.Series(equity_curve, index=df['timestamp'].tolist() + [final_equity_append_time])\n",
    "returns = equity_series.pct_change().dropna()\n",
    "\n",
    "# Total Return\n",
    "total_return = (capital - initial_capital) / initial_capital\n",
    "\n",
    "# Annualized Return - Based on Total Duration\n",
    "annualized_return = total_return # Default value if not enough data\n",
    "\n",
    "if len(df) > 1:\n",
    "    # Get the total duration of the backtest data\n",
    "    total_duration = df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]\n",
    "    total_duration_seconds = total_duration.total_seconds()\n",
    "\n",
    "    if total_duration_seconds > 0:\n",
    "        # Number of seconds in a year\n",
    "        seconds_in_year = 365 * 24 * 60 * 60\n",
    "\n",
    "        # Annualization factor: (seconds in a year) / (total seconds in backtest)\n",
    "        annualization_factor = seconds_in_year / total_duration_seconds\n",
    "\n",
    "        # Apply annualization\n",
    "        annualized_return = (1 + total_return)**annualization_factor - 1\n",
    "    else:\n",
    "        print(\"Warning: Total backtest duration is zero or invalid, cannot annualize return.\")\n",
    "else:\n",
    "    print(\"Warning: Not enough data points to calculate total duration for annualization.\")\n",
    "\n",
    "\n",
    "# Max Drawdown\n",
    "peak = equity_series.expanding(min_periods=1).max()\n",
    "drawdown = (equity_series - peak) / peak\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# Sharpe Ratio (assuming risk-free rate is 0)\n",
    "# For volatility, we use returns.std() and annualize it with the same factor\n",
    "annualized_volatility = returns.std() * np.sqrt(annualization_factor) if 'annualization_factor' in locals() and annualization_factor > 0 else returns.std()\n",
    "sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility != 0 else np.nan\n",
    "\n",
    "# Win Rate for closed trades\n",
    "if len(trades) > 0:\n",
    "    winning_trades = sum(1 for t in trades if 'pnl' in t and t['pnl'] > 0)\n",
    "    total_closed_trades = sum(1 for t in trades if 'pnl' in t)\n",
    "    win_rate = winning_trades / total_closed_trades if total_closed_trades > 0 else 0\n",
    "else:\n",
    "    win_rate = 0\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Backtesting Results Summary (Long-Short Strategy):\")\n",
    "print(f\"Initial Capital: {initial_capital:.2f}\")\n",
    "print(f\"Final Capital: {capital:.2f}\")\n",
    "print(f\"Total Return: {total_return:.2%}\")\n",
    "print(f\"Total Realized PnL: {realized_pnl:.2f}\")\n",
    "print(f\"å¹´åŒ–æ”¶ç›Š (è¿‘ä¼¼): {annualized_return:.2%}\")\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.2%}\")\n",
    "print(f\"Sharpe Ratio (Risk-Free Rate = 0): {sharpe_ratio:.2f}\")\n",
    "print(f\"Number of Trades: {len(trades)}\")\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===============================================\n",
    "# Plotting\n",
    "# ===============================================\n",
    "\n",
    "# Equity Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(equity_series.index, equity_series, label='Equity Curve')\n",
    "plt.title(f'{symbol_to_backtest} Long-Short Strategy Equity Curve')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Equity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Returns Distribution\n",
    "if not returns.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(returns, kde=True, bins=50)\n",
    "    plt.title(f'{symbol_to_backtest} Returns Distribution')\n",
    "    plt.xlabel('Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Drawdown Plot\n",
    "if not drawdown.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(drawdown.index, drawdown, label='Drawdown')\n",
    "    plt.fill_between(drawdown.index, drawdown, 0, where=(drawdown < 0), color='red', alpha=0.3)\n",
    "    plt.title(f'{symbol_to_backtest} Drawdown')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Drawdown (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81318af-ef28-4595-b642-f72f5d504fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "all_predictions_df = final_predictions_df\n",
    "all_predictions_df['timestamp'] = pd.to_datetime(all_predictions_df['timestamp'], unit='us') # Assuming microseconds\n",
    "\n",
    "symbol_to_backtest = all_predictions_df['symbol'].iloc[0] # Take the first symbol for now\n",
    "df = all_predictions_df[all_predictions_df['symbol'] == symbol_to_backtest].copy()\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# ===============================================\n",
    "# Backtesting Parameters (Ratios for costs)\n",
    "# ===============================================\n",
    "initial_capital = 100000  # Initial capital\n",
    "commission_ratio = 0.0005 # Commission ratio (e.0.05%)\n",
    "slippage_ratio = 0.001   # Slippage ratio (e.g., 0.02%)\n",
    "\n",
    "trade_size_ratio = 0.9    # Percentage of current equity to allocate per trade\n",
    "\n",
    "# Strategy Thresholds (needs optimization)\n",
    "long_threshold = 0.8 # Predicted probability above this to go long (buy)\n",
    "short_threshold = 0.2 # Predicted probability below this to go short (sell)\n",
    "\n",
    "beta = 3\n",
    "# Dynamic Take Profit / Stop Loss (in multiples of rolling_std/px)\n",
    "long_stop_loss_multiplier = beta\n",
    "long_take_profit_multiplier = beta\n",
    "short_stop_loss_multiplier = beta\n",
    "short_take_profit_multiplier = beta\n",
    "\n",
    "# ===============================================\n",
    "# Backtesting Main Logic (No change here from previous version)\n",
    "# ===============================================\n",
    "\n",
    "# Initialize account state\n",
    "capital = initial_capital\n",
    "position = 0          # Position size (positive for long, negative for short, 0 for flat)\n",
    "entry_price = 0       # Entry price for current position\n",
    "realized_pnl = 0      # Realized PnL from closed trades\n",
    "equity_curve = [initial_capital] # Equity curve\n",
    "\n",
    "# Record trades\n",
    "trades = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    current_time = df['timestamp'].iloc[i]\n",
    "    current_px = df['px'].iloc[i]\n",
    "    predicted_prob = df['predicted_prob_rolling_mean'].iloc[i]\n",
    "    current_rolling_std = df['rolling_std'].iloc[i]\n",
    "\n",
    "    # Calculate current total equity (capital + market value of position)\n",
    "    if position > 0: # Long position\n",
    "        unrealized_pnl = (current_px - entry_price) * position\n",
    "    elif position < 0: # Short position\n",
    "        unrealized_pnl = (entry_price - current_px) * abs(position) # Profit when price falls\n",
    "    else: # Flat\n",
    "        unrealized_pnl = 0\n",
    "\n",
    "    current_equity = capital\n",
    "    equity_curve.append(current_equity)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Strategy Execution - Long-Short\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Scenario 1: Currently FLAT (position == 0)\n",
    "    if position == 0:\n",
    "        if predicted_prob >= long_threshold: # Go Long Signal\n",
    "            trade_type = 'BUY_OPEN'\n",
    "            trade_price = current_px * (1 + slippage_ratio)\n",
    "            num_shares = (current_equity * trade_size_ratio) / (trade_price * (1 + commission_ratio))\n",
    "\n",
    "            if num_shares > 0:\n",
    "                position = num_shares\n",
    "                entry_price = trade_price\n",
    "                # capital -= (position * entry_price * (1 + commission_ratio))\n",
    "\n",
    "                trades.append({\n",
    "                    'timestamp': current_time, 'type': trade_type, 'price': entry_price, 'shares': position,\n",
    "                    'capital_after_trade': capital, 'equity_after_trade': current_equity, 'predicted_prob': predicted_prob\n",
    "                })\n",
    "                # print(f\"{current_time}: {trade_type} {position:.2f} @ {entry_price:.2f} (Prob: {predicted_prob:.4f}) | Capital: {capital:.2f}\")\n",
    "\n",
    "        elif predicted_prob <= short_threshold: # Go Short Signal\n",
    "            trade_type = 'SELL_SHORT_OPEN'\n",
    "            trade_price = current_px * (1 - slippage_ratio) # Price for short is lower (sell at market)\n",
    "            num_shares = (current_equity * trade_size_ratio) / (trade_price * (1 + commission_ratio))\n",
    "\n",
    "            if num_shares > 0:\n",
    "                position = -num_shares # Negative for short position\n",
    "                entry_price = trade_price\n",
    "                # capital -= (abs(position) * entry_price * (1 + commission_ratio))\n",
    "\n",
    "                trades.append({\n",
    "                    'timestamp': current_time, 'type': trade_type, 'price': entry_price, 'shares': position,\n",
    "                    'capital_after_trade': capital, 'equity_after_trade': current_equity, 'predicted_prob': predicted_prob\n",
    "                })\n",
    "                # print(f\"{current_time}: {trade_type} {position:.2f} @ {entry_price:.2f} (Prob: {predicted_prob:.4f}) | Capital: {capital:.2f}\")\n",
    "\n",
    "    # Scenario 2: Currently LONG (position > 0)\n",
    "    elif position > 0:\n",
    "        sl_price = entry_price - long_stop_loss_multiplier * current_rolling_std\n",
    "        tp_price = entry_price + long_take_profit_multiplier * current_rolling_std\n",
    "\n",
    "        should_close = False\n",
    "        reason = \"\"\n",
    "\n",
    "        if predicted_prob < short_threshold: # Changed from sell_threshold to short_threshold for consistent logic\n",
    "            should_close = True\n",
    "            reason = \"Prediction below short threshold (close long)\"\n",
    "        if current_px <= sl_price:\n",
    "            should_close = True\n",
    "            reason = \"Long Stop Loss Hit\"\n",
    "        elif current_px >= tp_price and predicted_prob < long_threshold:\n",
    "            should_close = True\n",
    "            reason = \"Long Take Profit Hit\"\n",
    "\n",
    "        if should_close:\n",
    "            trade_type = 'SELL_CLOSE_LONG'\n",
    "            close_price = current_px * (1 - slippage_ratio)\n",
    "            gross_pnl_on_trade = (close_price - entry_price) * position\n",
    "            net_pnl_on_trade = gross_pnl_on_trade - (close_price * position * commission_ratio)\n",
    "\n",
    "            realized_pnl += net_pnl_on_trade\n",
    "            capital += net_pnl_on_trade\n",
    "\n",
    "            trades.append({\n",
    "                'timestamp': current_time, 'type': trade_type, 'price': close_price, 'shares': position,\n",
    "                'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': current_equity,\n",
    "                'reason': reason, 'predicted_prob': predicted_prob\n",
    "            })\n",
    "            # print(f\"{current_time}: {trade_type} {position:.2f} @ {close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f} | Reason: {reason}\")\n",
    "\n",
    "            position = 0\n",
    "            entry_price = 0\n",
    "\n",
    "\n",
    "    # Scenario 3: Currently SHORT (position < 0)\n",
    "    elif position < 0:\n",
    "        sl_price = entry_price + long_stop_loss_multiplier * current_rolling_std\n",
    "        tp_price = entry_price - long_take_profit_multiplier * current_rolling_std\n",
    "\n",
    "        should_close = False\n",
    "        reason = \"\"\n",
    "\n",
    "        if predicted_prob >= long_threshold: # Changed from buy_threshold to long_threshold\n",
    "            should_close = True\n",
    "            reason = \"Prediction above long threshold (close short)\"\n",
    "        if current_px >= sl_price:\n",
    "            should_close = True\n",
    "            reason = \"Short Stop Loss Hit\"\n",
    "        elif current_px <= tp_price and predicted_prob > short_threshold:\n",
    "            should_close = True\n",
    "            reason = \"Short Take Profit Hit\"\n",
    "\n",
    "        if should_close:\n",
    "            trade_type = 'BUY_TO_COVER_SHORT'\n",
    "            close_price = current_px * (1 + slippage_ratio)\n",
    "            gross_pnl_on_trade = (entry_price - close_price) * abs(position)\n",
    "            net_pnl_on_trade = gross_pnl_on_trade - (close_price * abs(position) * commission_ratio)\n",
    "\n",
    "            realized_pnl += net_pnl_on_trade\n",
    "            capital += net_pnl_on_trade\n",
    "\n",
    "            trades.append({\n",
    "                'timestamp': current_time, 'type': trade_type, 'price': close_price, 'shares': position,\n",
    "                'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': current_equity,\n",
    "                'reason': reason, 'predicted_prob': predicted_prob\n",
    "            })\n",
    "            # print(f\"{current_time}: {trade_type} {abs(position):.2f} @ {close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f} | Reason: {reason}\")\n",
    "\n",
    "            position = 0\n",
    "            entry_price = 0\n",
    "\n",
    "# Final close-out at the end of backtest if any position is open\n",
    "final_equity_append_time = df['timestamp'].iloc[-1] + pd.Timedelta(seconds=1)\n",
    "if position != 0:\n",
    "    last_px = df['px'].iloc[-1]\n",
    "    if position > 0: # Close long\n",
    "        final_close_price = last_px * (1 - slippage_ratio)\n",
    "        gross_pnl_on_trade = (final_close_price - entry_price) * position\n",
    "        net_pnl_on_trade = gross_pnl_on_trade - (final_close_price * position * commission_ratio)\n",
    "        # capital += (position * final_close_price * (1 - commission_ratio))\n",
    "        trades.append({\n",
    "            'timestamp': final_equity_append_time, 'type': 'SELL_FINAL_LONG', 'price': final_close_price, 'shares': position,\n",
    "            'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': equity_curve[-1],\n",
    "            'reason': 'End of Backtest', 'predicted_prob': df['predicted_prob'].iloc[-1]\n",
    "        })\n",
    "        # print(f\"End of Backtest: SELL_FINAL_LONG {position:.2f} @ {final_close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f}\")\n",
    "    else: # Close short\n",
    "        final_close_price = last_px * (1 + slippage_ratio)\n",
    "        gross_pnl_on_trade = (entry_price - final_close_price) * abs(position)\n",
    "        net_pnl_on_trade = gross_pnl_on_trade - (final_close_price * abs(position) * commission_ratio)\n",
    "        # capital += (abs(position) * entry_price * (1 - commission_ratio))\n",
    "        trades.append({\n",
    "            'timestamp': final_equity_append_time, 'type': 'BUY_FINAL_SHORT', 'price': final_close_price, 'shares': position,\n",
    "            'pnl': net_pnl_on_trade, 'capital_after_trade': capital, 'equity_after_trade': equity_curve[-1],\n",
    "            'reason': 'End of Backtest', 'predicted_prob': df['predicted_prob'].iloc[-1]\n",
    "        })\n",
    "        # print(f\"End of Backtest: BUY_FINAL_SHORT {abs(position):.2f} @ {final_close_price:.2f} | PnL: {net_pnl_on_trade:.2f} | Capital: {capital:.2f}\")\n",
    "    realized_pnl += net_pnl_on_trade\n",
    "    position = 0\n",
    "\n",
    "# Final equity curve update\n",
    "equity_curve[-1] = capital\n",
    "\n",
    "# ===============================================\n",
    "# Performance Metrics Calculation (KEY CHANGES HERE)\n",
    "# ===============================================\n",
    "equity_series = pd.Series(equity_curve, index=df['timestamp'].tolist() + [final_equity_append_time])\n",
    "returns = equity_series.pct_change().dropna()\n",
    "\n",
    "# Total Return\n",
    "total_return = (capital - initial_capital) / initial_capital\n",
    "\n",
    "# Annualized Return - Based on Total Duration\n",
    "annualized_return = total_return # Default value if not enough data\n",
    "\n",
    "if len(df) > 1:\n",
    "    # Get the total duration of the backtest data\n",
    "    total_duration = df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]\n",
    "    total_duration_seconds = total_duration.total_seconds()\n",
    "\n",
    "    if total_duration_seconds > 0:\n",
    "        # Number of seconds in a year\n",
    "        seconds_in_year = 365 * 24 * 60 * 60\n",
    "\n",
    "        # Annualization factor: (seconds in a year) / (total seconds in backtest)\n",
    "        annualization_factor = seconds_in_year / total_duration_seconds\n",
    "\n",
    "        # Apply annualization\n",
    "        annualized_return = (1 + total_return)**annualization_factor - 1\n",
    "    else:\n",
    "        print(\"Warning: Total backtest duration is zero or invalid, cannot annualize return.\")\n",
    "else:\n",
    "    print(\"Warning: Not enough data points to calculate total duration for annualization.\")\n",
    "\n",
    "\n",
    "# Max Drawdown\n",
    "peak = equity_series.expanding(min_periods=1).max()\n",
    "drawdown = (equity_series - peak) / peak\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# Sharpe Ratio (assuming risk-free rate is 0)\n",
    "# For volatility, we use returns.std() and annualize it with the same factor\n",
    "annualized_volatility = returns.std() * np.sqrt(annualization_factor) if 'annualization_factor' in locals() and annualization_factor > 0 else returns.std()\n",
    "sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility != 0 else np.nan\n",
    "\n",
    "# Win Rate for closed trades\n",
    "if len(trades) > 0:\n",
    "    winning_trades = sum(1 for t in trades if 'pnl' in t and t['pnl'] > 0)\n",
    "    total_closed_trades = sum(1 for t in trades if 'pnl' in t)\n",
    "    win_rate = winning_trades / total_closed_trades if total_closed_trades > 0 else 0\n",
    "else:\n",
    "    win_rate = 0\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Backtesting Results Summary (Long-Short Strategy):\")\n",
    "print(f\"Initial Capital: {initial_capital:.2f}\")\n",
    "print(f\"Final Capital: {capital:.2f}\")\n",
    "print(f\"Total Return: {total_return:.2%}\")\n",
    "print(f\"Total Realized PnL: {realized_pnl:.2f}\")\n",
    "print(f\"å¹´åŒ–æ”¶ç›Š (è¿‘ä¼¼): {annualized_return:.2%}\")\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.2%}\")\n",
    "print(f\"Sharpe Ratio (Risk-Free Rate = 0): {sharpe_ratio:.2f}\")\n",
    "print(f\"Number of Trades: {len(trades)}\")\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===============================================\n",
    "# Plotting\n",
    "# ===============================================\n",
    "\n",
    "# Equity Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(equity_series.index, equity_series, label='Equity Curve')\n",
    "plt.title(f'{symbol_to_backtest} Long-Short Strategy Equity Curve')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Equity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Returns Distribution\n",
    "if not returns.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(returns, kde=True, bins=50)\n",
    "    plt.title(f'{symbol_to_backtest} Returns Distribution')\n",
    "    plt.xlabel('Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Drawdown Plot\n",
    "if not drawdown.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(drawdown.index, drawdown, label='Drawdown')\n",
    "    plt.fill_between(drawdown.index, drawdown, 0, where=(drawdown < 0), color='red', alpha=0.3)\n",
    "    plt.title(f'{symbol_to_backtest} Drawdown')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Drawdown (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3750f-a5d6-4e20-96f8-b3a5e2975cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3201e0-8f06-48f9-94a3-f1522a406274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
