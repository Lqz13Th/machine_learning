{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0511919-059e-406d-93cf-f587dbd9266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Asset  FutureK  Vol  Exp_Weight  RiskAdj_Weight  MV_Weight\n",
      "0   BTC     0.12  0.6       0.489           0.456      0.133\n",
      "1   ETH    -0.05  0.8      -0.169          -0.142     -0.102\n",
      "2   BNB     0.07  0.5       0.249           0.319      0.329\n",
      "3   SOL    -0.02  0.9      -0.063          -0.051     -0.362\n",
      "4   XRP     0.01  0.7       0.030           0.033     -0.073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# ===== 假设输入 =====\n",
    "assets = [\"BTC\", \"ETH\", \"BNB\", \"SOL\", \"XRP\"]\n",
    "\n",
    "# 模型预测未来收益 (正负都有)\n",
    "futurek = np.array([0.12, -0.05, 0.07, -0.02, 0.01])\n",
    "\n",
    "# 历史波动率（假设是年化标准差）\n",
    "volatility = np.array([0.6, 0.8, 0.5, 0.9, 0.7])\n",
    "\n",
    "# 协方差矩阵（随便造一个正定的）\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(len(assets), len(assets))\n",
    "cov_matrix = np.dot(A, A.T)\n",
    "\n",
    "\n",
    "# ===== 2. 指数权重 (允许多空) =====\n",
    "def exp_weights(scores, beta=5.0):\n",
    "    # 分成正和负两部分\n",
    "    pos = np.exp(beta * np.clip(scores, 0, None))\n",
    "    neg = -np.exp(beta * np.clip(-scores, 0, None))\n",
    "    w = pos + neg\n",
    "    return w / np.sum(np.abs(w))  # 归一化，使绝对值和=1\n",
    "\n",
    "w_exp = exp_weights(futurek, beta=5.0)\n",
    "\n",
    "\n",
    "# ===== 3. 风险调整权重 (score / vol) =====\n",
    "def risk_adjusted_weights(scores, vol):\n",
    "    adj = scores / vol\n",
    "    return adj / np.sum(np.abs(adj))  # 保证多空资金平衡\n",
    "\n",
    "w_risk = risk_adjusted_weights(futurek, volatility)\n",
    "\n",
    "\n",
    "# ===== 4. 均值–方差优化 (可多空) =====\n",
    "def mean_variance_opt(mu, cov, lam=0.1):\n",
    "    \"\"\"\n",
    "    mu: 预测收益向量 (正负都行)\n",
    "    cov: 协方差矩阵\n",
    "    lam: 风险厌恶参数\n",
    "    \"\"\"\n",
    "    cov_inv = inv(cov + lam * np.eye(len(mu)))  # 稳定化\n",
    "    w = cov_inv @ mu\n",
    "    return w / np.sum(np.abs(w))  # 归一化，保证多空资金平衡\n",
    "\n",
    "w_mv = mean_variance_opt(futurek, cov_matrix, lam=0.1)\n",
    "\n",
    "\n",
    "# ===== 输出对比 =====\n",
    "df = pd.DataFrame({\n",
    "    \"Asset\": assets,\n",
    "    \"FutureK\": futurek,\n",
    "    \"Vol\": volatility,\n",
    "    \"Exp_Weight\": w_exp.round(3),\n",
    "    \"RiskAdj_Weight\": w_risk.round(3),\n",
    "    \"MV_Weight\": w_mv.round(3),\n",
    "})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "319f761d-dce0-4421-b79d-e5e523931de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Asset  FutureK  Vol  Exp_Weight  RiskAdj_Weight  MV_Weight\n",
      "0   BTC     0.22  0.6       0.390           0.352      0.161\n",
      "1   ETH    -0.05  0.8      -0.362          -0.369     -0.090\n",
      "2   BNB     0.07  0.5       0.097           0.134      0.339\n",
      "3   SOL    -0.02  0.9      -0.138          -0.131     -0.343\n",
      "4   XRP     0.01  0.7       0.013           0.014     -0.067\n",
      "\n",
      "多头合计 / 空头合计：\n",
      "Exp   : 0.5 / -0.5\n",
      "Risk  : 0.5 / -0.5\n",
      "MV    : 0.5 / -0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# ===== 假设输入 =====\n",
    "assets = [\"BTC\", \"ETH\", \"BNB\", \"SOL\", \"XRP\"]\n",
    "\n",
    "# 模型预测未来收益 (正负都有)\n",
    "futurek = np.array([0.22, -0.05, 0.07, -0.02, 0.01])\n",
    "\n",
    "# 历史波动率（假设是年化标准差）\n",
    "volatility = np.array([0.6, 0.8, 0.5, 0.9, 0.7])\n",
    "\n",
    "# 协方差矩阵（正定）\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(len(assets), len(assets))\n",
    "cov_matrix = np.dot(A, A.T)\n",
    "\n",
    "\n",
    "# ===== 仓位中性化工具函数 =====\n",
    "def dollar_neutralize(weights):\n",
    "    \"\"\"\n",
    "    保证多空资金平衡：多头资金=空头资金=0.5\n",
    "    \"\"\"\n",
    "    long_sum = weights[weights > 0].sum()\n",
    "    short_sum = -weights[weights < 0].sum()\n",
    "    if long_sum == 0 or short_sum == 0:\n",
    "        return weights / np.sum(np.abs(weights))\n",
    "    w = weights.copy()\n",
    "    w[w > 0] /= long_sum\n",
    "    w[w < 0] /= short_sum\n",
    "    return w / 2  # 多头合计=0.5，空头合计=-0.5\n",
    "\n",
    "\n",
    "# ===== 2. 指数权重 (允许多空) =====\n",
    "def exp_weights(scores, beta=5.0):\n",
    "    pos = np.exp(beta * np.clip(scores, 0, None))\n",
    "    neg = -np.exp(beta * np.clip(-scores, 0, None))\n",
    "    w = pos + neg\n",
    "    return dollar_neutralize(w)\n",
    "\n",
    "w_exp = exp_weights(futurek, beta=3.0)\n",
    "\n",
    "\n",
    "# ===== 3. 风险调整权重 (score / vol) =====\n",
    "def risk_adjusted_weights(scores, vol):\n",
    "    adj = scores / vol\n",
    "    return dollar_neutralize(adj)\n",
    "\n",
    "w_risk = risk_adjusted_weights(futurek, volatility)\n",
    "\n",
    "\n",
    "# ===== 4. 均值–方差优化 (可多空) =====\n",
    "def mean_variance_opt(mu, cov, lam=0.1):\n",
    "    cov_inv = inv(cov + lam * np.eye(len(mu)))  # 稳定化\n",
    "    w = cov_inv @ mu\n",
    "    return dollar_neutralize(w)\n",
    "\n",
    "w_mv = mean_variance_opt(futurek, cov_matrix, lam=0.1)\n",
    "\n",
    "\n",
    "# ===== 输出对比 =====\n",
    "df = pd.DataFrame({\n",
    "    \"Asset\": assets,\n",
    "    \"FutureK\": futurek,\n",
    "    \"Vol\": volatility,\n",
    "    \"Exp_Weight\": w_exp.round(3),\n",
    "    \"RiskAdj_Weight\": w_risk.round(3),\n",
    "    \"MV_Weight\": w_mv.round(3),\n",
    "})\n",
    "\n",
    "print(df)\n",
    "print(\"\\n多头合计 / 空头合计：\")\n",
    "print(\"Exp   :\", w_exp[w_exp > 0].sum().round(3), \"/\", w_exp[w_exp < 0].sum().round(3))\n",
    "print(\"Risk  :\", w_risk[w_risk > 0].sum().round(3), \"/\", w_risk[w_risk < 0].sum().round(3))\n",
    "print(\"MV    :\", w_mv[w_mv > 0].sum().round(3), \"/\", w_mv[w_mv < 0].sum().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebae9d6-ac6f-4a2f-aaa0-1143a629b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def lag_exprs(col: str, lag: int) -> pl.Expr:\n",
    "    return pl.col(col).shift(lag).alias(f\"{col}_lag_{lag}\")\n",
    "\n",
    "def diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    return (pl.col(col) - pl.col(col).shift(lag)).alias(f\"{col}_diff_{lag}\")\n",
    "\n",
    "def second_order_diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    # 二阶差分 = 一阶差分的差分\n",
    "    first_diff = pl.col(col) - pl.col(col).shift(lag)\n",
    "    second_diff = first_diff - first_diff.shift(lag)\n",
    "    return second_diff.alias(f\"{col}_second_order_diff_{lag}\")\n",
    "\n",
    "def momentum_ratio_expr(col: str, window: int = 200) -> pl.Expr:\n",
    "    # 动量比率 = x_t / x_{t-lag}\n",
    "    EPSILON = 1e-5\n",
    "    return ((pl.col(col).abs() + EPSILON).log1p() - (pl.col(col).abs() + EPSILON).shift(window).log1p()).alias(f\"{col}_momentum_ratio_{window}\")\n",
    "\n",
    "def rolling_volatility_expr(col: str, window: int) -> pl.Expr:\n",
    "    return pl.col(col).rolling_std(window).alias(f\"{col}_volatility_{window}\")\n",
    "\n",
    "def cross_minus_expr(a: str, b: str) -> pl.Expr:\n",
    "    return (pl.col(a) - (pl.col(b) + 1e-8)).alias(f\"{a}_minus_{b}\")\n",
    "\n",
    "def cols_to_transforms(\n",
    "        df: pl.DataFrame,\n",
    "        exclude_cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        cols = df.collect_schema().names()\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    cols = [\n",
    "        col for col in cols\n",
    "        if col not in exclude_cols and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_scaled')\n",
    "        ) and not col.startswith(\"z_\")\n",
    "    ]\n",
    "\n",
    "    return cols\n",
    "\n",
    "def batch_apply_single_exprs(\n",
    "        window: int,\n",
    "        lags: [int],\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    single_exprs = []\n",
    "    # single features transformation\n",
    "    for col in cols:\n",
    "        single_exprs.extend([\n",
    "            momentum_ratio_expr(col, window),\n",
    "            rolling_volatility_expr(col, window),\n",
    "        ])\n",
    "        for lag in lags:\n",
    "            single_exprs.extend([\n",
    "            lag_exprs(col, lag),\n",
    "            diff_expr(col, lag),\n",
    "            second_order_diff_expr(col, lag),\n",
    "        ])\n",
    "            \n",
    "    return single_exprs\n",
    "\n",
    "def batch_apply_multi_exprs(\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    multi_exprs = []\n",
    "\n",
    "    n = len(cols)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            a, b = cols[i], cols[j]\n",
    "            multi_exprs.extend([\n",
    "                cross_minus_expr(a, b),\n",
    "            ])\n",
    "\n",
    "    return multi_exprs\n",
    "\n",
    "def batch_apply_transforms(\n",
    "        df_to_transforms: pl.DataFrame,\n",
    "        window: int,\n",
    "        lags: [int],\n",
    "        log1p_cols: List[str] = None,\n",
    "        exclude_cols: List[str] = None,\n",
    ") -> pl.DataFrame:\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "        \n",
    "    if log1p_cols is None:\n",
    "        log1p_cols = []\n",
    "\n",
    "    for col in log1p_cols:\n",
    "        if col in df_to_transforms.columns:\n",
    "            df_to_transforms = df_to_transforms.with_columns([\n",
    "                pl.col(col).clip(lower_bound=0.0).log1p().alias(col)\n",
    "            ])\n",
    "            \n",
    "    # base_cols = cols_to_transforms(df_to_transforms, exclude_cols)\n",
    "   \n",
    "    # single_exprs = batch_apply_single_exprs(window, lags, base_cols)\n",
    "    # multi_exprs = batch_apply_multi_exprs(base_cols)\n",
    "\n",
    "    # exprs = single_exprs + multi_exprs\n",
    "    # return df_to_transforms.with_columns(exprs)\n",
    "    return df_to_transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67429650-ff19-48a7-a11b-c752d155db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def split_df_by_month(\n",
    "    origin_input_df: pl.DataFrame,\n",
    "    ts_col: str = \"timestamp\"\n",
    ") -> List[pl.DataFrame]:\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(ts_col).cast(pl.Datetime(\"ms\")).alias(f\"{ts_col}_dt\")  # 注意这里加了 \"ms\"\n",
    "    ])\n",
    "\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(f\"{ts_col}_dt\").dt.truncate(\"1mo\").alias(\"month_start\")\n",
    "    ])\n",
    "\n",
    "    unique_months = origin_input_df.select(\"month_start\").unique().sort(\"month_start\")\n",
    "\n",
    "    monthly_dfs = [\n",
    "        origin_input_df.filter(pl.col(\"month_start\") == mt).drop(\"month_start\")\n",
    "        for mt in unique_months[\"month_start\"]\n",
    "    ]\n",
    "\n",
    "    return monthly_dfs\n",
    "\n",
    "    \n",
    "def split_df_by_week(\n",
    "    origin_input_df: pl.DataFrame,\n",
    "    ts_col: str = \"timestamp\"\n",
    ") -> List[pl.DataFrame]:\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(ts_col).cast(pl.Datetime(\"ms\")).alias(f\"{ts_col}_dt\")  # 注意这里加了 \"ms\"\n",
    "    ])\n",
    "\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(f\"{ts_col}_dt\").dt.truncate(\"1w\").alias(\"week_start\")\n",
    "    ])\n",
    "\n",
    "    unique_weeks = origin_input_df.select(\"week_start\").unique().sort(\"week_start\")\n",
    "\n",
    "    weekly_dfs = [\n",
    "        origin_input_df.filter(pl.col(\"week_start\") == wk).drop(\"week_start\")\n",
    "        for wk in unique_weeks[\"week_start\"]\n",
    "    ]\n",
    "\n",
    "    return weekly_dfs\n",
    "\n",
    "def clean_df_drop_nulls(\n",
    "        df_to_clean: pl.DataFrame,\n",
    "        null_threshold: int = 500000,\n",
    "        verbose: bool = True\n",
    ") -> pl.DataFrame:\n",
    "    pd_df = df_to_clean.to_pandas()\n",
    "    del df_to_clean\n",
    "    gc.collect()\n",
    "    print(\"converted\")\n",
    "    \n",
    "    null_counts = pd_df.isnull().sum()\n",
    "    cols_to_drop = null_counts[null_counts > null_threshold].index\n",
    "\n",
    "    pd_df_cleaned = pd_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"各列空值数量：\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        print(f\"删除空值超过 {null_threshold} 的列：{list(cols_to_drop)}\")\n",
    "        print(f\"删除列后，DataFrame形状：{pd_df_cleaned.shape}\")\n",
    "    \n",
    "    del pd_df\n",
    "    gc.collect()    \n",
    "    \n",
    "    pd_df_clean = pd_df_cleaned.dropna()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"删除空值行后，DataFrame形状：{pd_df_clean.shape}\")\n",
    "\n",
    "    del pd_df_cleaned\n",
    "    gc.collect()   \n",
    "    \n",
    "    pl_df_clean = pl.from_pandas(pd_df_clean)\n",
    "\n",
    "    del pd_df_clean\n",
    "    gc.collect()\n",
    "    \n",
    "    return pl_df_clean\n",
    "\n",
    "def avg_steps_to_volatility(prices: np.ndarray, target_ratio: float) -> int:\n",
    "    n = len(prices)\n",
    "    steps_list = []\n",
    "    for i in tqdm(range(n), desc=f\"cal abs change {target_ratio*100:.2f}% avg steps\"):\n",
    "        start_price = prices[i]\n",
    "        steps = -1\n",
    "        for j in range(i + 1, n):\n",
    "            change = abs(prices[j] / start_price - 1)\n",
    "            if change >= target_ratio:\n",
    "                steps = j - i\n",
    "                break\n",
    "        if steps != -1:\n",
    "            steps_list.append(steps)\n",
    "    if len(steps_list) == 0:\n",
    "        return -1\n",
    "    return int(np.mean(steps_list))\n",
    "\n",
    "def future_return_expr(price_col: str, step: int) -> pl.Expr:\n",
    "    return ((pl.col(price_col).shift(-step) - pl.col(price_col)) / pl.col(price_col)).alias(f\"future_return_{step}\")\n",
    "\n",
    "def rolling_minmax_scaled_expr(\n",
    "        col: str,\n",
    "        min_col: str,\n",
    "        max_col: str,\n",
    "        scaled_col: str\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        ((pl.col(col) - pl.col(min_col)) / (pl.col(max_col) - pl.col(min_col) + 1e-9))\n",
    "        .clip(0.0, 1.0)\n",
    "        .fill_null(0.5)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "def rolling_minmax_normalize(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')  # scaled 是最终产物，保留\n",
    "           and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_rolling_max') or\n",
    "                col.endswith('_rolling_min')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rolling_cols = []\n",
    "    for column in columns_to_normalize:\n",
    "        rolling_cols.extend([\n",
    "            pl.col(column).rolling_max(window, min_samples=1).alias(f\"{column}_rolling_max\"),\n",
    "            pl.col(column).rolling_min(window, min_samples=1).alias(f\"{column}_rolling_min\"),\n",
    "        ])\n",
    "\n",
    "    intermediate_cols = [\n",
    "                            f\"{column}_rolling_max\" for column in columns_to_normalize\n",
    "                        ] + [\n",
    "                            f\"{column}_rolling_min\" for column in columns_to_normalize\n",
    "                        ]\n",
    "\n",
    "    return (\n",
    "        rollin_df\n",
    "        .with_columns(rolling_cols)\n",
    "        .with_columns([\n",
    "            rolling_minmax_scaled_expr(\n",
    "                col=column,\n",
    "                min_col=f\"{column}_rolling_min\",\n",
    "                max_col=f\"{column}_rolling_max\",\n",
    "                scaled_col=f\"{column}_scaled\"\n",
    "            ) for column in columns_to_normalize\n",
    "        ])\n",
    "        .drop(intermediate_cols)\n",
    "    )\n",
    "\n",
    "def rolling_mean_tanh_scaled_expr(\n",
    "        col: str,\n",
    "        scaled_col: str,\n",
    "        window: int\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(col)\n",
    "        # .rolling_mean(window, min_samples=1)\n",
    "        .tanh()\n",
    "        # .rolling_mean(window, min_samples=1)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "def rolling_mean_tanh_normalize(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')\n",
    "    ]\n",
    "\n",
    "    return rollin_df.with_columns([\n",
    "        rolling_mean_tanh_scaled_expr(\n",
    "            col=column,\n",
    "            scaled_col=f\"{column}_scaled\",\n",
    "            window=window\n",
    "        ) for column in columns_to_normalize\n",
    "    ])\n",
    "\n",
    "def rolling_z_tanh_normalize(\n",
    "    rollin_df: pl.DataFrame,\n",
    "    window: int,\n",
    "    rolling_mean_window: int,\n",
    ") -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')\n",
    "    ]\n",
    "\n",
    "    return rollin_df.with_columns([\n",
    "        z_score_tanh_expr(\n",
    "            col=column,\n",
    "            scaled_col=f\"{column}_zscaled\",\n",
    "            window=window,\n",
    "            rolling_mean_window=rolling_mean_window,\n",
    "        ) for column in columns_to_normalize\n",
    "    ]) \n",
    "\n",
    "def z_score_tanh_expr(\n",
    "    col: str,\n",
    "    scaled_col: str,\n",
    "    window: int,\n",
    "    rolling_mean_window: int,\n",
    ") -> pl.Expr:\n",
    "    EPSILON = 1e-6\n",
    "    mean_expr = pl.col(col).rolling_mean(window, min_samples=1)\n",
    "    std_expr = pl.col(col).rolling_std(window, min_samples=1).fill_nan(0)\n",
    "\n",
    "    return (\n",
    "        ((pl.col(col) - mean_expr) / (std_expr + EPSILON))\n",
    "        .fill_nan(0)\n",
    "        .fill_null(0)\n",
    "        .clip(-3.0, 3.0)\n",
    "        .tanh()\n",
    "        .rolling_mean(rolling_mean_window, min_samples=1)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "\n",
    "def rolling_mean_smooth(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_smooth = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['px', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')\n",
    "    ]\n",
    "\n",
    "    return rollin_df.with_columns([\n",
    "        rolling_mean_scaled_expr(\n",
    "            col=column,\n",
    "            scaled_col=f\"{column}_scaled\",\n",
    "            window=window\n",
    "        ) for column in columns_to_smooth\n",
    "    ])\n",
    "\n",
    "def rolling_mean_scaled_expr(\n",
    "        col: str,\n",
    "        scaled_col: str,\n",
    "        window: int\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(col)\n",
    "        .rolling_mean(window)\n",
    "        .fill_null(strategy=\"zero\")  # 或 strategy=\"forward\" 也行\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "\n",
    "def rolling_ic_ir_icto_index(\n",
    "        df: pl.DataFrame,\n",
    "        target_col: str,\n",
    "        exclude_prefixes: list[str],\n",
    "        window_size: int,\n",
    "        step: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col.endswith(\"_scaled\") \n",
    "            and (col.startswith(\"z_\") or col.startswith(\"raw_\")) \n",
    "            and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "            and not col.startswith(\"future_return_\")\n",
    "            and col != \"px\"\n",
    "    ]\n",
    "\n",
    "    # feature_cols = [\n",
    "    #     col for col in df.columns\n",
    "    #     if col.startswith(\"z_\") \n",
    "    #         and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "    #         and not col.startswith(\"future_return_\")\n",
    "    #         and col != \"px\"\n",
    "    # ]\n",
    "\n",
    "    n = df.height\n",
    "    results = []\n",
    "    prev_ranks = {}\n",
    "\n",
    "    for start in tqdm(range(0, n - window_size + 1, step), desc=\"Rolling IC & ICTO\"):\n",
    "        end = start + window_size\n",
    "        df_win = df.slice(start, window_size)\n",
    "\n",
    "        # rank 转换\n",
    "        df_ranked = df_win.with_columns([\n",
    "            (pl.col(c).rank(method=\"average\") / window_size).alias(c + \"_rank\") for c in feature_cols + [target_col]\n",
    "        ])\n",
    "        target_rank_col = target_col + \"_rank\"\n",
    "\n",
    "        for feat in feature_cols:\n",
    "            feat_rank_col = feat + \"_rank\"\n",
    "            ic = df_ranked.select(\n",
    "                pl.corr(pl.col(feat_rank_col), pl.col(target_rank_col)).alias(\"ic\")\n",
    "            ).to_series()[0]\n",
    "\n",
    "            turnover = None\n",
    "            if feat in prev_ranks:\n",
    "                cur_ranks = df_ranked[feat_rank_col].to_numpy()\n",
    "                prev = prev_ranks[feat]\n",
    "                if len(prev) == len(cur_ranks):\n",
    "                    turnover = np.mean(np.abs(cur_ranks - prev))\n",
    "\n",
    "            # 更新 prev_ranks\n",
    "            prev_ranks[feat] = df_ranked[feat_rank_col].to_numpy()\n",
    "\n",
    "            results.append({\n",
    "                \"window_start\": int(start),\n",
    "                \"window_end\": int(end - 1),\n",
    "                \"factor\": str(feat),\n",
    "                \"ic\": float(ic) if not np.isnan(ic) else None,\n",
    "                \"turnover\": float(turnover) if turnover is not None else None\n",
    "            })\n",
    "\n",
    "    df_result = pl.DataFrame(\n",
    "        results,\n",
    "        schema={\n",
    "            \"window_start\": pl.Int64,\n",
    "            \"window_end\": pl.Int64,\n",
    "            \"factor\": pl.Utf8,\n",
    "            \"ic\": pl.Float64,\n",
    "            \"turnover\": pl.Float64,\n",
    "        }\n",
    "    )      \n",
    "    return (\n",
    "        df_result\n",
    "        .group_by(\"factor\")\n",
    "        .agg([\n",
    "            pl.mean(\"ic\").alias(\"mean_ic\"),\n",
    "            pl.std(\"ic\").alias(\"std_ic\"),\n",
    "            pl.mean(\"turnover\").alias(\"mean_turnover\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"mean_ic\") / pl.col(\"std_ic\")).alias(\"ir\"),\n",
    "            (pl.col(\"mean_ic\") / (pl.col(\"mean_turnover\") + 1e-8)).abs().alias(\"icto\")\n",
    "        ])\n",
    "        .sort(\"icto\", descending=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c30d194-5897-4d42-a831-d70640c473aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def fast_build_long_cross_sections(symbol_dfs: dict[str, pl.DataFrame]) -> pl.DataFrame:\n",
    "    long_df = pl.concat(list(symbol_dfs.values()))\n",
    "\n",
    "    return long_df.sort(\"timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bb5b20-9803-4ea0-a432-2d154dbd4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_symbol(\n",
    "    symbol: str,\n",
    "    threshold: str = \"0.002\",\n",
    "    feat_cal_window: str = \"500\",\n",
    "    data_dir: str = \"C:/quant/data/binance_resampled_data/\",\n",
    "    feat_trans_window: int = 350,\n",
    "    feat_trans_lags: list[int] = [5, 10, 20, 50, 100, 150, 200],\n",
    "    feat_norm_window: int = 500,\n",
    "    feat_norm_rolling_mean_window: int = 10,\n",
    ") -> pl.DataFrame:\n",
    "    symbol.upper()\n",
    "    \n",
    "    file = f\"{symbol}_factors_threshold{threshold}_rolling{feat_cal_window}.csv\"\n",
    "    path = data_dir + file\n",
    "    df = pl.read_csv(path)\n",
    "\n",
    "    # 删掉不需要的列\n",
    "    df = df.drop([\n",
    "        \"top_acc_longShortRatio\", \"top_pos_longShortRatio\", \"acc_longShortRatio\"\n",
    "    ])\n",
    "\n",
    "    # 对部分列进行 log1p + lag 变换\n",
    "    cols_to_log1p = [\n",
    "        \"far_bid_price\", \"far_ask_price\",\n",
    "        \"best_bid_price\", \"best_ask_price\",\n",
    "        \"sum_buy_sz\", \"sum_sell_sz\",\n",
    "        \"ts_duration\", \"real_bid_amount_sum\", \"real_ask_amount_sum\",\n",
    "    ]\n",
    "    df = batch_apply_transforms(df, feat_trans_window, feat_trans_lags, cols_to_log1p)\n",
    "\n",
    "    # rolling z-score + tanh 归一\n",
    "    df = rolling_z_tanh_normalize(df, feat_norm_window, feat_norm_rolling_mean_window)\n",
    "\n",
    "    # 去掉 std=0 的列\n",
    "    stds = df.select([\n",
    "        pl.col(col).std().alias(col)\n",
    "        for col in df.columns\n",
    "        if df[col].dtype in (pl.Float64, pl.Int64)\n",
    "    ])\n",
    "    zero_std_cols = [col for col in stds.columns if stds[0, col] == 0.0]\n",
    "    df = df.drop(zero_std_cols)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2a88b8-2140-46ef-8ec9-e4b4ea0e10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_valid_cross_sections(long_df: pl.DataFrame, num_symbols: int) -> pl.DataFrame:\n",
    "    valid_ts = (\n",
    "        long_df\n",
    "        .filter(pl.col(\"px\").is_not_null())\n",
    "        .group_by(\"timestamp\")\n",
    "        .agg(pl.len().alias(\"len\"))\n",
    "        .filter(pl.col(\"len\") == num_symbols)\n",
    "        .select(\"timestamp\")\n",
    "    )\n",
    "    return long_df.join(valid_ts, on=\"timestamp\", how=\"inner\")\n",
    "\n",
    "\n",
    "def build_long_cross_sections_fast(symbol_dfs: dict[str, pl.DataFrame]) -> pl.DataFrame:\n",
    "    # 1. 获取所有币种的所有时间戳，合并去重，得到全局时间轴 timeline\n",
    "    all_timestamps = (\n",
    "        pl.concat(\n",
    "            [df.select(\"timestamp\").unique() for df in symbol_dfs.values()]\n",
    "        )\n",
    "        .unique()\n",
    "        .sort(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    result_dfs = []\n",
    "\n",
    "    for symbol, df in symbol_dfs.items():\n",
    "        # 确保 df 按时间戳排序\n",
    "        df_sorted = df.sort(\"timestamp\")\n",
    "        # 2. 用 timeline 左连接（asof join）当前币种数据，找最近小于等于时间戳的行\n",
    "        joined = all_timestamps.join_asof(\n",
    "            df_sorted,\n",
    "            on=\"timestamp\",\n",
    "            strategy=\"backward\"  # 小于等于左表时间戳的最近一条右表数据\n",
    "        )\n",
    "        # 3. 补充 symbol 列（如果df本身有，确认无误）\n",
    "        # 如果原始 df 有 symbol，这里确认 symbol 是正确的\n",
    "        joined = joined.with_columns(pl.lit(symbol).alias(\"symbol\"))\n",
    "        result_dfs.append(joined)\n",
    "\n",
    "    long_df = pl.concat(result_dfs).sort([\"timestamp\", \"symbol\"])\n",
    "    clean_df = filter_valid_cross_sections(long_df, len(symbol_dfs))\n",
    "\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef06c1c-67e0-4fc0-8ba5-4e3c824ba0c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def process_all_symbols(params_dict):\n",
    "    symbol_dfs = {}\n",
    "    for sym, param in params_dict.items():\n",
    "        df = process_single_symbol(\n",
    "            symbol=sym,\n",
    "            threshold=param.get(\"threshold\", \"0.002\"),\n",
    "            feat_trans_window=param.get(\"feat_trans_window\", 350),\n",
    "            feat_norm_window=param.get(\"feat_norm_window\", 500),\n",
    "            feat_norm_rolling_mean_window=param.get(\"feat_norm_rolling_mean_window\", 10),\n",
    "        )\n",
    "        df = df.with_columns(pl.lit(sym).alias(\"symbol\"))\n",
    "        symbol_dfs[sym] = df\n",
    "        \n",
    "    return symbol_dfs\n",
    "\n",
    "# weekly_dataframes = split_df_by_week(a_df_filtered)\n",
    "# print(\"num weekly dfs:\", len(weekly_dataframes))\n",
    "\n",
    "feat_cal_window = 500\n",
    "feat_trans_window = 500\n",
    "feat_norm_window = 500\n",
    "feat_norm_rolling_mean_window = 500\n",
    "\n",
    "symbol_params = {\n",
    "    \"btcusdt\": {\n",
    "        \"threshold\": \"0.001\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"bnbusdt\": {\n",
    "        \"threshold\": \"0.001\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"ethusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"dogeusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"tonusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"filusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"ltcusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"avaxusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"atomusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"solusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"xrpusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"uniusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"adausdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    \"linkusdt\": {\n",
    "        \"threshold\": \"0.002\",\n",
    "        \"feat_cal_window\": str(feat_cal_window),\n",
    "        \"feat_trans_window\": feat_trans_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    },\n",
    "    # \"dotusdt\": {\n",
    "    #     \"threshold\": \"0.002\",\n",
    "    #     \"feat_cal_window\": str(feat_cal_window),\n",
    "    #     \"feat_trans_window\": feat_trans_window,\n",
    "    #     \"feat_norm_window\": feat_norm_window,\n",
    "    #     \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    # },\n",
    "    # \"sandusdt\": {\n",
    "    #     \"threshold\": \"0.002\",\n",
    "    #     \"feat_cal_window\": str(feat_cal_window),\n",
    "    #     \"feat_trans_window\": feat_trans_window,\n",
    "    #     \"feat_norm_window\": feat_norm_window,\n",
    "    #     \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    # },\n",
    "}\n",
    "\n",
    "symbol_dfs = process_all_symbols(symbol_params)\n",
    "long_df = build_long_cross_sections_fast(symbol_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e22158-709b-4d63-ac65-71df5523e970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del symbol_dfs  # 删除变量引用\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1449e8ed-1a20-4c99-a82d-5c41ee575220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9_556_652, 217)\n",
      "┌───────────────┬─────────┬────────────┬─────────────┬───┬─────────────────────────┬─────────────────────────┬─────────────────────────┬──────────┐\n",
      "│ timestamp     ┆ px      ┆ sum_buy_sz ┆ sum_sell_sz ┆ … ┆ z_factor_order_sentimen ┆ z_factor_oi_momentum_pu ┆ z_factor_oi_momentum_lo ┆ symbol   │\n",
      "│ ---           ┆ ---     ┆ ---        ┆ ---         ┆   ┆ t_diver…                ┆ nch_zsc…                ┆ ng_term…                ┆ ---      │\n",
      "│ i64           ┆ f64     ┆ f64        ┆ f64         ┆   ┆ ---                     ┆ ---                     ┆ ---                     ┆ str      │\n",
      "│               ┆         ┆            ┆             ┆   ┆ f64                     ┆ f64                     ┆ f64                     ┆          │\n",
      "╞═══════════════╪═════════╪════════════╪═════════════╪═══╪═════════════════════════╪═════════════════════════╪═════════════════════════╪══════════╡\n",
      "│ 1743984084668 ┆ 0.5732  ┆ 13.80055   ┆ 13.421752   ┆ … ┆ -0.304429               ┆ -0.304429               ┆ -0.304429               ┆ adausdt  │\n",
      "│ 1743984084668 ┆ 4.439   ┆ 10.386746  ┆ 9.22961     ┆ … ┆ 0.232122                ┆ 0.199704                ┆ -0.199706               ┆ atomusdt │\n",
      "│ 1743984084668 ┆ 16.034  ┆ 10.006134  ┆ 10.192644   ┆ … ┆ 0.304429                ┆ 0.304429                ┆ 0.304429                ┆ avaxusdt │\n",
      "│ 1743984084668 ┆ 556.56  ┆ 4.453766   ┆ 3.302113    ┆ … ┆ 0.02974                 ┆ -0.097446               ┆ 0.097446                ┆ bnbusdt  │\n",
      "│ 1743984084668 ┆ 78372.3 ┆ 5.279552   ┆ 4.994316    ┆ … ┆ -0.304429               ┆ -0.304429               ┆ -0.304429               ┆ btcusdt  │\n",
      "│ …             ┆ …       ┆ …          ┆ …           ┆ … ┆ …                       ┆ …                       ┆ …                       ┆ …        │\n",
      "│ 1754697449279 ┆ 123.8   ┆ 9.639119   ┆ 10.102513   ┆ … ┆ -0.24321                ┆ -0.08471                ┆ 0.027072                ┆ ltcusdt  │\n",
      "│ 1754697449279 ┆ 176.63  ┆ 10.110351  ┆ 11.232729   ┆ … ┆ -0.12028                ┆ 0.126396                ┆ 0.01314                 ┆ solusdt  │\n",
      "│ 1754697449279 ┆ 3.3389  ┆ 12.880822  ┆ 12.784127   ┆ … ┆ 0.149154                ┆ -0.043391               ┆ -0.167952               ┆ tonusdt  │\n",
      "│ 1754697449279 ┆ 10.867  ┆ 11.05008   ┆ 10.987138   ┆ … ┆ 0.424154                ┆ -0.101563               ┆ -0.094791               ┆ uniusdt  │\n",
      "│ 1754697449279 ┆ 3.284   ┆ 13.640195  ┆ 13.964859   ┆ … ┆ -0.396234               ┆ -0.345527               ┆ -0.189778               ┆ xrpusdt  │\n",
      "└───────────────┴─────────┴────────────┴─────────────┴───┴─────────────────────────┴─────────────────────────┴─────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8668cea-4fe9-49e6-b920-9454b875cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5646fc-030d-4247-a84c-3a8533b6fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4000\n",
    "target_col = f\"future_return_{N}\"\n",
    "exclude_prefixes = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34041273-d46c-4c93-bdbd-61550041cc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_symbol = LabelEncoder()\n",
    "\n",
    "all_symbols = set()\n",
    "all_symbols.update(long_df[\"symbol\"].unique())\n",
    "le_symbol.fit(sorted(list(all_symbols)))\n",
    "\n",
    "symbol_encoded = le_symbol.transform(long_df['symbol'].to_list())\n",
    "long_df = long_df.with_columns([pl.Series('enc_cat_symbol', symbol_encoded)])\n",
    "\n",
    "symbol_to_id = dict(zip(le_symbol.classes_, le_symbol.transform(le_symbol.classes_)))\n",
    "id_to_symbol = {v: k for k, v in symbol_to_id.items()}\n",
    "\n",
    "# 加 row_nr\n",
    "long_temp_df = long_df.with_row_index(name=\"row_nr\")\n",
    "del long_df\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92096bd1-07f7-42c5-b190-3cda78b94adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_future = (\n",
    "    long_temp_df.sort([\"symbol\", \"timestamp\"])\n",
    "    .group_by(\"symbol\", maintain_order=True)\n",
    "    .map_groups(lambda g: g.with_columns([\n",
    "        pl.col(\"px\").shift(-N).alias(\"px_future\"),\n",
    "        (pl.col(\"px\").shift(-N) / pl.col(\"px\")).log().alias(f\"future_return_{N}\")\n",
    "    ]))\n",
    "    .sort(\"row_nr\")\n",
    "    .drop(\"row_nr\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf44d5be-9c54-4995-87ab-ddea717c3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_feature_cols = [\n",
    "    col for col in df_with_future.columns\n",
    "    if (\n",
    "        (col.endswith(\"_cs\") or col.endswith(\"_zscaled\") or col == \"enc_cat_symbol\")\n",
    "        and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "        and not col.startswith(\"future_return_\")\n",
    "        and not col.startswith(\"px_future\")\n",
    "        and col != \"px\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91d836ad-87ff-48bf-a11b-c8b58ef9ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 long format 下，按 timestamp 做截面标准化 & 排序\n",
    "df_with_future = df_with_future.with_columns([\n",
    "    # 截面 z-score\n",
    "    ((pl.col(c) - pl.col(c).mean().over(\"timestamp\")) /\n",
    "     pl.col(c).std().over(\"timestamp\")).alias(f\"{c}_zscore_cs\")\n",
    "    for c in time_series_feature_cols\n",
    "] + [\n",
    "    # 截面 rank（归一化到 [0,1]）\n",
    "    (pl.col(c).rank(\"average\").over(\"timestamp\") /\n",
    "     pl.len().over(\"timestamp\")).alias(f\"{c}_rank_cs\")\n",
    "    for c in time_series_feature_cols\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2b74ec0-3f7f-45e8-aaeb-3e8bf97615bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 enc_cat_symbol 移到最后一列\n",
    "cols = [c for c in df_with_future.columns if c != \"enc_cat_symbol\"] + [\"enc_cat_symbol\"]\n",
    "df_with_future = df_with_future.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab268c7e-a80f-4261-8cf3-91a5183e3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    col for col in df_with_future.columns\n",
    "    if (\n",
    "        (col.endswith(\"_cs\") or col.endswith(\"_zscaled\") or col == \"enc_cat_symbol\")\n",
    "        and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "        and not col.startswith(\"future_return_\")\n",
    "        and not col.startswith(\"px_future\")\n",
    "        and col != \"px\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14cfe5df-9557-4c55-8178-72f3cc1ed570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n",
      "各列空值数量：\n",
      "px_future                                                56000\n",
      "future_return_4000                                       56000\n",
      "raw_factor_oi_change_sum_zscaled_zscore_cs                 868\n",
      "raw_factor_oi_change_sum_long_term_zscaled_zscore_cs       868\n",
      "raw_factor_short_term_oi_volatility_zscaled_zscore_cs       28\n",
      "raw_factor_long_term_oi_volatility_zscaled_zscore_cs       378\n",
      "raw_factor_short_term_oi_trend_zscaled_zscore_cs           196\n",
      "raw_factor_sentiment_net_zscaled_zscore_cs                 406\n",
      "z_factor_short_term_oi_volatility_zscaled_zscore_cs        322\n",
      "z_factor_long_term_oi_volatility_zscaled_zscore_cs          84\n",
      "z_factor_sentiment_net_zscaled_zscore_cs                    42\n",
      "log1p_sum_open_interest_zscaled_zscore_cs                  518\n",
      "oi_ls_term_diff_zscaled_zscore_cs                          868\n",
      "z_log1p_sum_open_interest_zscaled_zscore_cs                518\n",
      "z_oi_ls_term_diff_zscaled_zscore_cs                        868\n",
      "factor_oi_trend_slope_zscaled_zscore_cs                     70\n",
      "z_factor_oi_trend_slope_zscaled_zscore_cs                   70\n",
      "factor_momentum_trend_confirm_zscaled_zscore_cs             70\n",
      "z_factor_momentum_trend_confirm_zscaled_zscore_cs           70\n",
      "dtype: int64\n",
      "删除空值超过 500000 的列：[]\n",
      "删除列后，DataFrame形状：(9556652, 420)\n",
      "删除空值行后，DataFrame形状：(9499784, 420)\n"
     ]
    }
   ],
   "source": [
    "df_with_future = clean_df_drop_nulls(df_with_future)\n",
    "weekly_dataframes = split_df_by_week(df_with_future)  # 只拿 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de7260d9-f19d-40f9-838c-fdb2e780f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('adausdt'): np.int64(0), np.str_('atomusdt'): np.int64(1), np.str_('avaxusdt'): np.int64(2), np.str_('bnbusdt'): np.int64(3), np.str_('btcusdt'): np.int64(4), np.str_('dogeusdt'): np.int64(5), np.str_('ethusdt'): np.int64(6), np.str_('filusdt'): np.int64(7), np.str_('linkusdt'): np.int64(8), np.str_('ltcusdt'): np.int64(9), np.str_('solusdt'): np.int64(10), np.str_('tonusdt'): np.int64(11), np.str_('uniusdt'): np.int64(12), np.str_('xrpusdt'): np.int64(13)}\n",
      "{np.int64(0): np.str_('adausdt'), np.int64(1): np.str_('atomusdt'), np.int64(2): np.str_('avaxusdt'), np.int64(3): np.str_('bnbusdt'), np.int64(4): np.str_('btcusdt'), np.int64(5): np.str_('dogeusdt'), np.int64(6): np.str_('ethusdt'), np.int64(7): np.str_('filusdt'), np.int64(8): np.str_('linkusdt'), np.int64(9): np.str_('ltcusdt'), np.int64(10): np.str_('solusdt'), np.int64(11): np.str_('tonusdt'), np.int64(12): np.str_('uniusdt'), np.int64(13): np.str_('xrpusdt')}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_to_id) \n",
    "print(id_to_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b965055-74c5-470b-8ec8-427e294bd4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (257_208, 421)\n",
      "┌───────────────┬──────────┬────────────┬─────────────┬───┬──────────────────────────────┬────────────────────────┬────────────────┬──────────────┐\n",
      "│ timestamp     ┆ px       ┆ sum_buy_sz ┆ sum_sell_sz ┆ … ┆ z_factor_oi_momentum_long_te ┆ enc_cat_symbol_rank_cs ┆ enc_cat_symbol ┆ timestamp_dt │\n",
      "│ ---           ┆ ---      ┆ ---        ┆ ---         ┆   ┆ rm…                          ┆ ---                    ┆ ---            ┆ ---          │\n",
      "│ i64           ┆ f64      ┆ f64        ┆ f64         ┆   ┆ ---                          ┆ f64                    ┆ i64            ┆ datetime[ms] │\n",
      "│               ┆          ┆            ┆             ┆   ┆ f64                          ┆                        ┆                ┆              │\n",
      "╞═══════════════╪══════════╪════════════╪═════════════╪═══╪══════════════════════════════╪════════════════════════╪════════════════╪══════════════╡\n",
      "│ 1754265630049 ┆ 0.7267   ┆ 14.513676  ┆ 14.691346   ┆ … ┆ 0.714286                     ┆ 0.071429               ┆ 0              ┆ 2025-08-04   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 00:00:30.049 │\n",
      "│ 1754265630049 ┆ 4.267    ┆ 11.276557  ┆ 11.812582   ┆ … ┆ 0.785714                     ┆ 0.142857               ┆ 1              ┆ 2025-08-04   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 00:00:30.049 │\n",
      "│ 1754265630049 ┆ 21.362   ┆ 10.731034  ┆ 12.001677   ┆ … ┆ 0.071429                     ┆ 0.214286               ┆ 2              ┆ 2025-08-04   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 00:00:30.049 │\n",
      "│ 1754265630049 ┆ 751.27   ┆ 7.700779   ┆ 7.895194    ┆ … ┆ 0.285714                     ┆ 0.285714               ┆ 3              ┆ 2025-08-04   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 00:00:30.049 │\n",
      "│ 1754265630049 ┆ 114266.1 ┆ 4.463249   ┆ 1.909839    ┆ … ┆ 0.357143                     ┆ 0.357143               ┆ 4              ┆ 2025-08-04   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 00:00:30.049 │\n",
      "│ …             ┆ …        ┆ …          ┆ …           ┆ … ┆ …                            ┆ …                      ┆ …              ┆ …            │\n",
      "│ 1754616648557 ┆ 121.89   ┆ 6.648816   ┆ 7.11532     ┆ … ┆ 0.285714                     ┆ 0.714286               ┆ 9              ┆ 2025-08-08   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 01:30:48.557 │\n",
      "│ 1754616648557 ┆ 173.82   ┆ 10.641454  ┆ 11.288448   ┆ … ┆ 0.928571                     ┆ 0.785714               ┆ 10             ┆ 2025-08-08   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 01:30:48.557 │\n",
      "│ 1754616648557 ┆ 3.3263   ┆ 10.7214    ┆ 10.981421   ┆ … ┆ 0.428571                     ┆ 0.857143               ┆ 11             ┆ 2025-08-08   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 01:30:48.557 │\n",
      "│ 1754616648557 ┆ 10.394   ┆ 11.121541  ┆ 10.919606   ┆ … ┆ 0.714286                     ┆ 0.928571               ┆ 12             ┆ 2025-08-08   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 01:30:48.557 │\n",
      "│ 1754616648557 ┆ 3.3014   ┆ 12.950148  ┆ 12.784599   ┆ … ┆ 0.5                          ┆ 1.0                    ┆ 13             ┆ 2025-08-08   │\n",
      "│               ┆          ┆            ┆             ┆   ┆                              ┆                        ┆                ┆ 01:30:48.557 │\n",
      "└───────────────┴──────────┴────────────┴─────────────┴───┴──────────────────────────────┴────────────────────────┴────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(weekly_dataframes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78f05334-0391-4732-88f5-4746cdd74644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 [299] [14]\n"
     ]
    }
   ],
   "source": [
    "# 用第一个 df 定义 feature_cols\n",
    "sample_df = weekly_dataframes[0]\n",
    "\n",
    "cat_idxs = [feature_cols.index('enc_cat_symbol')]\n",
    "cat_dims = [sample_df.select('enc_cat_symbol').n_unique()]\n",
    "cat_emb_dim = 8\n",
    "print(len(feature_cols), cat_idxs, cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e0b316a-d96c-4b26-a919-f52ece1f0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cross_section(symbols, y_true, y_binary, y_pred_prob, px, alpha=1.0):\n",
    "    x = np.arange(len(symbols))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    width = 0.2\n",
    "\n",
    "    # 真实未来收益（连续）\n",
    "    ax1.bar(x - width, y_true, width=width, label='Future Return', alpha=0.6)\n",
    "    ax1.set_ylabel('Future Return')\n",
    "\n",
    "    # 价格线\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, px, label='Price', color='tab:blue', marker='o')\n",
    "    ax2.set_ylabel('Price')\n",
    "\n",
    "    # 分类标签（二分类）\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))\n",
    "    ax3.scatter(x, y_binary, label='GMM Label', color='tab:orange', marker='x')\n",
    "    ax3.set_ylim(-0.1, 1.1)\n",
    "    ax3.set_ylabel('Binary Label')\n",
    "\n",
    "    # 预测概率\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines.right.set_position((\"outward\", 120))\n",
    "    ax4.plot(x, y_pred_prob, label='Predicted Prob', color='tab:green', marker='^')\n",
    "    ax4.set_ylim(-0.05, 1.05)\n",
    "    ax4.set_ylabel('Predicted Probability')\n",
    "\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(symbols, rotation=45)\n",
    "    ax1.set_xlabel('Symbols')\n",
    "\n",
    "    # 合并图例\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "    lines_4, labels_4 = ax4.get_legend_handles_labels()\n",
    "\n",
    "    ax1.legend(\n",
    "        lines_1 + lines_2 + lines_3 + lines_4,\n",
    "        labels_1 + labels_2 + labels_3 + labels_4,\n",
    "        loc='upper left'\n",
    "    )\n",
    "\n",
    "    plt.title(\"Cross-Section Comparison at One Timestamp\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe8623-9034-4078-9ae8-92758b1cbd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Fold 0: Train 0~9, Val 10, Test 11\n",
      "Train: 2025-04-07 00:05:01.741000 to 2025-06-15 23:57:10.054000\n",
      "Val: 2025-06-16 00:00:41.474000 to 2025-06-22 23:59:51.006000\n",
      "Test: 2025-06-23 00:00:29.647000 to 2025-06-29 23:59:01.040000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.00763 | val_0_mae: 0.02828 |  0:02:18s\n",
      "epoch 2  | loss: 0.00051 | val_0_mae: 0.03188 |  0:06:33s\n",
      "epoch 4  | loss: 0.00018 | val_0_mae: 0.03182 |  0:10:57s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mae = 0.02828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343112, 1) (343112, 1) (343112,) (343112, 1)\n",
      "MSE: 0.001312\n",
      "MAE: 0.027154\n",
      "============================================================\n",
      "Fold 1: Train 1~10, Val 11, Test 12\n",
      "Train: 2025-04-14 00:00:23.127000 to 2025-06-22 23:59:51.006000\n",
      "Val: 2025-06-23 00:00:29.647000 to 2025-06-29 23:59:01.040000\n",
      "Test: 2025-06-30 00:00:07.640000 to 2025-07-06 23:56:03.190000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.00841 | val_0_mae: 0.02699 |  0:01:54s\n",
      "epoch 2  | loss: 0.00053 | val_0_mae: 0.03256 |  0:05:32s\n",
      "epoch 4  | loss: 0.00021 | val_0_mae: 0.03237 |  0:09:17s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mae = 0.02699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(268002, 1) (268002, 1) (268002,) (268002, 1)\n",
      "MSE: 0.001804\n",
      "MAE: 0.033275\n",
      "============================================================\n",
      "Fold 2: Train 2~11, Val 12, Test 13\n",
      "Train: 2025-04-21 00:04:34.816000 to 2025-06-29 23:59:01.040000\n",
      "Val: 2025-06-30 00:00:07.640000 to 2025-07-06 23:56:03.190000\n",
      "Test: 2025-07-07 00:00:19.227000 to 2025-07-13 23:59:02.794000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0089  | val_0_mae: 0.03381 |  0:01:52s\n",
      "epoch 2  | loss: 0.00053 | val_0_mae: 0.0363  |  0:05:27s\n",
      "epoch 4  | loss: 0.00021 | val_0_mae: 0.0357  |  0:09:06s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mae = 0.03381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406896, 1) (406896, 1) (406896,) (406896, 1)\n",
      "MSE: 0.001866\n",
      "MAE: 0.033440\n",
      "============================================================\n",
      "Fold 3: Train 3~12, Val 13, Test 14\n",
      "Train: 2025-04-28 00:00:14.614000 to 2025-07-06 23:56:03.190000\n",
      "Val: 2025-07-07 00:00:19.227000 to 2025-07-13 23:59:02.794000\n",
      "Test: 2025-07-14 00:00:11.246000 to 2025-07-20 23:59:17.557000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grayman\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "n_train_weeks = 10 # 可配置\n",
    "n_val_weeks = 1    # 一般 1 周验证\n",
    "n_test_weeks = 1   # 后 1 周做 test\n",
    "\n",
    "tabnet = None\n",
    "\n",
    "all_preds = []  # 放到 for 循环外\n",
    "\n",
    "for i in range(len(weekly_dataframes) - n_train_weeks - n_val_weeks - n_test_weeks + 1):\n",
    "    train_dfs = weekly_dataframes[i : i + n_train_weeks]\n",
    "    val_dfs = weekly_dataframes[i + n_train_weeks : i + n_train_weeks + n_val_weeks]\n",
    "    test_dfs = weekly_dataframes[i + n_train_weeks + n_val_weeks : i + n_train_weeks + n_val_weeks + n_test_weeks]\n",
    "\n",
    "    train_df = pl.concat(train_dfs)\n",
    "    val_df = pl.concat(val_dfs)\n",
    "    test_df = pl.concat(test_dfs)\n",
    "    \n",
    "    def process_df_np(df):\n",
    "        df = df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px'])\n",
    "        X = df.select(feature_cols).to_numpy()  # Polars DataFrame 转 numpy ndarray\n",
    "        y = df.select(target_col).to_numpy().reshape(-1, 1)\n",
    "        px = df.select('px').to_numpy()\n",
    "        ts = df.select('timestamp').to_numpy()\n",
    "        symbol_enc = df.select(\"enc_cat_symbol\")\n",
    "        return X, y, px, ts, symbol_enc\n",
    "\n",
    "    X_train, y_train, px_train, ts_train, sb_train = process_df_np(train_df)\n",
    "    X_val, y_val, px_val, ts_val, sb_val = process_df_np(val_df)\n",
    "    X_test, y_test, px_test, ts_test, sb_test = process_df_np(test_df)\n",
    "\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Fold {i}: Train {i}~{i+n_train_weeks-1}, Val {i+n_train_weeks}, Test {i+n_train_weeks+1}\")\n",
    "    print(\"Train:\", train_df['timestamp_dt'][0], \"to\", train_df['timestamp_dt'][-1])\n",
    "    print(\"Val:\", val_df['timestamp_dt'][0], \"to\", val_df['timestamp_dt'][-1])\n",
    "    print(\"Test:\", test_df['timestamp_dt'][0], \"to\", test_df['timestamp_dt'][-1])\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        # 模型结构参数\n",
    "        \"n_d\": 16,                      # 决策输出维度\n",
    "        \"n_a\": 16,                      # 注意力机制维度\n",
    "        \"n_steps\": 3,                  # 决策步数\n",
    "        \"gamma\": 1.3,                  # 控制特征复用的程度（>1）\n",
    "        \"n_independent\": 2,           # 每个 step 的独立 Feature Transformer 层数\n",
    "        \"n_shared\": 1,                # 每个 step 的共享 Feature Transformer 层数\n",
    "    \n",
    "        # 分类特征嵌入（如果你用的都是 float 特征，可以全留空）\n",
    "        \"cat_idxs\": cat_idxs,               # 类别特征的列索引\n",
    "        \"cat_dims\": cat_dims,               # 每个类别特征的类别数\n",
    "        \"cat_emb_dim\": cat_emb_dim,             # 类别特征的嵌入维度（或 list）\n",
    "    \n",
    "        # 正则化与数值稳定性\n",
    "        \"lambda_sparse\": 1e-5,        # 稀疏正则\n",
    "        \"epsilon\": 1e-15,             # sparsemax 稳定项\n",
    "        \"momentum\": 0.03,             # BatchNorm 的动量\n",
    "        \"clip_value\": 3.0,            # 梯度裁剪\n",
    "        \n",
    "        # 注意力 mask 类型\n",
    "        \"mask_type\": \"sparsemax\",     # sparsemax 或 entmax\n",
    "    \n",
    "        # 优化器设置（函数和参数）\n",
    "        # \"optimizer_fn\": torch.optim.Adam,    \n",
    "        \"optimizer_params\": {\"lr\": 5e-3},\n",
    "    \n",
    "        # 学习率调度器（可选）\n",
    "        \"scheduler_fn\": None,         # torch.optim.lr_scheduler.StepLR 等\n",
    "        \"scheduler_params\": {},       # 比如 {\"step_size\": 20, \"gamma\": 0.95}\n",
    "    \n",
    "        # 预训练解码器结构（一般用不到）\n",
    "        \"n_shared_decoder\": 1,\n",
    "        \"n_indep_decoder\": 1,\n",
    "    \n",
    "        # 训练环境和调试\n",
    "        \"seed\": 7,\n",
    "        \"verbose\": 2,\n",
    "        \"device_name\": \"cuda\",        # auto / cpu / cuda\n",
    "    }\n",
    "\n",
    "    init_fit_params = {\n",
    "        \"eval_metric\": ['mae'],\n",
    "        \"max_epochs\": 20,\n",
    "        \"patience\": 5,\n",
    "        \"batch_size\": 2048,\n",
    "        \"virtual_batch_size\": 512,\n",
    "        \"compute_importance\": False,\n",
    "    }\n",
    "\n",
    "    inc_fit_params = {\n",
    "        \"eval_metric\": ['mae'],\n",
    "        \"max_epochs\": 5,\n",
    "        \"patience\": 50,\n",
    "        \"batch_size\": 2048,\n",
    "        \"virtual_batch_size\": 512,\n",
    "        \"compute_importance\": False,\n",
    "        \"warm_start\": True,\n",
    "    }\n",
    "\n",
    "    \n",
    "    # if i == 0:\n",
    "    tabnet = TabNetRegressor(**params )\n",
    "    tabnet.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        **init_fit_params,\n",
    "    )\n",
    "    # else:\n",
    "    #     tabnet.fit(\n",
    "    #         X_train=X_train,\n",
    "    #         y_train=y_train,\n",
    "    #         eval_set=[(X_val, y_val)],\n",
    "    #         **inc_fit_params,\n",
    "    #     )\n",
    "\n",
    "    y_pred = tabnet.predict(X_test).squeeze()\n",
    "    print(ts_test.shape, y_test.shape, y_pred.shape, px_test.shape)\n",
    "\n",
    "    print(f\"MSE: {mean_squared_error(y_test, y_pred):.6f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.6f}\")\n",
    "    current_window_results = {\n",
    "        'timestamp': ts_test,\n",
    "        'symbol_enc': sb_test, # 收集价格，回测时需要\n",
    "        'true_label': y_test,\n",
    "        'predicted_prob': y_pred,\n",
    "        'px': px_test, # 收集价格，回测时需要\n",
    "    }\n",
    "    \n",
    "    all_preds.append(current_window_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e498f-f9a6-4026-bc78-4321cf9b7eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74ec55-a5a5-4ac8-b278-0240c8c5accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"all_preds length: {len(all_preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098d5f8-9d85-403d-a94c-9952fc74e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cross_section_comparison(symbols, true_labels, pred_probs, prices, std_array=None, alpha=1):\n",
    "    \"\"\"\n",
    "    symbols: list/array of symbol names或编码（横轴）\n",
    "    true_labels: array，对应每个币种的真实标签\n",
    "    pred_probs: array，对应每个币种的预测概率\n",
    "    prices: array，对应每个币种的价格\n",
    "    std_array: array，可选，价格的波动区间\n",
    "    alpha: 标准差放大倍数\n",
    "    \"\"\"\n",
    "    x = np.arange(len(symbols))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "    # 真实标签（可以用点图）\n",
    "    ax1.scatter(x, true_labels, label=\"True Label\", color='tab:blue', marker='o', s=50, alpha=0.7)\n",
    "    ax1.set_ylabel(\"True Label\", color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(min(true_labels)*1.1, max(true_labels)*1.1)\n",
    "\n",
    "    # 预测概率\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, pred_probs, label=\"Predicted Probability\", color='tab:green', marker='x', linestyle='-', alpha=0.7)\n",
    "    ax2.set_ylabel(\"Predicted Probability\", color='tab:green')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # 价格\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))\n",
    "    ax3.plot(x, prices, label=\"Price\", color='tab:red', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # 价格区间带\n",
    "    if std_array is not None:\n",
    "        ax3.fill_between(x, prices - alpha * std_array, prices + alpha * std_array,\n",
    "                         color='tab:red', alpha=0.15, label=\"Price ± std\")\n",
    "\n",
    "    ax3.set_ylabel(\"Price\", color='tab:red')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # 横轴是币种名\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(symbols, rotation=45, ha='right')\n",
    "\n",
    "    plt.title(\"Cross-Sectional Comparison: True Label, Prediction & Price\")\n",
    "    # 合并图例\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2 + lines_3, labels_1 + labels_2 + labels_3, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37709e65-e595-40a2-855a-93840255faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "for i, result in enumerate(all_preds):\n",
    "    try:\n",
    "        # 先处理 symbol_enc 转成 numpy array\n",
    "        # 假设 result['symbol_enc'] 是 Polars DataFrame，列名是 'enc_cat_symbol'\n",
    "        if hasattr(result['symbol_enc'], \"to_pandas\"):\n",
    "            symbol_enc_array = result['symbol_enc'].to_pandas()['enc_cat_symbol'].values\n",
    "        else:\n",
    "            # 如果已经是 np.ndarray 或 list\n",
    "            symbol_enc_array = result['symbol_enc']\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': result['timestamp'].squeeze(),  # (N,)\n",
    "            'symbol_enc': symbol_enc_array.squeeze(),    # (N,)\n",
    "            'true_label': result['true_label'].squeeze(),\n",
    "            'predicted_prob': result['predicted_prob'].squeeze(),\n",
    "            'px': result['px'].squeeze()\n",
    "        })\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Warning: Empty dataframe at index {i}\")\n",
    "            continue\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "\n",
    "full_df = pd.concat(df_list, ignore_index=True)\n",
    "print(full_df.shape)\n",
    "print(full_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede02bb-ee15-42ab-9d4a-77d449fac495",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f7817-6ddc-4f7c-9b5a-333820f897a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_analysis(factor_name, weekly_dataframes, target_col, num_bins=5):\n",
    "    bin_returns = [0.0] * num_bins\n",
    "    bin_counts = [0] * num_bins\n",
    "\n",
    "    for df in weekly_dataframes:\n",
    "        if factor_name not in df.columns or target_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        sub_df = df.select([factor_name, target_col]).drop_nulls().to_pandas()\n",
    "        if len(sub_df) < num_bins:\n",
    "            continue\n",
    "\n",
    "        sub_df[\"bin\"] = pd.qcut(sub_df[factor_name], q=num_bins, labels=False, duplicates=\"drop\")\n",
    "        for i in range(num_bins):\n",
    "            group = sub_df[sub_df[\"bin\"] == i]\n",
    "            if not group.empty:\n",
    "                bin_returns[i] += group[target_col].mean()\n",
    "                bin_counts[i] += 1\n",
    "\n",
    "    avg_returns = [r / c if c > 0 else 0 for r, c in zip(bin_returns, bin_counts)]\n",
    "    return avg_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc0c0b-efbd-43d7-a5db-52d4c4a63f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def calc_ic_per_factor(weekly_dataframes, feature_cols, target_col):\n",
    "    ic_records = []\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        ic_list = []\n",
    "        for df in weekly_dataframes:\n",
    "            if feature not in df.columns or target_col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            sub_df = df.select([feature, target_col]).drop_nulls().to_pandas()\n",
    "            if len(sub_df) < 5:\n",
    "                continue\n",
    "\n",
    "            rank_ic, _ = spearmanr(sub_df[feature], sub_df[target_col])\n",
    "            if pd.notna(rank_ic):\n",
    "                ic_list.append(rank_ic)\n",
    "\n",
    "        if ic_list:\n",
    "            ic_mean = sum(ic_list) / len(ic_list)\n",
    "            ic_std = pd.Series(ic_list).std()\n",
    "            ic_ir = ic_mean / ic_std if ic_std > 1e-6 else 0\n",
    "            ic_records.append({\n",
    "                'factor': feature,\n",
    "                'IC Mean': ic_mean,\n",
    "                'IC Std': ic_std,\n",
    "                'IC IR': ic_ir\n",
    "            })\n",
    "\n",
    "    ic_df = pd.DataFrame(ic_records).sort_values(by='IC IR', ascending=False)\n",
    "    return ic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480d2ef-65e6-4bd4-8151-ce670f895d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = calc_ic_per_factor(weekly_dataframes, feature_cols, target_col)\n",
    "print(\"Top10 IC 因子：\")\n",
    "print(ic_df.head(10))\n",
    "\n",
    "top_factors = ic_df.head(10)['factor'].tolist()\n",
    "for factor in top_factors:\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}, 分桶收益: {returns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f17a7d-3c87-47f9-99a6-728d2f652822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in ic_df.iterrows():\n",
    "    factor = row['factor']\n",
    "    ic = row['IC Mean']\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}, IC: {ic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35de0cb-13fc-43be-b1a2-2d9f12dc9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ic_df)\n",
    "tail_factors = ic_df.tail(10)['factor'].tolist()\n",
    "for factor in tail_factors:\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}, 分桶收益: {returns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05a3ad-e8b0-4490-bf87-beb402bdddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, factor in enumerate(top_factors):\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    axes[i].bar(range(1, 1 + len(returns)), returns)\n",
    "    axes[i].set_title(f\"Factor: {factor}\")\n",
    "    axes[i].set_xlabel(\"Bin\")\n",
    "    axes[i].set_ylabel(\"Mean Return\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ddd381-8e0f-4287-a4f6-8db2e3ed7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, factor in enumerate(tail_factors):\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    axes[i].bar(range(1, 1 + len(returns)), returns)\n",
    "    axes[i].set_title(f\"Factor: {factor}\")\n",
    "    axes[i].set_xlabel(\"Bin\")\n",
    "    axes[i].set_ylabel(\"Mean Return\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1c350-e67c-4f47-a1b4-948110c33827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"总样本数：\", len(full_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e2dee-fc93-4173-9b41-553d46cc9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你已有 df，包含 symbol_enc、timestamp、true_label、predicted_prob、px、position（自行添加）\n",
    "# 转换 timestamp 为时间格式（如需要）\n",
    "df = full_df\n",
    "df[\"timestamp\"] = pd.to_datetime(full_df[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "# 遍历每个币种\n",
    "symbols = df[\"symbol_enc\"].unique()\n",
    "fig, axs = plt.subplots(len(symbols), 1, figsize=(14, 3 * len(symbols)), sharex=True)\n",
    "\n",
    "if len(symbols) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for i, sym in enumerate(symbols):\n",
    "    sym_df = df[df[\"symbol_enc\"] == sym].copy()\n",
    "    \n",
    "    ax = axs[i]\n",
    "    sym_str = id_to_symbol[int(sym)]\n",
    "    \n",
    "    ax.set_title(f\"{sym_str}\")\n",
    "    \n",
    "    # 主轴: 价格\n",
    "    ax.plot(sym_df[\"timestamp\"], sym_df[\"px\"], label=\"Price\", color=\"black\")\n",
    "    ax.set_ylabel(\"Price\", color=\"black\")\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    # 第二轴: label\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(sym_df[\"timestamp\"], sym_df[\"true_label\"], label=\"Label\", color=\"blue\", alpha=0.6)\n",
    "    ax2.set_ylabel(\"Label\", color=\"blue\")\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # 第三轴: predicted prob\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines.right.set_position((\"axes\", 1.1))  # 偏移右边\n",
    "    ax3.plot(sym_df[\"timestamp\"], sym_df[\"predicted_prob\"], label=\"Pred Prob\", color=\"orange\", alpha=0.6)\n",
    "    ax3.set_ylabel(\"Predicted\", color=\"orange\")\n",
    "    ax3.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "    # 第四轴: position（如果你有这个字段）\n",
    "    if \"position\" in sym_df.columns:\n",
    "        ax4 = ax.twinx()\n",
    "        ax4.spines.right.set_position((\"axes\", 1.2))  # 更偏移右边\n",
    "        ax4.plot(sym_df[\"timestamp\"], sym_df[\"position\"], label=\"Position\", color=\"green\", alpha=0.6)\n",
    "        ax4.set_ylabel(\"Position\", color=\"green\")\n",
    "        ax4.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433617ae-5a7f-4a79-8f3b-5ed533b84ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "ic_list = []\n",
    "time_list = []\n",
    "\n",
    "# 计算每个时间截面的 IC\n",
    "for ts, group in tqdm(full_df.groupby('timestamp'), total=full_df['timestamp'].nunique(), desc=\"Calculating IC\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    if group['predicted_prob'].nunique() < 2 or group['true_label'].nunique() < 2:\n",
    "        continue\n",
    "    ic, _ = spearmanr(group['predicted_prob'], group['true_label'])\n",
    "    ic_list.append(ic)\n",
    "    time_list.append(ts)\n",
    "\n",
    "# 转为 np.array 和 datetime 格式（如需要）\n",
    "ic_array = np.array(ic_list)\n",
    "time_array = np.array(time_list)\n",
    "\n",
    "# 排序时间（可选，保险做法）\n",
    "sorted_idx = np.argsort(time_array)\n",
    "time_array = time_array[sorted_idx]\n",
    "ic_array = ic_array[sorted_idx]\n",
    "\n",
    "# 计算累计 IC（cum IC）\n",
    "cum_ic = np.cumsum(ic_array)\n",
    "\n",
    "# 计算 IR（信息比率 = 平均IC / IC标准差）\n",
    "ir = np.mean(ic_array) / np.std(ic_array)\n",
    "\n",
    "# 打印信息\n",
    "print(f\"平均IC: {np.mean(ic_array):.4f}\")\n",
    "print(f\"平均IC std: {np.std(ic_array):.4f}\")\n",
    "\n",
    "print(f\"信息比率 IR: {ir:.4f}\")\n",
    "\n",
    "# 每隔500点采样一次\n",
    "step = 500\n",
    "time_array_sampled = time_array[::step]\n",
    "ic_array_sampled = ic_array[::step]\n",
    "cum_ic_sampled = cum_ic[::step]\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 子图1: IC 时间序列图（采样后）\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time_array_sampled, ic_array_sampled, marker='o', label='IC')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Information Coefficient (IC) over Time (sampled every 500 points)\")\n",
    "plt.ylabel(\"IC\")\n",
    "plt.legend()\n",
    "\n",
    "# 子图2: 累积 IC 图（采样后）\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time_array_sampled, cum_ic_sampled, color='orange', label='Cumulative IC')\n",
    "plt.title(f\"Cumulative IC (IR={ir:.4f})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative IC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfe395-3407-4797-ade7-fde243bfa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # 用于显示进度条\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp': 原始时间戳（例如毫秒），唯一且递增\n",
    "# - 'symbol_enc': 币种编码\n",
    "# - 'px': 当期价格\n",
    "# - 'predicted_label': 模型预测的标签（分类或排名），越大表示越好\n",
    "\n",
    "# --- 配置参数 ---\n",
    "N_INTERVAL = N # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "\n",
    "# N_INTERVAL = 1000 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "\n",
    "# --- 数据预处理 ---\n",
    "# 1. 转换时间戳为 datetime 类型，方便处理和绘图\n",
    "bt_df = full_df\n",
    "bt_df['dt'] = pd.to_datetime(full_df['timestamp'], unit='ms')\n",
    "\n",
    "# 2. 确保数据按时间戳和币种排序，这是后续分组和shift操作的基础\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "\n",
    "# 3. 获取所有唯一的、排序后的时间戳列表\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "\n",
    "# 4. 确定所有调仓时间点\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）---\n",
    "# 记录每个调仓时间点应持有的多空币种\n",
    "rebalance_signals = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Identifying Rebalance Signals\"):\n",
    "    # 筛选出当前调仓时间点 t 的所有币种数据（截面数据）\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t]\n",
    "    if current_snapshot.empty:\n",
    "        continue\n",
    "\n",
    "    # 找出 predicted_label 最高（最好）和最低（最差）的币种\n",
    "    # 使用 .idxmax() 和 .idxmin() 找到索引，再用 .loc[] 提取 symbol_enc\n",
    "    long_symbol = current_snapshot.loc[current_snapshot['predicted_prob'].idxmax(), 'symbol_enc']\n",
    "    short_symbol = current_snapshot.loc[current_snapshot['predicted_prob'].idxmin(), 'symbol_enc']\n",
    "\n",
    "    rebalance_signals[t] = {'long': long_symbol, 'short': short_symbol}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 ---\n",
    "# 存储每个调仓周期的策略收益\n",
    "period_returns = []\n",
    "\n",
    "# 初始化当前持仓，确保从第一个调仓点开始生效\n",
    "current_long_symbol = None\n",
    "current_short_symbol = None\n",
    "\n",
    "# 遍历每个调仓周期\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    # 确定当前调仓周期结束时间\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    # 从 rebalance_signals 获取当前周期的多空币种\n",
    "    if start_time in rebalance_signals:\n",
    "        current_long_symbol = rebalance_signals[start_time]['long']\n",
    "        current_short_symbol = rebalance_signals[start_time]['short']\n",
    "    else:\n",
    "        # 如果当前调仓点没有信号（不应发生），则沿用上一个周期的头寸或保持空仓\n",
    "        # 这里为了简化，假设如果有信号就会找到，没有则保持上一个有效头寸\n",
    "        # 如果需要严格空仓，可以在这里设置 current_long_symbol = None, current_short_symbol = None\n",
    "        pass\n",
    "\n",
    "    # 如果没有有效头寸，跳过此周期（收益为0）\n",
    "    if current_long_symbol is None or current_short_symbol is None:\n",
    "        period_returns.append({'dt': start_time, 'strategy_log_return': 0.0})\n",
    "        continue\n",
    "\n",
    "    # 筛选出当前调仓周期内的所有数据\n",
    "    # 包括起始时间点（进行调仓），但不包括结束时间点（结算）\n",
    "    period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "\n",
    "    # 获取多头和空头币种在该周期开始和结束时的价格\n",
    "    long_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == current_long_symbol)]['px'].iloc[0]\n",
    "    long_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == current_long_symbol)]['px'].iloc[0]\n",
    "\n",
    "    short_start_px = period_data[(period_data['dt'] == start_time) & (period_data['dt'] <= end_time) & (period_data['symbol_enc'] == current_short_symbol)]['px'].iloc[0]\n",
    "    short_end_px = period_data[(period_data['dt'] == end_time) & (period_data['dt'] <= end_time) & (period_data['symbol_enc'] == current_short_symbol)]['px'].iloc[0]\n",
    "\n",
    "    # 计算多头和空头在该周期内的对数收益率\n",
    "    long_log_ret = np.log(long_end_px) - np.log(long_start_px)\n",
    "    short_log_ret = np.log(short_end_px) - np.log(short_start_px)\n",
    "\n",
    "    # 计算策略在该周期内的总对数收益（多头收益 + 空头收益的绝对值）\n",
    "    # 假设各持仓权重相等，所以是 (多头收益 - 空头收益) / 2\n",
    "    # 或者说，做多一个，做空一个，组合总收益\n",
    "    strategy_period_log_return = (long_log_ret - short_log_ret) / 2 # 平均对冲策略\n",
    "\n",
    "    period_returns.append({\n",
    "        'dt': start_time,\n",
    "        'strategy_log_return': strategy_period_log_return\n",
    "    })\n",
    "\n",
    "# 将周期收益转换为 DataFrame\n",
    "strategy_returns_df = pd.DataFrame(period_returns).set_index('dt')\n",
    "strategy_returns_series = strategy_returns_df['strategy_log_return']\n",
    "\n",
    "# --- 绩效指标函数 ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    \"\"\"\n",
    "    计算并返回策略的绩效统计数据。\n",
    "    return_series: 每个周期的对数收益率序列。\n",
    "    periods_per_year: 一年内有多少个这样的周期（用于年化）。\n",
    "    \"\"\"\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp) # 对数收益累加后转回普通收益\n",
    "    total_return = cum_ret.iloc[-1] - 1 # 累计普通收益\n",
    "\n",
    "    # 年化收益率 (几何平均)\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year) # 年化波动率\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    # 最大回撤 (基于普通收益)\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 ---\n",
    "# 假设每个时间戳是10分钟，N_INTERVAL=1000，则一个周期是 1000 * 10分钟 = 10000 分钟\n",
    "# 一年大约有 52560 个10分钟的间隔 (365天 * 24小时 * 6个10分钟/小时)\n",
    "# 则一年大约有 52560 / 1000 = 52.56 个 N_INTERVAL 周期\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) # 10分钟一档\n",
    "# 或者更直接： (总时间戳数 / N_INTERVAL) / 总年数\n",
    "\n",
    "stats = perf_stats(strategy_returns_series, periods_per_year_for_annualization)\n",
    "print(\"\\n--- Strategy Performance Statistics ---\")\n",
    "print(pd.Series(stats))\n",
    "\n",
    "# --- 绘制累计收益曲线 ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "# 对数收益累加，然后取指数，得到累计普通收益曲线\n",
    "strategy_returns_series.cumsum().apply(np.exp).plot()\n",
    "plt.title(f\"Long-Short Strategy Cumulative Return (Rebalance every {N_INTERVAL} bars)\")\n",
    "plt.xlabel(\"Rebalance Timestamp\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b8da1-39d5-494b-b3c4-13eaadf19d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp'（时间戳，可转为 datetime）\n",
    "# - 'symbol_enc'（币种编码）\n",
    "# - 'px'（当期价格）\n",
    "# - 'predicted_label'（模型预测的 rank_label，0=最差、3=最好，或你自己定义的分类）\n",
    "\n",
    "# --- 配置参数 ---\n",
    "\n",
    "N_INTERVAL = N # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "# N_INTERVAL = 1000 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "N_LONG = 2         # 做多的品种数量（例如，前2个最好的）\n",
    "N_SHORT = 2       # 做空的品种数量（例如，后2个最差的）\n",
    "\n",
    "# --- 数据预处理 ---\n",
    "# 1. 转换时间戳为 datetime 类型，方便处理和绘图\n",
    "bt_df = full_df\n",
    "bt_df['dt'] = pd.to_datetime(bt_df['timestamp'], unit='ms')\n",
    "\n",
    "# 2. 确保数据按时间戳和币种排序，这是后续分组和shift操作的基础\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "\n",
    "# 3. 获取所有唯一的、排序后的时间戳列表\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "\n",
    "# 4. 确定所有调仓时间点\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）---\n",
    "# 记录每个调仓时间点应持有的多空币种列表\n",
    "rebalance_signals = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Identifying Rebalance Signals\"):\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t].copy() # 使用.copy()避免SettingWithCopyWarning\n",
    "    if current_snapshot.empty:\n",
    "        continue\n",
    "\n",
    "    # 根据 predicted_label 排序，选择前N_LONG和后N_SHORT个品种\n",
    "    # 降序排列找到最好的，升序排列找到最差的\n",
    "    sorted_by_prediction = current_snapshot.sort_values(by='predicted_prob', ascending=False)\n",
    "\n",
    "    # 选择前 N_LONG 个最好的品种做多\n",
    "    long_symbols = sorted_by_prediction.head(N_LONG)['symbol_enc'].tolist()\n",
    "    \n",
    "    # 选择后 N_SHORT 个最差的品种做空\n",
    "    # 注意：如果 N_LONG + N_SHORT > 总品种数，需要避免重复\n",
    "    # 确保做空品种与做多品种不重叠\n",
    "    short_symbols_potential = sorted_by_prediction.tail(N_SHORT)['symbol_enc'].tolist()\n",
    "    short_symbols = [s for s in short_symbols_potential if s not in long_symbols]\n",
    "\n",
    "    rebalance_signals[t] = {'long': long_symbols, 'short': short_symbols}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 ---\n",
    "# 存储每个调仓周期的策略收益\n",
    "period_returns = []\n",
    "\n",
    "# 初始化当前持仓，确保从第一个调仓点开始生效\n",
    "current_long_symbols = []\n",
    "current_short_symbols = []\n",
    "\n",
    "# 遍历每个调仓周期\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    # 获取当前周期的多空币种列表\n",
    "    if start_time in rebalance_signals:\n",
    "        current_long_symbols = rebalance_signals[start_time]['long']\n",
    "        current_short_symbols = rebalance_signals[start_time]['short']\n",
    "    else:\n",
    "        # 如果当前调仓点没有信号（不应发生），则沿用上一个周期的头寸或保持空仓\n",
    "        pass # 维持前一周期头寸\n",
    "\n",
    "    # 如果没有有效头寸（或数量不够），跳过此周期（收益为0）\n",
    "    if not current_long_symbols or not current_short_symbols:\n",
    "        period_returns.append({'dt': start_time, 'strategy_log_return': 0.0})\n",
    "        continue\n",
    "\n",
    "    # 筛选出当前调仓周期内的所有数据\n",
    "    period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "\n",
    "    total_period_log_return = 0.0\n",
    "\n",
    "    # 计算多头组合收益\n",
    "    long_portfolio_log_return = 0.0\n",
    "    for symbol in current_long_symbols:\n",
    "        try:\n",
    "            long_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "            long_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "            long_portfolio_log_return += (np.log(long_end_px) - np.log(long_start_px))\n",
    "        except IndexError:\n",
    "            # 币种数据缺失处理\n",
    "            # print(f\"Warning: Data for {symbol} missing at {start_time} or {end_time}\")\n",
    "            pass # 或者采取其他更严格的缺失处理方式\n",
    "\n",
    "    # 计算空头组合收益\n",
    "    short_portfolio_log_return = 0.0\n",
    "    for symbol in current_short_symbols:\n",
    "        try:\n",
    "            short_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "            short_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "            short_portfolio_log_return += (np.log(short_end_px) - np.log(short_start_px))\n",
    "        except IndexError:\n",
    "            # 币种数据缺失处理\n",
    "            # print(f\"Warning: Data for {symbol} missing at {start_time} or {end_time}\")\n",
    "            pass # 或者采取其他更严格的缺失处理方式\n",
    "\n",
    "    # 平均权重计算组合收益：(多头组合收益 - 空头组合收益) / 总持仓品种数\n",
    "    # 假设每个选中的品种权重相等\n",
    "    num_traded_symbols = len(current_long_symbols) + len(current_short_symbols)\n",
    "    if num_traded_symbols > 0:\n",
    "        total_period_log_return = (long_portfolio_log_return - short_portfolio_log_return) / num_traded_symbols\n",
    "    else:\n",
    "        total_period_log_return = 0.0 # 没有有效交易则收益为0\n",
    "\n",
    "    period_returns.append({\n",
    "        'dt': start_time,\n",
    "        'strategy_log_return': total_period_log_return\n",
    "    })\n",
    "\n",
    "# 将周期收益转换为 DataFrame\n",
    "strategy_returns_df = pd.DataFrame(period_returns).set_index('dt')\n",
    "strategy_returns_series = strategy_returns_df['strategy_log_return']\n",
    "\n",
    "# --- 绩效指标函数 (与之前相同，但强调基于周期性收益) ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp)\n",
    "    total_return = cum_ret.iloc[-1] - 1\n",
    "\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 ---\n",
    "# 假设每个时间戳是10分钟，N_INTERVAL=1000，则一个周期是 1000 * 10分钟 = 10000 分钟\n",
    "# 则一年大约有 52560 / 1000 = 52.56 个 N_INTERVAL 周期\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) # 10分钟一档\n",
    "# 请根据你的数据实际频率和 N_INTERVAL 重新计算此值！\n",
    "\n",
    "stats = perf_stats(strategy_returns_series, periods_per_year_for_annualization)\n",
    "print(\"\\n--- Strategy Performance Statistics ---\")\n",
    "print(pd.Series(stats))\n",
    "\n",
    "# --- 绘制累计收益曲线 ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "strategy_returns_series.cumsum().apply(np.exp).plot()\n",
    "plt.title(f\"Long-Short Strategy Cumulative Return (Rebalance every {N_INTERVAL} bars, Long {N_LONG}, Short {N_SHORT})\")\n",
    "plt.xlabel(\"Rebalance Timestamp\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843c6c1-0789-410e-9bde-c8169cc67690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns # 用于绘制热力图\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp'（时间戳，可转为 datetime）\n",
    "# - 'symbol_enc'（币种编码）\n",
    "# - 'px'（当期价格）\n",
    "# - 'predicted_prob'（模型预测的概率，越大表示越好）\n",
    "\n",
    "# --- 配置参数 ---\n",
    "bt_df = full_df\n",
    "\n",
    "N_INTERVAL = N * 1 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "N_LONG = 2    # 做多的品种数量\n",
    "N_SHORT = 2      # 做空的品种数量\n",
    "TRANSACTION_COST_PER_TRADE = 0.001 / (N_LONG + N_SHORT)\n",
    "\n",
    "\n",
    "# --- 数据预处理 (与之前相同) ---\n",
    "bt_df['dt'] = pd.to_datetime(bt_df['timestamp'], unit='ms')\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）(与之前相同) ---\n",
    "rebalance_signals = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Identifying Rebalance Signals\"):\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t].copy()\n",
    "    if current_snapshot.empty:\n",
    "        continue\n",
    "\n",
    "    sorted_by_prediction = current_snapshot.sort_values(by='predicted_prob', ascending=False)\n",
    "    long_symbols = sorted_by_prediction.head(N_LONG)['symbol_enc'].tolist()\n",
    "    short_symbols_potential = sorted_by_prediction.tail(N_SHORT)['symbol_enc'].tolist()\n",
    "    short_symbols = [s for s in short_symbols_potential if s not in long_symbols]\n",
    "\n",
    "    rebalance_signals[t] = {'long': long_symbols, 'short': short_symbols}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 (与之前相同) ---\n",
    "period_results = []\n",
    "all_positions_by_period = []\n",
    "\n",
    "current_long_symbols = []\n",
    "current_short_symbols = []\n",
    "\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    prev_long_symbols = current_long_symbols[:]\n",
    "    prev_short_symbols = current_short_symbols[:]\n",
    "\n",
    "    if start_time in rebalance_signals:\n",
    "        current_long_symbols = rebalance_signals[start_time]['long']\n",
    "        current_short_symbols = rebalance_signals[start_time]['short']\n",
    "    \n",
    "    # --- 记录当前调仓周期所有品种的持仓状态 ---\n",
    "    all_unique_symbols = bt_df['symbol_enc'].unique()\n",
    "    current_period_positions = {symbol: 0 for symbol in all_unique_symbols}\n",
    "\n",
    "    for symbol in current_long_symbols:\n",
    "        current_period_positions[symbol] = 1\n",
    "\n",
    "    for symbol in current_short_symbols:\n",
    "        if current_period_positions[symbol] != 1:\n",
    "            current_period_positions[symbol] = -1\n",
    "\n",
    "    all_positions_by_period.append({'dt': start_time, **current_period_positions})\n",
    "\n",
    "    # --- 计算本周期的策略收益 (毛收益) ---\n",
    "    gross_period_log_return = 0.0\n",
    "    if not current_long_symbols or not current_short_symbols:\n",
    "        gross_period_log_return = 0.0\n",
    "    else:\n",
    "        period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "\n",
    "        long_portfolio_log_return = 0.0\n",
    "        for symbol in current_long_symbols:\n",
    "            try:\n",
    "                long_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                long_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                long_portfolio_log_return += (np.log(long_end_px) - np.log(long_start_px))\n",
    "            except IndexError:\n",
    "                long_portfolio_log_return += 0.0\n",
    "                pass\n",
    "\n",
    "        short_portfolio_log_return = 0.0\n",
    "        for symbol in current_short_symbols:\n",
    "            try:\n",
    "                short_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                short_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                short_portfolio_log_return += (np.log(short_end_px) - np.log(short_start_px))\n",
    "            except IndexError:\n",
    "                short_portfolio_log_return += 0.0\n",
    "                pass\n",
    "\n",
    "        num_traded_symbols = len(current_long_symbols) + len(current_short_symbols)\n",
    "        if num_traded_symbols > 0:\n",
    "            gross_period_log_return = (long_portfolio_log_return - short_portfolio_log_return) / num_traded_symbols\n",
    "        else:\n",
    "            gross_period_log_return = 0.0\n",
    "\n",
    "    # --- 计算本周期的交易手续费 ---\n",
    "    transaction_cost_this_period = 0.0\n",
    "    if i > 0:\n",
    "        close_long_count = len([s for s in prev_long_symbols if s not in current_long_symbols])\n",
    "        close_short_count = len([s for s in prev_short_symbols if s not in current_short_symbols])\n",
    "        \n",
    "        open_long_count = len([s for s in current_long_symbols if s not in prev_long_symbols])\n",
    "        open_short_count = len([s for s in current_short_symbols if s not in prev_short_symbols])\n",
    "        \n",
    "        transaction_cost_this_period = (close_long_count + close_short_count + open_long_count + open_short_count) * TRANSACTION_COST_PER_TRADE\n",
    "    else:\n",
    "        transaction_cost_this_period = (len(current_long_symbols) + len(current_short_symbols)) * TRANSACTION_COST_PER_TRADE\n",
    "\n",
    "    net_period_log_return = gross_period_log_return - transaction_cost_this_period \n",
    "    \n",
    "    period_results.append({\n",
    "        'dt': start_time,\n",
    "        'gross_strategy_log_return': gross_period_log_return,\n",
    "        'transaction_cost': transaction_cost_this_period,\n",
    "        'net_strategy_log_return': net_period_log_return,\n",
    "        'num_long_positions': len(current_long_symbols),\n",
    "        'num_short_positions': len(current_short_symbols),\n",
    "        'long_symbols_list': current_long_symbols,\n",
    "        'short_symbols_list': current_short_symbols\n",
    "    })\n",
    "\n",
    "# 将周期结果转换为 DataFrame\n",
    "strategy_results_df = pd.DataFrame(period_results).set_index('dt')\n",
    "\n",
    "# --- 准备持仓热力图数据 ---\n",
    "positions_heatmap_df = pd.DataFrame(all_positions_by_period).set_index('dt')\n",
    "positions_heatmap_df = positions_heatmap_df[sorted(positions_heatmap_df.columns)]\n",
    "\n",
    "\n",
    "# --- 绩效指标函数 (与之前相同) ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp)\n",
    "    total_return = cum_ret.iloc[-1] - 1\n",
    "\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 (与之前相同) ---\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) \n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Gross) ---\")\n",
    "gross_stats = perf_stats(strategy_results_df['gross_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(gross_stats))\n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Net of Costs) ---\")\n",
    "net_stats = perf_stats(strategy_results_df['net_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(net_stats))\n",
    "\n",
    "print(f\"\\nTotal Transaction Cost (Sum of individual costs): {strategy_results_df['transaction_cost'].sum():.6f}\")\n",
    "\n",
    "# --- 绘制图表 (使用低饱和度配色) ---\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 22), sharex=False, gridspec_kw={'height_ratios': [0.35, 0.2, 0.2, 0.25]})\n",
    "\n",
    "# 定义低饱和度颜色\n",
    "# 可以选择 seaborn 的 palette，或者自定义 HEX 颜色\n",
    "# muted, pastel, dark, bright, deep, colorblind\n",
    "# 例如：sns.color_palette(\"muted\") 或 sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "# 曲线图颜色\n",
    "COLOR_GROSS_RETURN = sns.color_palette(\"Paired\")[1] # 柔和的蓝色\n",
    "COLOR_NET_RETURN = sns.color_palette(\"Paired\")[3]   # 柔和的绿色\n",
    "COLOR_TRANSACTION_COST = sns.color_palette(\"Paired\")[5] # 柔和的红色\n",
    "COLOR_LONG_POSITIONS = sns.color_palette(\"Paired\")[7] # 柔和的紫色\n",
    "COLOR_SHORT_POSITIONS = sns.color_palette(\"Paired\")[9] # 柔和的橙色\n",
    "\n",
    "# 热力图颜色映射 (低饱和度 RdBu)\n",
    "HEATMAP_CMAP = sns.diverging_palette(240, 10, as_cmap=True, s=70, l=60, sep=1) # 240 (蓝) 到 10 (红), 饱和度70, 亮度60, 分离度1\n",
    "# 其他可选的低饱和度发散型色图： 'coolwarm_r', 'Spectral_r', 'vlag', 'icefire'\n",
    "# 或者自定义一个更温和的 RdBu\n",
    "# HEATMAP_CMAP = plt.cm.get_cmap('RdBu', 3) # RdBu色图取3个离散值，可能需要进一步调低饱和度\n",
    "# HEATMAP_CMAP = LinearSegmentedColormap.from_list(\"mycmap\", [(0, \"darkred\"), (0.5, \"lightgray\"), (1, \"darkblue\")]) # 更精细自定义\n",
    "\n",
    "# 设置图表背景风格 (可选，例如设置为白色背景)\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # 或者 'seaborn-v0_8-pastel' / 'seaborn-v0_8-muted'\n",
    "\n",
    "\n",
    "# 子图1: 累计收益 (毛收益 vs. 净收益)\n",
    "strategy_results_df['gross_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Gross Return', color=COLOR_GROSS_RETURN)\n",
    "strategy_results_df['net_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Net Return', color=COLOR_NET_RETURN)\n",
    "axes[0].set_title(f\"Cumulative Strategy Returns (Rebalance every {N_INTERVAL} bars, Long {N_LONG}, Short {N_SHORT})\")\n",
    "axes[0].set_ylabel(\"Cumulative Return\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle=':', alpha=0.7) # 调整网格线样式和透明度\n",
    "axes[0].set_xlabel(\"\")\n",
    "\n",
    "# 子图2: 累计手续费消耗\n",
    "strategy_results_df['transaction_cost'].cumsum().plot(ax=axes[1], label='Cumulative Transaction Cost', color=COLOR_TRANSACTION_COST)\n",
    "axes[1].set_title(\"Cumulative Transaction Cost Over Time\")\n",
    "axes[1].set_ylabel(\"Total Cost\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[1].set_xlabel(\"\")\n",
    "\n",
    "# 子图3: 持仓数量变化 (保留，作为快速概览)\n",
    "strategy_results_df['num_long_positions'].plot(ax=axes[2], label='Number of Long Positions', color=COLOR_LONG_POSITIONS, drawstyle='steps-post')\n",
    "strategy_results_df['num_short_positions'].plot(ax=axes[2], label='Number of Short Positions', color=COLOR_SHORT_POSITIONS, drawstyle='steps-post')\n",
    "axes[2].set_title(\"Number of Long and Short Positions Over Time (Overview)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_xlabel(\"\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# 子图4: 详细持仓热力图\n",
    "if not positions_heatmap_df.empty:\n",
    "    sns.heatmap(\n",
    "        positions_heatmap_df.T,\n",
    "        cmap=HEATMAP_CMAP, # 使用新的低饱和度色图\n",
    "        cbar_kws={'ticks': [-1, 0, 1], 'label': 'Position Status (-1: Short, 0: None, 1: Long)'},\n",
    "        ax=axes[3],\n",
    "        yticklabels=True,\n",
    "        xticklabels=True,\n",
    "        linewidths=0.5, # 增加网格线\n",
    "        linecolor='lightgray' # 网格线颜色\n",
    "    )\n",
    "    axes[3].set_title(\"Detailed Position Status Heatmap (Per Symbol)\")\n",
    "    axes[3].set_xlabel(\"Rebalance Timestamp\")\n",
    "    axes[3].set_ylabel(\"Symbol\")\n",
    "    \n",
    "    # 调整x轴刻度标签，避免重叠\n",
    "    num_ticks = 10 \n",
    "    if len(positions_heatmap_df.index) > num_ticks:\n",
    "        tick_interval = len(positions_heatmap_df.index) // num_ticks\n",
    "        axes[3].set_xticks(np.arange(0, len(positions_heatmap_df.index), tick_interval))\n",
    "        axes[3].set_xticklabels(positions_heatmap_df.index[::tick_interval].strftime('%Y-%m-%d %H:%M'))\n",
    "    else:\n",
    "        axes[3].set_xticklabels(positions_heatmap_df.index.strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(id_to_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f507b-4d63-4e0f-bbef-9636a4fe24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_to_symbol)\n",
    "print(type(id_to_symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36ef44-90b9-42a3-b30a-f22871ffd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns # 用于绘制热力图\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp'（时间戳，可转为 datetime）\n",
    "# - 'symbol_enc'（币种编码）\n",
    "# - 'px'（当期价格）\n",
    "# - 'predicted_prob'（模型预测的概率，越大表示越好）\n",
    "\n",
    "# --- 配置参数 ---\n",
    "# 注意：你的原始代码中 bt_df = full_df，这里假设 full_df 已经被传入或定义\n",
    "# 如果 full_df 是一个未定义的变量，你需要在这里定义或加载它\n",
    "# 例如：bt_df = pd.read_csv('your_data.csv')\n",
    "# 或者 bt_df = full_df # 如果 full_df 在此脚本外部定义\n",
    "\n",
    "# 这里N是一个未定义的变量，你需要给它一个具体的值，例如 100\n",
    "bt_df = full_df \n",
    "\n",
    "\n",
    "N_INTERVAL = N * 1 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "# N_INTERVAL = 200\n",
    "N_LONG = 5     # 做多的品种数量\n",
    "N_SHORT = 5      # 做空的品种数量\n",
    "TRANSACTION_COST_PER_TRADE = 0.0007 / (N_LONG + N_SHORT)\n",
    "\n",
    "# 新增：单币种止损参数\n",
    "STOP_LOSS_PERCENT = 0.15 # 单币种止损百分比，例如 0.05 表示 5% 的亏损止损\n",
    "\n",
    "# --- 数据预处理 ---\n",
    "bt_df['dt'] = pd.to_datetime(bt_df['timestamp'], unit='ms')\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）---\n",
    "rebalance_signals = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Identifying Rebalance Signals\"):\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t].copy()\n",
    "    if current_snapshot.empty:\n",
    "        rebalance_signals[t] = {'long': [], 'short': [], 'prices': {}} # 增加prices字典\n",
    "        continue\n",
    "\n",
    "    # 根据预测值排序\n",
    "    sorted_by_prediction = current_snapshot.sort_values(by='predicted_prob', ascending=False)\n",
    "    \n",
    "    # 选择多头和空头品种\n",
    "    long_symbols = sorted_by_prediction.head(N_LONG)['symbol_enc'].tolist()\n",
    "    short_symbols_potential = sorted_by_prediction.tail(N_SHORT)['symbol_enc'].tolist()\n",
    "    \n",
    "    # 确保做空品种不与做多品种重复\n",
    "    short_symbols = [s for s in short_symbols_potential if s not in long_symbols]\n",
    "\n",
    "    # !!! 强制多空同时开仓 !!!\n",
    "    if not long_symbols or not short_symbols:\n",
    "        rebalance_signals[t] = {'long': [], 'short': [], 'prices': {}}\n",
    "    else:\n",
    "        # 记录开仓时的价格\n",
    "        prices = {}\n",
    "        for symbol in long_symbols:\n",
    "            px = current_snapshot[current_snapshot['symbol_enc'] == symbol]['px'].iloc[0]\n",
    "            prices[symbol] = px\n",
    "        for symbol in short_symbols:\n",
    "            px = current_snapshot[current_snapshot['symbol_enc'] == symbol]['px'].iloc[0]\n",
    "            prices[symbol] = px\n",
    "\n",
    "        rebalance_signals[t] = {'long': long_symbols, 'short': short_symbols, 'prices': prices}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 ---\n",
    "period_results = []\n",
    "all_positions_by_period = []\n",
    "\n",
    "current_long_positions = {}  # {symbol: open_price}\n",
    "current_short_positions = {} # {symbol: open_price}\n",
    "\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    # 获取当前周期的数据快照\n",
    "    period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "    if period_data.empty:\n",
    "        # 如果当前时间段没有数据，则跳过\n",
    "        continue\n",
    "\n",
    "    # 获取当前时间点（周期开始）的价格快照，用于调仓决策\n",
    "    current_moment_prices = bt_df[bt_df['dt'] == start_time].set_index('symbol_enc')['px'].to_dict()\n",
    "    current_moment_preds = bt_df[bt_df['dt'] == start_time].set_index('symbol_enc')['predicted_prob'].to_dict()\n",
    "\n",
    "    # 复制上一个周期的持仓，用于计算交易费用\n",
    "    prev_long_symbols_for_cost = list(current_long_positions.keys())\n",
    "    prev_short_symbols_for_cost = list(current_short_positions.keys())\n",
    "\n",
    "    # --- Step 1: 检查并处理单币种止损 ---\n",
    "    # 在调仓之前，先处理已有的持仓是否触及止损\n",
    "    symbols_to_close = {'long': [], 'short': []}\n",
    "    current_period_transaction_cost = 0.0 # 用于累加本周期内的所有交易成本\n",
    "\n",
    "    # 遍历现有长仓检查止损\n",
    "    for symbol, open_px in list(current_long_positions.items()): # 使用list()避免在迭代时修改字典\n",
    "        current_px = current_moment_prices.get(symbol)\n",
    "        if current_px is not None and open_px is not None and (open_px - current_px) / open_px >= STOP_LOSS_PERCENT:\n",
    "            # 触发止损\n",
    "            del current_long_positions[symbol]\n",
    "            symbols_to_close['long'].append(symbol)\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE * 2 # 开仓一次，平仓一次，所以是2\n",
    "\n",
    "            # print(f\"Stop-loss triggered for LONG {symbol} at {start_time.strftime('%Y-%m-%d %H:%M')}. Loss: {(open_px - current_px) / open_px:.2%}\")\n",
    "\n",
    "            # 联动平仓最弱的空头仓位\n",
    "            if current_short_positions:\n",
    "                # 找到当前所有空头品种的预测值，选择预测值最高的（最不看跌的，即最弱的空头）\n",
    "                weakest_short_symbol = None\n",
    "                highest_pred_for_short = -float('inf')\n",
    "                \n",
    "                # 遍历当前空头持仓，结合最新预测值来判断哪个是“最弱”的空头\n",
    "                # “最弱”的空头意味着其看跌倾向最不明显，或者甚至开始看涨了\n",
    "                for s in current_short_positions.keys():\n",
    "                    pred_val = current_moment_preds.get(s)\n",
    "                    if pred_val is not None and pred_val > highest_pred_for_short:\n",
    "                        highest_pred_for_short = pred_val\n",
    "                        weakest_short_symbol = s\n",
    "                \n",
    "                if weakest_short_symbol:\n",
    "                    del current_short_positions[weakest_short_symbol]\n",
    "                    symbols_to_close['short'].append(weakest_short_symbol)\n",
    "                    current_period_transaction_cost += TRANSACTION_COST_PER_TRADE * 2 # 联动平仓也算费用\n",
    "                    print(f\"  -> Linkage: Closed weakest SHORT {weakest_short_symbol} to rebalance.\")\n",
    "            \n",
    "    # 遍历现有短仓检查止损\n",
    "    for symbol, open_px in list(current_short_positions.items()):\n",
    "        current_px = current_moment_prices.get(symbol)\n",
    "        if current_px is not None and open_px is not None and (current_px - open_px) / open_px >= STOP_LOSS_PERCENT:\n",
    "            # 触发止损\n",
    "            del current_short_positions[symbol]\n",
    "            symbols_to_close['short'].append(symbol)\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE * 2\n",
    "\n",
    "            # print(f\"Stop-loss triggered for SHORT {symbol} at {start_time.strftime('%Y-%m-%d %H:%M')}. Loss: {(current_px - open_px) / open_px:.2%}\")\n",
    "\n",
    "            # 联动平仓最弱的多头仓位\n",
    "            if current_long_positions:\n",
    "                # 找到当前所有多头品种的预测值，选择预测值最低的（最不看涨的，即最弱的多头）\n",
    "                weakest_long_symbol = None\n",
    "                lowest_pred_for_long = float('inf')\n",
    "\n",
    "                # 遍历当前多头持仓，结合最新预测值来判断哪个是“最弱”的多头\n",
    "                # “最弱”的多头意味着其看涨倾向最不明显，或者甚至开始看跌了\n",
    "                for s in current_long_positions.keys():\n",
    "                    pred_val = current_moment_preds.get(s)\n",
    "                    if pred_val is not None and pred_val < lowest_pred_for_long:\n",
    "                        lowest_pred_for_long = pred_val\n",
    "                        weakest_long_symbol = s\n",
    "\n",
    "                if weakest_long_symbol:\n",
    "                    del current_long_positions[weakest_long_symbol]\n",
    "                    symbols_to_close['long'].append(weakest_long_symbol)\n",
    "                    current_period_transaction_cost += TRANSACTION_COST_PER_TRADE * 2\n",
    "                    print(f\"  -> Linkage: Closed weakest LONG {weakest_long_symbol} to rebalance.\")\n",
    "\n",
    "\n",
    "    # --- Step 2: 根据调仓信号更新持仓 ---\n",
    "    # 获取本周期的目标持仓\n",
    "    target_long_symbols = rebalance_signals[start_time]['long']\n",
    "    target_short_symbols = rebalance_signals[start_time]['short']\n",
    "    target_prices = rebalance_signals[start_time]['prices'] # 目标开仓价格\n",
    "\n",
    "    # 平仓不再持有的仓位\n",
    "    for symbol in list(current_long_positions.keys()):\n",
    "        if symbol not in target_long_symbols:\n",
    "            del current_long_positions[symbol]\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE # 平仓费用\n",
    "            symbols_to_close['long'].append(symbol) # 记录平仓\n",
    "\n",
    "    for symbol in list(current_short_positions.keys()):\n",
    "        if symbol not in target_short_symbols:\n",
    "            del current_short_positions[symbol]\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE # 平仓费用\n",
    "            symbols_to_close['short'].append(symbol) # 记录平仓\n",
    "\n",
    "    # 开仓新的仓位\n",
    "    for symbol in target_long_symbols:\n",
    "        if symbol not in current_long_positions:\n",
    "            current_long_positions[symbol] = target_prices.get(symbol)\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE # 开仓费用\n",
    "\n",
    "    for symbol in target_short_symbols:\n",
    "        if symbol not in current_short_positions:\n",
    "            current_short_positions[symbol] = target_prices.get(symbol)\n",
    "            current_period_transaction_cost += TRANSACTION_COST_PER_TRADE # 开仓费用\n",
    "\n",
    "    # --- Step 3: 计算本周期的策略收益 (毛收益) ---\n",
    "    gross_period_log_return = 0.0\n",
    "    if not current_long_positions and not current_short_positions: # 如果本周期开始和结束都没有持仓\n",
    "        gross_period_log_return = 0.0\n",
    "    else:\n",
    "        long_portfolio_log_return = 0.0\n",
    "        for symbol, open_px in current_long_positions.items():\n",
    "            try:\n",
    "                # 获取本周期末的价格\n",
    "                long_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                # 这里应该用当前周期开始的价格（start_time的价格）来计算周期收益，而不是开仓价格\n",
    "                long_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                long_portfolio_log_return += (np.log(long_end_px) - np.log(long_start_px))\n",
    "            except IndexError:\n",
    "                # 如果这个品种在当前周期数据中缺失，则不计算其收益\n",
    "                long_portfolio_log_return += 0.0\n",
    "                pass\n",
    "\n",
    "        short_portfolio_log_return = 0.0\n",
    "        for symbol, open_px in current_short_positions.items():\n",
    "            try:\n",
    "                short_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                short_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == symbol)]['px'].iloc[0]\n",
    "                short_portfolio_log_return += (np.log(short_end_px) - np.log(short_start_px))\n",
    "            except IndexError:\n",
    "                short_portfolio_log_return += 0.0\n",
    "                pass\n",
    "\n",
    "        num_active_positions = len(current_long_positions) + len(current_short_positions)\n",
    "        if num_active_positions > 0:\n",
    "            gross_period_log_return = (long_portfolio_log_return - short_portfolio_log_return) / num_active_positions\n",
    "        else:\n",
    "            gross_period_log_return = 0.0\n",
    "\n",
    "    net_period_log_return = gross_period_log_return - current_period_transaction_cost\n",
    "\n",
    "    # --- 记录当前调仓周期所有品种的持仓状态 (用于热力图) ---\n",
    "    all_unique_symbols = bt_df['symbol_enc'].unique()\n",
    "    current_period_positions_for_heatmap = {symbol: 0 for symbol in all_unique_symbols}\n",
    "\n",
    "    for symbol in current_long_positions.keys():\n",
    "        current_period_positions_for_heatmap[symbol] = 1\n",
    "\n",
    "    for symbol in current_short_positions.keys():\n",
    "        if current_period_positions_for_heatmap[symbol] != 1: # 避免与多头冲突\n",
    "            current_period_positions_for_heatmap[symbol] = -1\n",
    "\n",
    "    all_positions_by_period.append({'dt': start_time, **current_period_positions_for_heatmap})\n",
    "\n",
    "\n",
    "    period_results.append({\n",
    "        'dt': start_time,\n",
    "        'gross_strategy_log_return': gross_period_log_return,\n",
    "        'transaction_cost': current_period_transaction_cost, # 本周期总费用\n",
    "        'net_strategy_log_return': net_period_log_return,\n",
    "        'num_long_positions': len(current_long_positions),\n",
    "        'num_short_positions': len(current_short_positions),\n",
    "        'long_symbols_list': list(current_long_positions.keys()),\n",
    "        'short_symbols_list': list(current_short_positions.keys())\n",
    "    })\n",
    "\n",
    "# 将周期结果转换为 DataFrame\n",
    "strategy_results_df = pd.DataFrame(period_results).set_index('dt')\n",
    "\n",
    "# --- 准备持仓热力图数据 ---\n",
    "positions_heatmap_df = pd.DataFrame(all_positions_by_period).set_index('dt')\n",
    "positions_heatmap_df = positions_heatmap_df[sorted(positions_heatmap_df.columns)]\n",
    "\n",
    "\n",
    "# --- 绩效指标函数 (与之前相同) ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp)\n",
    "    total_return = cum_ret.iloc[-1] - 1\n",
    "\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 ---\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) # 假设10ms一个bar\n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Gross) ---\")\n",
    "gross_stats = perf_stats(strategy_results_df['gross_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(gross_stats))\n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Net of Costs) ---\")\n",
    "net_stats = perf_stats(strategy_results_df['net_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(net_stats))\n",
    "\n",
    "print(f\"\\nTotal Transaction Cost (Sum of individual costs): {strategy_results_df['transaction_cost'].sum():.6f}\")\n",
    "\n",
    "# --- 绘制图表 (使用低饱和度配色) ---\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 22), sharex=False, gridspec_kw={'height_ratios': [0.35, 0.2, 0.2, 0.25]})\n",
    "\n",
    "# 定义低饱和度颜色\n",
    "COLOR_GROSS_RETURN = sns.color_palette(\"Paired\")[1]\n",
    "COLOR_NET_RETURN = sns.color_palette(\"Paired\")[3]\n",
    "COLOR_TRANSACTION_COST = sns.color_palette(\"Paired\")[5]\n",
    "COLOR_LONG_POSITIONS = sns.color_palette(\"Paired\")[7]\n",
    "COLOR_SHORT_POSITIONS = sns.color_palette(\"Paired\")[9]\n",
    "\n",
    "# 热力图颜色映射 (低饱和度 RdBu)\n",
    "HEATMAP_CMAP = sns.diverging_palette(240, 10, as_cmap=True, s=70, l=60, sep=1)\n",
    "\n",
    "# 设置图表背景风格 (可选，例如设置为白色背景)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 子图1: 累计收益 (毛收益 vs. 净收益)\n",
    "strategy_results_df['gross_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Gross Return', color=COLOR_GROSS_RETURN)\n",
    "strategy_results_df['net_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Net Return', color=COLOR_NET_RETURN)\n",
    "axes[0].set_title(f\"Cumulative Strategy Returns (Rebalance every {N_INTERVAL} bars, Long {N_LONG}, Short {N_SHORT})\")\n",
    "axes[0].set_ylabel(\"Cumulative Return\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[0].set_xlabel(\"\")\n",
    "\n",
    "# 子图2: 累计手续费消耗\n",
    "strategy_results_df['transaction_cost'].cumsum().plot(ax=axes[1], label='Cumulative Transaction Cost', color=COLOR_TRANSACTION_COST)\n",
    "axes[1].set_title(\"Cumulative Transaction Cost Over Time\")\n",
    "axes[1].set_ylabel(\"Total Cost\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[1].set_xlabel(\"\")\n",
    "\n",
    "# 子图3: 持仓数量变化 (保留，作为快速概览)\n",
    "strategy_results_df['num_long_positions'].plot(ax=axes[2], label='Number of Long Positions', color=COLOR_LONG_POSITIONS, drawstyle='steps-post')\n",
    "strategy_results_df['num_short_positions'].plot(ax=axes[2], label='Number of Short Positions', color=COLOR_SHORT_POSITIONS, drawstyle='steps-post')\n",
    "axes[2].set_title(\"Number of Long and Short Positions Over Time (Overview)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_xlabel(\"\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# 子图4: 详细持仓热力图\n",
    "if not positions_heatmap_df.empty:\n",
    "    sns.heatmap(\n",
    "        positions_heatmap_df.T,\n",
    "        cmap=HEATMAP_CMAP,\n",
    "        cbar_kws={'ticks': [-1, 0, 1], 'label': 'Position Status (-1: Short, 0: None, 1: Long)'},\n",
    "        ax=axes[3],\n",
    "        yticklabels=True,\n",
    "        xticklabels=True,\n",
    "        linewidths=0.5,\n",
    "        linecolor='lightgray'\n",
    "    )\n",
    "    axes[3].set_title(\"Detailed Position Status Heatmap (Per Symbol)\")\n",
    "    axes[3].set_xlabel(\"Rebalance Timestamp\")\n",
    "    axes[3].set_ylabel(\"Symbol\")\n",
    "    \n",
    "    # 调整x轴刻度标签，避免重叠\n",
    "    num_ticks = 10 \n",
    "    if len(positions_heatmap_df.index) > num_ticks:\n",
    "        tick_interval = len(positions_heatmap_df.index) // num_ticks\n",
    "        axes[3].set_xticks(np.arange(0, len(positions_heatmap_df.index), tick_interval))\n",
    "        axes[3].set_xticklabels(positions_heatmap_df.index[::tick_interval].strftime('%Y-%m-%d %H:%M'))\n",
    "    else:\n",
    "        axes[3].set_xticklabels(positions_heatmap_df.index.strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbadab-c011-4806-b4cf-0889a588012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# import seaborn as sns # 用于绘制热力图\n",
    "\n",
    "# # --- 假设 full_df 已经包含以下列 ---\n",
    "# # - 'timestamp'（时间戳，可转为 datetime）\n",
    "# # - 'symbol_enc'（币种编码）\n",
    "# # - 'px'（当期价格）\n",
    "# # - 'predicted_prob'（模型预测的概率，越大表示越好）\n",
    "\n",
    "# # --- 配置参数 ---\n",
    "# # 注意：你的原始代码中 bt_df = full_df，这里假设 full_df 已经被传入或定义\n",
    "# # 如果 full_df 是一个未定义的变量，你需要在这里定义或加载它\n",
    "# # 例如：bt_df = pd.read_csv('your_data.csv')\n",
    "# # 或者 bt_df = full_df # 如果 full_df 在此脚本外部定义\n",
    "\n",
    "# # 这里N是一个未定义的变量，你需要给它一个具体的值，例如 100\n",
    "# bt_df = full_df \n",
    "\n",
    "# # N = 2000\n",
    "# N_INTERVAL = N * 1 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "# N_LONG = 2      # 做多的品种数量\n",
    "# N_SHORT = 2       # 做空的品种数量\n",
    "# TRANSACTION_COST_PER_TRADE = 0.001 / (N_LONG + N_SHORT)\n",
    "\n",
    "# # 新增：单币种止损参数\n",
    "# STOP_LOSS_PERCENT = 0.15 # 单币种止损百分比，例如 0.05 表示 5% 的亏损止损\n",
    "\n",
    "# # --- 数据预处理 ---\n",
    "# bt_df['dt'] = pd.to_datetime(bt_df['timestamp'], unit='ms')\n",
    "# bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "# timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "\n",
    "# # --- 识别调仓信号 (在每个可能的调仓时间点，提前计算以提高效率) ---\n",
    "# # This part remains similar, as rebalance signals are still based on fixed intervals.\n",
    "# rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "# rebalance_signals = {}\n",
    "\n",
    "# for t in tqdm(rebalance_times, desc=\"Pre-calculating Rebalance Signals\"):\n",
    "#     current_snapshot = bt_df[bt_df['dt'] == t].copy()\n",
    "#     if current_snapshot.empty:\n",
    "#         rebalance_signals[t] = {'long': [], 'short': [], 'prices': {}}\n",
    "#         continue\n",
    "\n",
    "#     sorted_by_prediction = current_snapshot.sort_values(by='predicted_prob', ascending=False)\n",
    "    \n",
    "#     long_symbols = sorted_by_prediction.head(N_LONG)['symbol_enc'].tolist()\n",
    "#     short_symbols_potential = sorted_by_prediction.tail(N_SHORT)['symbol_enc'].tolist()\n",
    "    \n",
    "#     short_symbols = [s for s in short_symbols_potential if s not in long_symbols]\n",
    "\n",
    "#     if not long_symbols or not short_symbols:\n",
    "#         rebalance_signals[t] = {'long': [], 'short': [], 'prices': {}}\n",
    "#     else:\n",
    "#         prices = {}\n",
    "#         for symbol in long_symbols:\n",
    "#             px = current_snapshot[current_snapshot['symbol_enc'] == symbol]['px'].iloc[0]\n",
    "#             prices[symbol] = px\n",
    "#         for symbol in short_symbols:\n",
    "#             px = current_snapshot[current_snapshot['symbol_enc'] == symbol]['px'].iloc[0]\n",
    "#             prices[symbol] = px\n",
    "#         rebalance_signals[t] = {'long': long_symbols, 'short': short_symbols, 'prices': prices}\n",
    "\n",
    "# # --- Bar-by-Bar 回测模拟 ---\n",
    "# daily_results = []\n",
    "# all_positions_by_bar = []\n",
    "\n",
    "# # current_long_positions: {symbol: {'open_px': price, 'open_time': datetime}}\n",
    "# # current_short_positions: {symbol: {'open_px': price, 'open_time': datetime}}\n",
    "# current_long_positions = {}\n",
    "# current_short_positions = {}\n",
    "\n",
    "# previous_timestamp_data = None # Store prices from the previous bar for return calculation\n",
    "\n",
    "# total_cumulative_gross_log_return = 0.0\n",
    "# total_cumulative_net_log_return = 0.0\n",
    "# total_transaction_cost_acc = 0.0\n",
    "\n",
    "# for i, current_timestamp in tqdm(enumerate(timestamps_sorted), total=len(timestamps_sorted), desc=\"Simulating Bar-by-Bar\"):\n",
    "#     current_bar_data = bt_df[bt_df['dt'] == current_timestamp]\n",
    "#     if current_bar_data.empty:\n",
    "#         continue # Skip if no data for this timestamp\n",
    "\n",
    "#     current_prices = current_bar_data.set_index('symbol_enc')['px'].to_dict()\n",
    "#     current_preds = current_bar_data.set_index('symbol_enc')['predicted_prob'].to_dict()\n",
    "\n",
    "#     bar_transaction_cost = 0.0\n",
    "#     bar_gross_log_return = 0.0\n",
    "\n",
    "#     # --- Step 1: 检查并处理单币种止损 (在每个bar都检查) ---\n",
    "#     # Use a copy of keys to iterate safely while modifying the dict\n",
    "#     long_symbols_to_close_sl = []\n",
    "#     for symbol, pos_info in list(current_long_positions.items()):\n",
    "#         open_px = pos_info['open_px']\n",
    "#         current_px = current_prices.get(symbol)\n",
    "#         if current_px is not None and open_px is not None and (open_px - current_px) / open_px >= STOP_LOSS_PERCENT:\n",
    "#             long_symbols_to_close_sl.append(symbol)\n",
    "#             bar_transaction_cost += TRANSACTION_COST_PER_TRADE * 2 # Open + Close cost\n",
    "\n",
    "#             # Linkage: close weakest short\n",
    "#             if current_short_positions:\n",
    "#                 weakest_short_symbol = None\n",
    "#                 highest_pred_for_short = -float('inf')\n",
    "#                 for s_short in current_short_positions.keys():\n",
    "#                     pred_val = current_preds.get(s_short)\n",
    "#                     if pred_val is not None and pred_val > highest_pred_for_short:\n",
    "#                         highest_pred_for_short = pred_val\n",
    "#                         weakest_short_symbol = s_short\n",
    "#                 if weakest_short_symbol:\n",
    "#                     del current_short_positions[weakest_short_symbol]\n",
    "#                     bar_transaction_cost += TRANSACTION_COST_PER_TRADE * 2\n",
    "#                     print(f\"  -> Linkage: Closed weakest SHORT {weakest_short_symbol} due to LONG {symbol} stop-loss.\")\n",
    "\n",
    "#     short_symbols_to_close_sl = []\n",
    "#     for symbol, pos_info in list(current_short_positions.items()):\n",
    "#         open_px = pos_info['open_px']\n",
    "#         current_px = current_prices.get(symbol)\n",
    "#         if current_px is not None and open_px is not None and (current_px - open_px) / open_px >= STOP_LOSS_PERCENT:\n",
    "#             short_symbols_to_close_sl.append(symbol)\n",
    "#             bar_transaction_cost += TRANSACTION_COST_PER_TRADE * 2 # Open + Close cost\n",
    "\n",
    "#             # Linkage: close weakest long\n",
    "#             if current_long_positions:\n",
    "#                 weakest_long_symbol = None\n",
    "#                 lowest_pred_for_long = float('inf')\n",
    "#                 for s_long in current_long_positions.keys():\n",
    "#                     pred_val = current_preds.get(s_long)\n",
    "#                     if pred_val is not None and pred_val < lowest_pred_for_long:\n",
    "#                         lowest_pred_for_long = pred_val\n",
    "#                         weakest_long_symbol = s_long\n",
    "#                 if weakest_long_symbol:\n",
    "#                     del current_long_positions[weakest_long_symbol]\n",
    "#                     bar_transaction_cost += TRANSACTION_COST_PER_TRADE * 2\n",
    "#                     print(f\"  -> Linkage: Closed weakest LONG {weakest_long_symbol} due to SHORT {symbol} stop-loss.\")\n",
    "\n",
    "#     # Remove stop-lossed positions after iterating\n",
    "#     for symbol in long_symbols_to_close_sl:\n",
    "#         if symbol in current_long_positions:\n",
    "#             del current_long_positions[symbol]\n",
    "#     for symbol in short_symbols_to_close_sl:\n",
    "#         if symbol in current_short_positions:\n",
    "#             del current_short_positions[symbol]\n",
    "\n",
    "#     # --- Step 2: 在调仓时间点更新持仓 ---\n",
    "#     is_rebalance_time = current_timestamp in rebalance_times\n",
    "#     if is_rebalance_time:\n",
    "#         target_long_symbols = rebalance_signals[current_timestamp]['long']\n",
    "#         target_short_symbols = rebalance_signals[current_timestamp]['short']\n",
    "#         target_prices_at_rebalance = rebalance_signals[current_timestamp]['prices']\n",
    "\n",
    "#         # Determine positions to close and open for rebalancing\n",
    "#         symbols_to_close_long_rebalance = [s for s in current_long_positions.keys() if s not in target_long_symbols]\n",
    "#         symbols_to_close_short_rebalance = [s for s in current_short_positions.keys() if s not in target_short_symbols]\n",
    "        \n",
    "#         symbols_to_open_long_rebalance = [s for s in target_long_symbols if s not in current_long_positions]\n",
    "#         symbols_to_open_short_rebalance = [s for s in target_short_symbols if s not in current_short_positions]\n",
    "\n",
    "#         # Close positions\n",
    "#         for symbol in symbols_to_close_long_rebalance:\n",
    "#             del current_long_positions[symbol]\n",
    "#             bar_transaction_cost += TRANSACTION_COST_PER_TRADE # Closing cost\n",
    "#         for symbol in symbols_to_close_short_rebalance:\n",
    "#             del current_short_positions[symbol]\n",
    "#             bar_transaction_cost += TRANSACTION_COST_PER_TRADE # Closing cost\n",
    "\n",
    "#         # Open new positions\n",
    "#         for symbol in symbols_to_open_long_rebalance:\n",
    "#             open_px = target_prices_at_rebalance.get(symbol)\n",
    "#             if open_px is not None:\n",
    "#                 current_long_positions[symbol] = {'open_px': open_px, 'open_time': current_timestamp}\n",
    "#                 bar_transaction_cost += TRANSACTION_COST_PER_TRADE # Opening cost\n",
    "#         for symbol in symbols_to_open_short_rebalance:\n",
    "#             open_px = target_prices_at_rebalance.get(symbol)\n",
    "#             if open_px is not None:\n",
    "#                 current_short_positions[symbol] = {'open_px': open_px, 'open_time': current_timestamp}\n",
    "#                 bar_transaction_cost += TRANSACTION_COST_PER_TRADE # Opening cost\n",
    "\n",
    "#     # --- Step 3: 计算本bar的策略收益 ---\n",
    "#     if previous_timestamp_data is not None:\n",
    "#         long_portfolio_log_return = 0.0\n",
    "#         for symbol, pos_info in current_long_positions.items():\n",
    "#             prev_px = previous_timestamp_data.get(symbol)\n",
    "#             current_px = current_prices.get(symbol)\n",
    "#             if prev_px is not None and current_px is not None and prev_px > 0:\n",
    "#                 long_portfolio_log_return += (np.log(current_px) - np.log(prev_px))\n",
    "\n",
    "#         short_portfolio_log_return = 0.0\n",
    "#         for symbol, pos_info in current_short_positions.items():\n",
    "#             prev_px = previous_timestamp_data.get(symbol)\n",
    "#             current_px = current_prices.get(symbol)\n",
    "#             if prev_px is not None and current_px is not None and prev_px > 0:\n",
    "#                 short_portfolio_log_return += (np.log(current_px) - np.log(prev_px))\n",
    "\n",
    "#         num_active_positions = len(current_long_positions) + len(current_short_positions)\n",
    "#         if num_active_positions > 0:\n",
    "#             bar_gross_log_return = (long_portfolio_log_return - short_portfolio_log_return) / num_active_positions\n",
    "#         else:\n",
    "#             bar_gross_log_return = 0.0\n",
    "#     else: # First bar, no previous data to calculate return\n",
    "#         bar_gross_log_return = 0.0\n",
    "\n",
    "#     bar_net_log_return = bar_gross_log_return - bar_transaction_cost\n",
    "\n",
    "#     total_cumulative_gross_log_return += bar_gross_log_return\n",
    "#     total_cumulative_net_log_return += bar_net_log_return\n",
    "#     total_transaction_cost_acc += bar_transaction_cost\n",
    "\n",
    "#     # --- 记录当前bar所有品种的持仓状态 (用于热力图) ---\n",
    "#     all_unique_symbols = bt_df['symbol_enc'].unique()\n",
    "#     current_bar_positions_for_heatmap = {symbol: 0 for symbol in all_unique_symbols}\n",
    "\n",
    "#     for symbol in current_long_positions.keys():\n",
    "#         current_bar_positions_for_heatmap[symbol] = 1\n",
    "\n",
    "#     for symbol in current_short_positions.keys():\n",
    "#         if current_bar_positions_for_heatmap[symbol] != 1: # Avoid conflict if somehow a symbol is both long and short (shouldn't happen with current logic)\n",
    "#             current_bar_positions_for_heatmap[symbol] = -1\n",
    "\n",
    "#     all_positions_by_bar.append({'dt': current_timestamp, **current_bar_positions_for_heatmap})\n",
    "\n",
    "#     daily_results.append({\n",
    "#         'dt': current_timestamp,\n",
    "#         'gross_strategy_log_return': bar_gross_log_return,\n",
    "#         'transaction_cost': bar_transaction_cost,\n",
    "#         'net_strategy_log_return': bar_net_log_return,\n",
    "#         'num_long_positions': len(current_long_positions),\n",
    "#         'num_short_positions': len(current_short_positions),\n",
    "#         'long_symbols_list': list(current_long_positions.keys()),\n",
    "#         'short_symbols_list': list(current_short_positions.keys()),\n",
    "#         'cumulative_gross_return': np.exp(total_cumulative_gross_log_return),\n",
    "#         'cumulative_net_return': np.exp(total_cumulative_net_log_return)\n",
    "#     })\n",
    "    \n",
    "#     # Store current prices for the next iteration's return calculation\n",
    "#     previous_timestamp_data = current_prices\n",
    "\n",
    "# # 将bar结果转换为 DataFrame\n",
    "# strategy_results_df = pd.DataFrame(daily_results).set_index('dt')\n",
    "\n",
    "# # --- 准备持仓热力图数据 ---\n",
    "# positions_heatmap_df = pd.DataFrame(all_positions_by_bar).set_index('dt')\n",
    "# positions_heatmap_df = positions_heatmap_df[sorted(positions_heatmap_df.columns)]\n",
    "\n",
    "\n",
    "# # --- 绩效指标函数 (与之前相同) ---\n",
    "# def perf_stats(return_series, periods_per_year):\n",
    "#     if return_series.empty:\n",
    "#         return {\n",
    "#             'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "#             'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "#         }\n",
    "\n",
    "#     cum_ret = return_series.cumsum().apply(np.exp)\n",
    "#     total_return = cum_ret.iloc[-1] - 1\n",
    "\n",
    "#     num_periods = len(return_series)\n",
    "#     if num_periods > 0:\n",
    "#         # Use simple return for annualization calculation if working with log returns\n",
    "#         # For log returns, sum them and then exp to get cumulative simple return for total_return\n",
    "#         # For annualization, (1 + total_return)^(periods_per_year / num_periods) - 1\n",
    "#         ann_return = (1 + total_return)**(periods_per_year / num_periods) - 1\n",
    "#     else:\n",
    "#         ann_return = np.nan\n",
    "\n",
    "#     ann_vol = return_series.std() * np.sqrt(periods_per_year)\n",
    "#     sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "#     running_max = cum_ret.cummax()\n",
    "#     drawdown = (cum_ret - running_max) / running_max\n",
    "#     max_dd = drawdown.min()\n",
    "#     return {\n",
    "#         'Cumulative Return': total_return,\n",
    "#         'Annualized Return': ann_return,\n",
    "#         'Annualized Volatility': ann_vol,\n",
    "#         'Sharpe Ratio': sharpe,\n",
    "#         'Max Drawdown': max_dd\n",
    "#     }\n",
    "\n",
    "# # --- 计算和展示绩效 ---\n",
    "# # periods_per_year_for_annualization now based on individual bars\n",
    "# # Assuming 10ms per bar, 100 bars per second, 6000 bars per minute, 360000 bars per hour, 8640000 bars per day\n",
    "# # So, periods_per_year = (number of milliseconds in a year) / (duration of one bar in ms)\n",
    "# # 1 year = 365 days * 24 hours * 60 minutes * 60 seconds * 1000 milliseconds = 31,536,000,000 ms\n",
    "# # If 1 bar = 10ms, then periods_per_year = 31,536,000,000 / 10 = 3,153,600,000\n",
    "# # More simply, 1 bar every 10ms means 100 bars per second. So 100 * 60 * 60 * 24 * 365 = 3,153,600,000 bars per year\n",
    "# # This is a very high frequency for \"periods per year\". If your 'timestamp' is in ms and represents a bar,\n",
    "# # and each bar is 10ms apart, then N_INTERVAL * 10ms is the rebalancing interval in ms.\n",
    "# # The `periods_per_year` should reflect the frequency of your `return_series`.\n",
    "# # Since `return_series` is `bar_net_log_return` (return per bar), we need periods per year for bars.\n",
    "# bar_duration_ms = 10 # Assuming each bar represents 10 milliseconds based on original comment\n",
    "# periods_per_year_for_annualization = (1000 * 60 * 60 * 24 * 365) / bar_duration_ms\n",
    "\n",
    "\n",
    "# print(\"\\n--- Strategy Performance Statistics (Gross) ---\")\n",
    "# gross_stats = perf_stats(strategy_results_df['gross_strategy_log_return'], periods_per_year_for_annualization)\n",
    "# print(pd.Series(gross_stats))\n",
    "\n",
    "# print(\"\\n--- Strategy Performance Statistics (Net of Costs) ---\")\n",
    "# net_stats = perf_stats(strategy_results_df['net_strategy_log_return'], periods_per_year_for_annualization)\n",
    "# print(pd.Series(net_stats))\n",
    "\n",
    "# print(f\"\\nTotal Transaction Cost (Sum of individual costs): {strategy_results_df['transaction_cost'].sum():.6f}\")\n",
    "\n",
    "# # --- 绘制图表 (使用低饱和度配色) ---\n",
    "# fig, axes = plt.subplots(4, 1, figsize=(16, 22), sharex=False, gridspec_kw={'height_ratios': [0.35, 0.2, 0.2, 0.25]})\n",
    "\n",
    "# # 定义低饱和度颜色\n",
    "# COLOR_GROSS_RETURN = sns.color_palette(\"Paired\")[1]\n",
    "# COLOR_NET_RETURN = sns.color_palette(\"Paired\")[3]\n",
    "# COLOR_TRANSACTION_COST = sns.color_palette(\"Paired\")[5]\n",
    "# COLOR_LONG_POSITIONS = sns.color_palette(\"Paired\")[7]\n",
    "# COLOR_SHORT_POSITIONS = sns.color_palette(\"Paired\")[9]\n",
    "\n",
    "# # 热力图颜色映射 (低饱和度 RdBu)\n",
    "# HEATMAP_CMAP = sns.diverging_palette(240, 10, as_cmap=True, s=70, l=60, sep=1)\n",
    "\n",
    "# # 设置图表背景风格\n",
    "# plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# # 子图1: 累计收益 (毛收益 vs. 净收益)\n",
    "# strategy_results_df['cumulative_gross_return'].plot(\n",
    "#     ax=axes[0], label='Cumulative Gross Return', color=COLOR_GROSS_RETURN\n",
    "# )\n",
    "# strategy_results_df['cumulative_net_return'].plot(\n",
    "#     ax=axes[0], label='Cumulative Net Return', color=COLOR_NET_RETURN\n",
    "# )\n",
    "# axes[0].set_title(f\"Cumulative Strategy Returns (Rebalance every {N_INTERVAL} bars, Long {N_LONG}, Short {N_SHORT})\")\n",
    "# axes[0].set_ylabel(\"Cumulative Return\")\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True, linestyle=':', alpha=0.7)\n",
    "# axes[0].set_xlabel(\"\")\n",
    "\n",
    "# # 子图2: 累计手续费消耗\n",
    "# strategy_results_df['transaction_cost'].cumsum().plot(\n",
    "#     ax=axes[1], label='Cumulative Transaction Cost', color=COLOR_TRANSACTION_COST\n",
    "# )\n",
    "# axes[1].set_title(\"Cumulative Transaction Cost Over Time\")\n",
    "# axes[1].set_ylabel(\"Total Cost\")\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True, linestyle=':', alpha=0.7)\n",
    "# axes[1].set_xlabel(\"\")\n",
    "\n",
    "# # 子图3: 持仓数量变化\n",
    "# strategy_results_df['num_long_positions'].plot(\n",
    "#     ax=axes[2], label='Number of Long Positions', color=COLOR_LONG_POSITIONS, drawstyle='steps-post'\n",
    "# )\n",
    "# strategy_results_df['num_short_positions'].plot(\n",
    "#     ax=axes[2], label='Number of Short Positions', color=COLOR_SHORT_POSITIONS, drawstyle='steps-post'\n",
    "# )\n",
    "# axes[2].set_title(\"Number of Long and Short Positions Over Time (Overview)\")\n",
    "# axes[2].set_ylabel(\"Count\")\n",
    "# axes[2].set_xlabel(\"\")\n",
    "# axes[2].legend()\n",
    "# axes[2].grid(True, linestyle=':', alpha=0.7)\n",
    "# axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# # 子图4: 详细持仓热力图 (采样每 1000 点)\n",
    "# if not positions_heatmap_df.empty:\n",
    "#     sns.heatmap(\n",
    "#         positions_heatmap_df.T,\n",
    "#         cmap=HEATMAP_CMAP,\n",
    "#         cbar_kws={'ticks': [-1, 0, 1], 'label': 'Position Status (-1: Short, 0: None, 1: Long)'},\n",
    "#         ax=axes[3],\n",
    "#         yticklabels=True,\n",
    "#         xticklabels=False,\n",
    "#         linewidths=0.5,\n",
    "#         linecolor='lightgray'\n",
    "#     )\n",
    "#     axes[3].set_title(\"Detailed Position Status Heatmap (Per Symbol)\")\n",
    "#     axes[3].set_xlabel(\"Timestamp\")\n",
    "#     axes[3].set_ylabel(\"Symbol\")\n",
    "\n",
    "#     # 每隔 1000 个 Bar 采样一次作为刻度\n",
    "#     total_bars = len(positions_heatmap_df.index)\n",
    "#     sample_step = 1000\n",
    "#     tick_indices = list(range(0, total_bars, sample_step))\n",
    "#     # 确保包含最后一个点\n",
    "#     if total_bars - 1 not in tick_indices:\n",
    "#         tick_indices.append(total_bars - 1)\n",
    "\n",
    "#     axes[3].set_xticks(tick_indices)\n",
    "#     axes[3].set_xticklabels(\n",
    "#         [positions_heatmap_df.index[i].strftime('%Y-%m-%d %H:%M') for i in tick_indices],\n",
    "#         rotation=45,\n",
    "#         ha='right'\n",
    "#     )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9ac6d-2b44-468b-9ad9-8403aef23644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def save_tabnet_checkpoint(\n",
    "    model,\n",
    "    base_save_dir: str,\n",
    "    model_params: dict,\n",
    "    feature_names: list[str],\n",
    "    training_meta: dict,\n",
    "    unique_id: str = None,\n",
    "):\n",
    "    if unique_id is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        save_dir = os.path.join(base_save_dir, f\"tabnet_{timestamp}\")\n",
    "    else:\n",
    "        save_dir = os.path.join(base_save_dir, unique_id)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = os.path.join(save_dir, \"tabnet_model\")\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    # 保存模型参数和元信息\n",
    "    config_path = os.path.join(save_dir, \"model_metadata.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"model_params\": model_params,\n",
    "            \"meta_info\": training_meta,\n",
    "        }, f, indent=4)\n",
    "\n",
    "    # 保存辅助对象\n",
    "    aux_path = os.path.join(save_dir, \"auxiliary.pkl\")\n",
    "    with open(aux_path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"feature_names\": feature_names,\n",
    "        }, f)\n",
    "\n",
    "    print(f\"✅ 模型和元信息已保存到: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc556b-235e-4fdf-a1ed-b0e14ad1e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_cols, cat_idxs, cat_dims)\n",
    "N_INTERVAL = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa14571-600d-4c4c-893c-240c8b7b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(symbol_to_id.keys())\n",
    "for sym, df in symbol_to_id.items():\n",
    "    print(sym)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8583e9-e7c2-4bcc-b001-f650fa0804ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train_weeks = 10 # 可配置\n",
    "# n_val_weeks = 1    # 一般 1 周验证\n",
    "# n_test_weeks = 1   # 后 1 周做 test\n",
    "\n",
    "\n",
    "# train_dfs = weekly_dataframes[-(n_train_weeks + n_val_weeks):-n_val_weeks]\n",
    "# val_dfs = weekly_dataframes[-1]\n",
    "\n",
    "# train_df = pl.concat(train_dfs)\n",
    "# val_df = val_dfs\n",
    "\n",
    "# def process_df_np(df):\n",
    "#     df = df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px'])\n",
    "#     X = df.select(feature_cols).to_numpy()  # Polars DataFrame 转 numpy ndarray\n",
    "#     y = df.select(target_col).to_numpy().reshape(-1, 1)\n",
    "#     px = df.select('px').to_numpy()\n",
    "#     ts = df.select('timestamp').to_numpy()\n",
    "#     symbol_enc = df.select(\"enc_cat_symbol\")\n",
    "#     return X, y, px, ts, symbol_enc\n",
    "\n",
    "# X_train, y_train, px_train, ts_train, sb_train = process_df_np(train_df)\n",
    "# X_val, y_val, px_val, ts_val, sb_val = process_df_np(val_df)\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"Train {0}~{n_train_weeks}, Val {n_train_weeks+n_val_weeks}\")\n",
    "# print(\"Train:\", train_df['timestamp_dt'][0], \"to\", train_df['timestamp_dt'][-1])\n",
    "# print(\"Val:\", val_df['timestamp_dt'][0], \"to\", val_df['timestamp_dt'][-1])\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     # 模型结构参数\n",
    "#     \"n_d\": 16,                      # 决策输出维度\n",
    "#     \"n_a\": 16,                      # 注意力机制维度\n",
    "#     \"n_steps\": 3,                  # 决策步数\n",
    "#     \"gamma\": 1.3,                  # 控制特征复用的程度（>1）\n",
    "#     \"n_independent\": 2,           # 每个 step 的独立 Feature Transformer 层数\n",
    "#     \"n_shared\": 1,                # 每个 step 的共享 Feature Transformer 层数\n",
    "\n",
    "#     # 分类特征嵌入（如果你用的都是 float 特征，可以全留空）\n",
    "#     \"cat_idxs\": cat_idxs,               # 类别特征的列索引\n",
    "#     \"cat_dims\": cat_dims,               # 每个类别特征的类别数\n",
    "#     \"cat_emb_dim\": cat_emb_dim,             # 类别特征的嵌入维度（或 list）\n",
    "\n",
    "#     # 正则化与数值稳定性\n",
    "#     \"lambda_sparse\": 1e-5,        # 稀疏正则\n",
    "#     \"epsilon\": 1e-15,             # sparsemax 稳定项\n",
    "#     \"momentum\": 0.03,             # BatchNorm 的动量\n",
    "#     \"clip_value\": 3.0,            # 梯度裁剪\n",
    "    \n",
    "#     # 注意力 mask 类型\n",
    "#     \"mask_type\": \"sparsemax\",     # sparsemax 或 entmax\n",
    "\n",
    "#     # 优化器设置（函数和参数）\n",
    "#     # \"optimizer_fn\": torch.optim.Adam,    \n",
    "#     \"optimizer_params\": {\"lr\": 5e-3},\n",
    "\n",
    "#     # 学习率调度器（可选）\n",
    "#     \"scheduler_fn\": None,         # torch.optim.lr_scheduler.StepLR 等\n",
    "#     \"scheduler_params\": {},       # 比如 {\"step_size\": 20, \"gamma\": 0.95}\n",
    "\n",
    "#     # 预训练解码器结构（一般用不到）\n",
    "#     \"n_shared_decoder\": 1,\n",
    "#     \"n_indep_decoder\": 1,\n",
    "\n",
    "#     # 训练环境和调试\n",
    "#     \"seed\": 7,\n",
    "#     \"verbose\": 2,\n",
    "#     \"device_name\": \"cuda\",        # auto / cpu / cuda\n",
    "# }\n",
    "\n",
    "# fit_params = {\n",
    "#     \"eval_metric\": ['mae'],\n",
    "#     \"max_epochs\": 20,\n",
    "#     \"patience\": 5,\n",
    "#     \"batch_size\": 2048,\n",
    "#     \"virtual_batch_size\": 512,\n",
    "#     \"compute_importance\": False,\n",
    "# }\n",
    "\n",
    "\n",
    "# tabnet = TabNetRegressor(**params)\n",
    "# tabnet.fit(\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     eval_set=[(X_val, y_val)],\n",
    "#     **fit_params,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019490a6-4636-4183-bfa5-904fbb3596e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_np_to_builtin(d):\n",
    "    return {str(k): int(v) for k, v in d.items()}\n",
    "\n",
    "def convert_dict_keys_to_str_and_values_to_builtin(d):\n",
    "    new_d = {}\n",
    "    for k, v in d.items():\n",
    "        # key 可能是 np.int64\n",
    "        new_key = int(k) if isinstance(k, (np.integer,)) else str(k)\n",
    "        # value 可能是 np.str_\n",
    "        new_val = str(v) if isinstance(v, (np.str_,)) else v\n",
    "        new_d[new_key] = new_val\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d7f85-e67f-41da-8078-eb781d86d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "a  =convert_dict_np_to_builtin(symbol_to_id)\n",
    "print(a[\"bnbusdt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae90bd2-edc5-4413-b407-442015c82b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = convert_dict_keys_to_str_and_values_to_builtin(id_to_symbol)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7e08f-c899-489d-8dd7-93fc614fe854",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# save_tabnet_checkpoint(\n",
    "#     model=tabnet,\n",
    "#     base_save_dir=\"./saved_models/tabnet_crosec\",\n",
    "#     model_params=params,\n",
    "#     feature_names=feature_cols,\n",
    "#     training_meta={\n",
    "#         \"symbol_to_id\": convert_dict_np_to_builtin(symbol_to_id),\n",
    "#         \"id_to_symbol\": convert_dict_keys_to_str_and_values_to_builtin(id_to_symbol),\n",
    "#         \"train_n_week\": n_train_weeks,\n",
    "#         \"fit_params\": fit_params,\n",
    "#         \"label_window\": N,\n",
    "#         \"saved_timestamp\": str(pd.Timestamp.now()),\n",
    "#         \"feat_cal_window\": int(feat_cal_window),\n",
    "#         \"feat_norm_window\": feat_norm_window,\n",
    "#         \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "#         \"n_interval\": N_INTERVAL,\n",
    "#         \"n_long\": N_LONG,\n",
    "#         \"n_short\": N_SHORT,\n",
    "#         \"sl_percent\": STOP_LOSS_PERCENT,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5cc64-65a8-4123-be08-8bbc68296b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1408ea-8d84-4887-b41a-47983fb1321f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d32fdd-c2d7-4c21-9675-bc559721064c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
