{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3260df-8db6-4522-887c-921e601c96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def kalman_filter_series(\n",
    "        series: pl.Series,\n",
    "        R: float = 0.01,  # ËßÇÊµãÂô™Â£∞ÂçèÊñπÂ∑ÆÔºàË∂äÂ∞èË∂ä‰ø°Ôºâ\n",
    "        Q: float = 1e-5   # ËøáÁ®ãÂô™Â£∞ÂçèÊñπÂ∑ÆÔºàË∂äÂ∞èË∂äÁ®≥Ôºâ\n",
    ") -> pl.Series:\n",
    "    z = series.to_numpy()\n",
    "    n = len(z)\n",
    "    x_hat = np.zeros(n)      # ‰º∞ËÆ°ÂÄº\n",
    "    P = np.zeros(n)          # ‰º∞ËÆ°ËØØÂ∑ÆÂçèÊñπÂ∑Æ\n",
    "    x_hat[0] = z[0]\n",
    "    P[0] = 1.0\n",
    "\n",
    "    for k in range(1, n):\n",
    "        # È¢ÑÊµãÊõ¥Êñ∞\n",
    "        x_hat_minus = x_hat[k - 1]\n",
    "        P_minus = P[k - 1] + Q\n",
    "\n",
    "        # ËßÇÊµãÊõ¥Êñ∞\n",
    "        K = P_minus / (P_minus + R)  # Âç°Â∞îÊõºÂ¢ûÁõä\n",
    "        x_hat[k] = x_hat_minus + K * (z[k] - x_hat_minus)\n",
    "        P[k] = (1 - K) * P_minus\n",
    "\n",
    "    return pl.Series(name=f\"{series.name}_kalman\", values=x_hat)\n",
    "\n",
    "# Âà§Êñ≠Â∫èÂàóÊòØÊî∂Êïõ„ÄÅÂèëÊï£ËøòÊòØÊ∑∑Ê≤åÁ≥ªÁªüÔºö\n",
    "def lyapunov_series(s: pl.Series, window: int) -> pl.Series:\n",
    "    values = s.to_numpy()\n",
    "    out = [None] * len(values)\n",
    "    for i in range(window - 1, len(values)):\n",
    "        x = values[i - window + 1:i + 1]\n",
    "        lyap = np.mean(np.log(np.abs(np.diff(x)) + 1e-8))\n",
    "        out[i] = lyap\n",
    "    return pl.Series(name=f\"{s.name}_lyap_{window}\", values=out)\n",
    "\n",
    "def fft_power_topk_series(series: pl.Series, window: int = 64, k: int = 3) -> pl.Series:\n",
    "    values = series.to_list()\n",
    "    out = []\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        if i < window - 1:\n",
    "            out.append(None)\n",
    "        else:\n",
    "            window_data = values[i - window + 1 : i + 1]\n",
    "            fft = np.fft.fft(window_data)\n",
    "            powers = np.abs(fft[: window // 2])\n",
    "            out.append(np.sum(np.sort(powers)[-k:]))\n",
    "\n",
    "    return pl.Series(f\"{series.name}_fft_power_top{k}\", out)\n",
    "\n",
    "def batch_apply_single_series(\n",
    "        df_single_series_cal: pl.DataFrame,\n",
    "        window: int,\n",
    "        cols: List[str] = None\n",
    ") -> List[pl.Series]:\n",
    "    single_series = []\n",
    "    # single features transformation\n",
    "    for col in cols:\n",
    "        df_col_series = df_single_series_cal[col]\n",
    "        single_series.extend([\n",
    "            kalman_filter_series(df_col_series),\n",
    "            lyapunov_series(df_col_series, window),\n",
    "            fft_power_topk_series(df_col_series, window),\n",
    "        ])\n",
    "\n",
    "    return single_series\n",
    "\n",
    "def squared_expr(col: str) -> pl.Expr:\n",
    "    return (pl.col(col) ** 2).alias(f\"{col}_squared\")\n",
    "\n",
    "def rolling_volatility_expr(col: str, window: int) -> pl.Expr:\n",
    "    return pl.col(col).rolling_std(window).alias(f\"{col}_volatility_{window}\")\n",
    "\n",
    "def rolling_skew_expr(col: str, window: int) -> pl.Expr:\n",
    "    mean = pl.col(col).rolling_mean(window)\n",
    "    std = pl.col(col).rolling_std(window) + 1e-8\n",
    "    m3 = ((pl.col(col) - mean) ** 3).rolling_mean(window)\n",
    "    return (m3 / (std ** 3)).alias(f\"{col}_skew\")\n",
    "\n",
    "def rolling_kurt_expr(col: str, window: int) -> pl.Expr:\n",
    "    mean = pl.col(col).rolling_mean(window)\n",
    "    std = pl.col(col).rolling_std(window) + 1e-8\n",
    "    m4 = ((pl.col(col) - mean) ** 4).rolling_mean(window)\n",
    "    return (m4 / (std ** 4)).alias(f\"{col}_kurt\")\n",
    "\n",
    "def diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    return (pl.col(col) - pl.col(col).shift(lag)).alias(f\"{col}_diff_{lag}\")\n",
    "\n",
    "def second_order_diff_expr(col: str, lag: int = 1) -> pl.Expr:\n",
    "    # ‰∫åÈò∂Â∑ÆÂàÜ = ‰∏ÄÈò∂Â∑ÆÂàÜÁöÑÂ∑ÆÂàÜ\n",
    "    first_diff = pl.col(col) - pl.col(col).shift(lag)\n",
    "    second_diff = first_diff - first_diff.shift(lag)\n",
    "    return second_diff.alias(f\"{col}_second_order_diff_{lag}\")\n",
    "\n",
    "\n",
    "def momentum_ratio_expr(col: str, lag: int = 200) -> pl.Expr:\n",
    "    # Âä®ÈáèÊØîÁéá = x_t / x_{t-lag}\n",
    "    return (pl.col(col) / (pl.col(col).shift(lag) + 1e-8)).alias(f\"{col}_momentum_ratio_{lag}\")\n",
    "\n",
    "def lag_expr(col: str, lag: int = 200) -> pl.Expr:\n",
    "    return pl.col(col).shift(lag).alias(f\"{col}_lag_{lag}\")\n",
    "\n",
    "def inverse_expr(col: str) -> pl.Expr:\n",
    "    return (1 / (pl.col(col) + 1e-8)).alias(f\"{col}_inverse\")\n",
    "\n",
    "def abs_expr(col: str) -> pl.Expr:\n",
    "    return pl.col(col).abs().alias(f\"{col}_abs\")\n",
    "\n",
    "def cross_product_expr(a: str, b: str) -> pl.Expr:\n",
    "    return (pl.col(a) * pl.col(b)).alias(f\"{a}_X_{b}\")\n",
    "\n",
    "def cross_div_expr(a: str, b: str) -> pl.Expr:\n",
    "    return (pl.col(a) / (pl.col(b) + 1e-8)).alias(f\"{a}_DIV_{b}\")\n",
    "\n",
    "def spread_product_expr(a: str, b: str) -> pl.Expr:\n",
    "    col_a = pl.col(a)\n",
    "    col_b = pl.col(b)\n",
    "    max_col = pl.when(col_a >= col_b).then(col_a).otherwise(col_b)\n",
    "    min_col = pl.when(col_a < col_b).then(col_a).otherwise(col_b)\n",
    "    return ((max_col - min_col) * (col_a + col_b)).alias(f\"{a}_SPREAD_X_MAG_{b}\")\n",
    "    \n",
    "def conditioned_cross_expr_rolling(a: str, b: str, window: int) -> pl.Expr:\n",
    "    mean_col = pl.col(a).rolling_mean(window)\n",
    "    std_col = pl.col(a).rolling_std(window)\n",
    "    upper = mean_col + std_col\n",
    "    lower = mean_col - std_col\n",
    "\n",
    "    return (\n",
    "        pl.when((pl.col(a) > upper) | (pl.col(a) < lower))\n",
    "        .then(pl.col(a) * pl.col(b))\n",
    "        .otherwise(0.0)\n",
    "        .alias(f\"{a}_X_{b}_cond_dev_rolling{window}\")\n",
    "    )\n",
    "\n",
    "def cols_to_transforms(\n",
    "        df: pl.DataFrame,\n",
    "        exclude_cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['timestamp', 'timestamp_dt', 'symbol']\n",
    "\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        cols = df.collect_schema().names()\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    cols = [\n",
    "        col for col in cols\n",
    "        if col not in exclude_cols and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_scaled')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return cols\n",
    "\n",
    "def batch_apply_single_exprs(\n",
    "        window: int,\n",
    "        lag: int,\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    single_exprs = []\n",
    "    # single features transformation\n",
    "    for col in cols:\n",
    "        single_exprs.extend([\n",
    "            squared_expr(col),\n",
    "            rolling_volatility_expr(col, window),\n",
    "            rolling_skew_expr(col, window),\n",
    "            rolling_kurt_expr(col, window),\n",
    "            diff_expr(col, lag),\n",
    "            second_order_diff_expr(col, lag),\n",
    "            momentum_ratio_expr(col, lag),\n",
    "            lag_expr(col, lag),\n",
    "            inverse_expr(col),\n",
    "            abs_expr(col),\n",
    "        ])\n",
    "\n",
    "    return single_exprs\n",
    "\n",
    "def batch_apply_multi_exprs(\n",
    "        window: int,\n",
    "        cols: List[str] = None\n",
    ") -> List[str]:\n",
    "    multi_exprs = []\n",
    "\n",
    "    n = len(cols)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            a, b = cols[i], cols[j]\n",
    "            multi_exprs.extend([\n",
    "                cross_product_expr(a, b),\n",
    "                cross_div_expr(a, b),\n",
    "                spread_product_expr(a, b,),\n",
    "                conditioned_cross_expr_rolling(a, b, window),\n",
    "            ])\n",
    "\n",
    "    return multi_exprs\n",
    "\n",
    "def batch_apply_transforms(\n",
    "        df_to_transforms: pl.DataFrame,\n",
    "        window: int,\n",
    "        lag: int,\n",
    "        exclude_cols: List[str] = None\n",
    ") -> pl.DataFrame:\n",
    "    base_cols = cols_to_transforms(df_to_transforms, exclude_cols)\n",
    "    series = batch_apply_single_series(df_to_transforms, window, base_cols)\n",
    "\n",
    "    for i, s in enumerate(series):\n",
    "        series[i] = s.fill_nan(0.0).fill_null(strategy=\"forward\")\n",
    "\n",
    "    df_to_transforms = df_to_transforms.with_columns(series)\n",
    "\n",
    "    single_exprs = batch_apply_single_exprs(window, lag, base_cols)\n",
    "    multi_exprs = batch_apply_multi_exprs(window, base_cols)\n",
    "\n",
    "    exprs = single_exprs + multi_exprs\n",
    "    return df_to_transforms.with_columns(exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8dee99-6038-40a8-8d12-d2c2895dc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def split_df_by_week(\n",
    "        origin_input_df: pl.DataFrame,\n",
    "        ts_col: str = \"timestamp\"\n",
    ") -> List[pl.DataFrame]:\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(ts_col).cast(pl.Datetime).alias(f\"{ts_col}_dt\")\n",
    "    ])\n",
    "\n",
    "    origin_input_df = origin_input_df.with_columns([\n",
    "        pl.col(f\"{ts_col}_dt\").dt.truncate(\"1w\").alias(\"week_start\")\n",
    "    ])\n",
    "\n",
    "    unique_weeks = origin_input_df.select(\"week_start\").unique().sort(\"week_start\")\n",
    "\n",
    "    weekly_dfs = [\n",
    "        origin_input_df.filter(pl.col(\"week_start\") == wk).drop(\"week_start\")\n",
    "        for wk in unique_weeks[\"week_start\"]\n",
    "    ]\n",
    "\n",
    "    return weekly_dfs\n",
    "\n",
    "\n",
    "def split_df_by_month(\n",
    "        df: pl.DataFrame,\n",
    "        ts_col: str = \"timestamp\"\n",
    ") -> List[pl.DataFrame]:\n",
    "    df = df.with_columns([\n",
    "        pl.col(ts_col).cast(pl.Datetime).alias(f\"{ts_col}_dt\")\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.col(f\"{ts_col}_dt\").dt.truncate(\"1mo\").alias(\"month_start\")\n",
    "    ])\n",
    "\n",
    "    unique_months = df.select(\"month_start\").unique().sort(\"month_start\")\n",
    "\n",
    "    monthly_dfs = [\n",
    "        df.filter(pl.col(\"month_start\") == mo).drop(\"month_start\")\n",
    "        for mo in unique_months[\"month_start\"]\n",
    "    ]\n",
    "\n",
    "    return monthly_dfs\n",
    "\n",
    "\n",
    "def clean_df_drop_nulls(\n",
    "        df_to_clean: pl.DataFrame,\n",
    "        null_threshold: int = 5000,\n",
    "        verbose: bool = True\n",
    ") -> pl.DataFrame:\n",
    "    pd_df = df_to_clean.to_pandas()\n",
    "\n",
    "    null_counts = pd_df.isnull().sum()\n",
    "    cols_to_drop = null_counts[null_counts > null_threshold].index\n",
    "\n",
    "    pd_df_cleaned = pd_df.drop(columns=cols_to_drop)\n",
    "    pd_df_clean = pd_df_cleaned.dropna()\n",
    "    pl_df_clean = pl.from_pandas(pd_df_clean)\n",
    "\n",
    "    if verbose:\n",
    "        max_null_col = null_counts.idxmax()\n",
    "        max_null_count = null_counts.max()\n",
    "        print(\"ÂêÑÂàóÁ©∫ÂÄºÊï∞ÈáèÔºö\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        print(f\"Âà†Èô§Á©∫ÂÄºË∂ÖËøá {null_threshold} ÁöÑÂàóÔºö{list(cols_to_drop)}\")\n",
    "        print(f\"Âà†Èô§ÂàóÂêéÔºåDataFrameÂΩ¢Áä∂Ôºö{pd_df_cleaned.shape}\")\n",
    "        print(f\"Á©∫ÂÄºÊúÄÂ§öÁöÑÂàóÊòØÔºö{max_null_col}ÔºåÂÖ±Êúâ {max_null_count} ‰∏™Á©∫ÂÄº\")\n",
    "        print(f\"Âà†Èô§Á©∫ÂÄºË°åÂêéÔºåDataFrameÂΩ¢Áä∂Ôºö{pd_df_clean.shape}\")\n",
    "\n",
    "    return pl_df_clean\n",
    "\n",
    "def avg_steps_to_volatility(prices: np.ndarray, target_ratio: float) -> int:\n",
    "    n = len(prices)\n",
    "    steps_list = []\n",
    "    for i in tqdm(range(n), desc=f\"cal abs change {target_ratio*100:.2f}% avg steps\"):\n",
    "        start_price = prices[i]\n",
    "        steps = -1\n",
    "        for j in range(i + 1, n):\n",
    "            change = abs(prices[j] / start_price - 1)\n",
    "            if change >= target_ratio:\n",
    "                steps = j - i\n",
    "                break\n",
    "        if steps != -1:\n",
    "            steps_list.append(steps)\n",
    "    if len(steps_list) == 0:\n",
    "        return -1\n",
    "    return int(np.mean(steps_list))\n",
    "\n",
    "def future_return_expr(price_col: str, step: int) -> pl.Expr:\n",
    "    return ((pl.col(price_col).shift(-step) - pl.col(price_col)) / pl.col(price_col)).alias(f\"future_return_{step}\")\n",
    "\n",
    "def fast_spearman_ic(df: pl.DataFrame, target_col: str, exclude_prefixes: list[str]) -> dict:\n",
    "    exclude_prefixes += [col for col in df.columns if col.startswith(\"future_return_\")]\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_prefixes]\n",
    "\n",
    "    ic_dict = {}\n",
    "\n",
    "    rank_cols = feature_cols + [target_col]\n",
    "    df_ranked = df.with_columns([\n",
    "        pl.col(col).rank(method=\"average\").alias(col + \"_rank\") for col in rank_cols\n",
    "    ])\n",
    "\n",
    "    target_rank = target_col + \"_rank\"\n",
    "\n",
    "    for feat in tqdm(feature_cols, desc=\"Calculating IC\"):\n",
    "        feat_rank = feat + \"_rank\"\n",
    "        corr = df_ranked.select(\n",
    "            pl.corr(pl.col(feat_rank), pl.col(target_rank)).alias(\"corr\")\n",
    "        ).to_series()[0]\n",
    "        ic_dict[feat] = corr\n",
    "\n",
    "    return ic_dict\n",
    "\n",
    "\n",
    "def calc_hourly_rankic(\n",
    "        df_to_cal_rankic: pl.DataFrame,\n",
    "        timestamp_col: str,\n",
    "        target_col: str,\n",
    "        exclude_cols: list[str] = None,\n",
    "        factor_prefix_exclude: str = \"future_return_\"\n",
    ") -> pl.DataFrame:\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    factor_cols = [\n",
    "        col for col in df_to_cal_rankic.columns\n",
    "        if col not in exclude_cols and not col.startswith(factor_prefix_exclude)\n",
    "    ]\n",
    "\n",
    "    agg_exprs = []\n",
    "    for factor in factor_cols:\n",
    "        agg_exprs.append(\n",
    "            pl.corr(pl.col(factor).rank(method=\"average\"), pl.col(target_col).rank(method=\"average\"),\n",
    "                    method=\"spearman\").alias(factor)\n",
    "        )\n",
    "\n",
    "    ic_df = (\n",
    "        df_to_cal_rankic\n",
    "        .with_columns([\n",
    "            pl.col(timestamp_col).cast(pl.Int64).cast(pl.Datetime(\"us\")),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(timestamp_col).dt.truncate(\"1h\").alias(\"hour_group\"),\n",
    "        ])\n",
    "        .group_by(\"hour_group\")\n",
    "        .agg(agg_exprs)\n",
    "        .sort(\"hour_group\")\n",
    "    )\n",
    "\n",
    "    return ic_df\n",
    "\n",
    "def summarize_ic_df_wide(ic_df: pl.DataFrame, exclude_prefixes: list[str] = None) -> pl.DataFrame:\n",
    "    if exclude_prefixes is None:\n",
    "        exclude_prefixes = []\n",
    "\n",
    "    factor_cols = [\n",
    "        col for col in ic_df.columns\n",
    "        if col.endswith(\"_scaled\")\n",
    "           and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and col != \"price\"\n",
    "    ]\n",
    "\n",
    "    # ‰ΩøÁî® pl.DataFrame.select ÊâπÈáèËÆ°ÁÆó mean Âíå std\n",
    "    means = ic_df.select([pl.col(col).mean().alias(col) for col in factor_cols])\n",
    "    stds = ic_df.select([pl.col(col).std().alias(col) for col in factor_cols])\n",
    "\n",
    "    # ÊûÑÈÄ†ÁªìÊûú\n",
    "    data = []\n",
    "    for col in factor_cols:\n",
    "        mean_ic = means[0, col]\n",
    "        std_ic = stds[0, col]\n",
    "        ir = mean_ic / std_ic if std_ic and std_ic != 0 else None\n",
    "        data.append({\"factor\": col, \"mean_ic\": mean_ic, \"std_ic\": std_ic, \"ir\": ir})\n",
    "\n",
    "    return pl.DataFrame(data)\n",
    "\n",
    "def get_top_bottom_ic_ir(\n",
    "    ic_summary: pl.DataFrame,\n",
    "    top_n: int = 5\n",
    "):\n",
    "    pdf = ic_summary.to_pandas()\n",
    "    pdf = pdf.dropna(subset=[\"mean_ic\", \"ir\"])\n",
    "\n",
    "    def format_rows(rows, metric_name):\n",
    "        if not rows:\n",
    "            return [\"(No valid factors)\"]\n",
    "\n",
    "        max_len = max(len(row['factor']) for row in rows)\n",
    "        return [\n",
    "            f\"{i+1:>2}. {row['factor']:<{max_len}} {metric_name}: {row[metric_name]:.6f}\"\n",
    "            for i, row in enumerate(rows)\n",
    "        ]\n",
    "\n",
    "    ic_top = pdf.sort_values(\"mean_ic\", ascending=False).head(top_n).to_dict(orient=\"records\")\n",
    "    ic_bottom = pdf.sort_values(\"mean_ic\", ascending=True).head(top_n).to_dict(orient=\"records\")\n",
    "    ir_top = pdf.sort_values(\"ir\", ascending=False).head(top_n).to_dict(orient=\"records\")\n",
    "\n",
    "    print(\"üìà Top IC Factors:\")\n",
    "    print(\"\\n\".join(format_rows(ic_top, \"mean_ic\")), end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"üìâ Bottom IC Factors:\")\n",
    "    print(\"\\n\".join(format_rows(ic_bottom, \"mean_ic\")), end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"üìà Top IR Factors:\")\n",
    "    print(\"\\n\".join(format_rows(ir_top, \"ir\")), end=\"\\n\\n\")\n",
    "\n",
    "    # ÂêåÊó∂ËøîÂõûÂõ†Â≠êÂêçÂàóË°®ÔºàÂ¶ÇÈúÄËøõ‰∏ÄÊ≠•Êìç‰ΩúÔºâ\n",
    "    ic_top_names = [row[\"factor\"] for row in ic_top]\n",
    "    ic_bot_names = [row[\"factor\"] for row in ic_bottom]\n",
    "\n",
    "    ir_top_names = [row[\"factor\"] for row in ir_top]\n",
    "    return ic_top_names, ic_bot_names, ir_top_names\n",
    "\n",
    "\n",
    "def calc_monotonicity(bin_returns) -> float:\n",
    "    bins = list(range(len(bin_returns)))\n",
    "    rho, _ = spearmanr(bins, bin_returns)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def calc_binned_return_and_stability(\n",
    "        df: pl.DataFrame,\n",
    "        future_return_col: str,\n",
    "        exclude_prefixes: list[str] = None,\n",
    "        n_bins: int = 5,\n",
    "):\n",
    "    if exclude_prefixes is None:\n",
    "        exclude_prefixes = []\n",
    "\n",
    "    factors = [\n",
    "        col for col in df.columns\n",
    "        if col.endswith(\"_scaled\")\n",
    "           and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and col != \"price\"\n",
    "    ]\n",
    "\n",
    "    pdf = df.select([*factors, future_return_col]).to_pandas().dropna()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for factor in factors:\n",
    "        try:\n",
    "            pdf['factor_bin'] = pd.qcut(pdf[factor], q=n_bins, duplicates='drop')\n",
    "            if pdf[factor].nunique() < n_bins:\n",
    "                continue\n",
    "                \n",
    "            grouped = pdf.groupby('factor_bin', observed=False)[future_return_col].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "            if grouped.shape[0] < n_bins:\n",
    "                continue  # bin ‰∏çË∂≥Â∞±Ë∑≥Ëøá\n",
    "\n",
    "            bin_means = grouped['mean'].tolist()\n",
    "            spread = bin_means[-1] - bin_means[0]\n",
    "            mean_std = grouped['std'].mean()\n",
    "            mean_mean = grouped['mean'].mean()\n",
    "            stability = mean_std / (abs(mean_mean) + 1e-6)\n",
    "            monotonicity = calc_monotonicity(bin_means)\n",
    "\n",
    "            results.append({\n",
    "                'factor': factor,\n",
    "                'spread': spread,\n",
    "                'stability': stability,\n",
    "                'monotonicity': monotonicity,\n",
    "                'mean_return_top_bin': bin_means[-1],\n",
    "                'mean_return_bottom_bin': bin_means[0],\n",
    "                'bin_returns': bin_means,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped factor {factor} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    results_sorted_by_spread = sorted(results, key=lambda x: x['spread'], reverse=True)\n",
    "    results_sorted_by_stability = sorted(results, key=lambda x: x['stability'] if not np.isnan(x['stability']) else 1e9)\n",
    "    results_sorted_by_monotonicity = sorted(results, key=lambda x: abs(x['monotonicity']), reverse=True)\n",
    "    return {\n",
    "        'by_spread': results_sorted_by_spread,\n",
    "        'by_stability': results_sorted_by_stability,\n",
    "        'by_monotonicity': results_sorted_by_monotonicity,\n",
    "        'raw': results\n",
    "    }\n",
    "\n",
    "def filter_by_spearman_corr(df: pl.DataFrame, factor_cols: list[str], threshold: float = 0.9) -> list[str]:\n",
    "    matrix = np.column_stack([df[col].to_numpy() for col in factor_cols])\n",
    "\n",
    "    corr_matrix, _ = spearmanr(matrix)  # ‰∏ÄÊ¨°ÊÄßËÆ°ÁÆóÊâÄÊúâÂõ†Â≠êÁöÑ Spearman Áõ∏ÂÖ≥Á≥ªÊï∞Áü©ÈòµÔºåshape = (n, n)\n",
    "    n = len(factor_cols)\n",
    "    keep = []\n",
    "    removed = set()\n",
    "\n",
    "    print(\"Spearman correlation matrix:\")\n",
    "    print(np.round(corr_matrix, 3))\n",
    "\n",
    "    for i in range(n):\n",
    "        if factor_cols[i] in removed:\n",
    "            # print(f\"Skip {factor_cols[i]} as it is removed\")\n",
    "            continue\n",
    "        keep.append(factor_cols[i])\n",
    "        # print(f\"Keep {factor_cols[i]}\")\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            if factor_cols[j] in removed:\n",
    "                continue\n",
    "\n",
    "            corr_value = corr_matrix[i, j]\n",
    "\n",
    "            if abs(corr_value) >= threshold:\n",
    "                removed.add(factor_cols[j])\n",
    "                # print(f\"Remove {factor_cols[j]} because corr({factor_cols[i]}, {factor_cols[j]}) = {corr_value:.3f} >= {threshold}\")\n",
    "\n",
    "    return keep\n",
    "\n",
    "def filter_by_mutual_info(df: pl.DataFrame, factor_cols: list[str], target_col: str, top_k: int = 50) -> list[str]:\n",
    "    X = np.column_stack([df[col].to_numpy() for col in factor_cols])\n",
    "    y = df[target_col].to_numpy()\n",
    "\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=False)\n",
    "    sorted_idx = np.argsort(mi_scores)[::-1]\n",
    "    selected = [factor_cols[i] for i in sorted_idx[:top_k]]\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def single_asset_rolling_quantile_backtest(\n",
    "        df,\n",
    "        signal_col='signal',\n",
    "        price_col='price',\n",
    "        timestamp_col='timestamp',\n",
    "        long_quantile=0.9,\n",
    "        short_quantile=0.1,\n",
    "        window=500,\n",
    "        fee=0.001,\n",
    "        signal_mode='normal',\n",
    "        plot=True,\n",
    "):\n",
    "    df = df.select([signal_col, price_col, timestamp_col]).to_pandas()\n",
    "    if not np.issubdtype(df[timestamp_col].dtype, np.datetime64):\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], unit='us')\n",
    "    df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "    if signal_mode == 'reverse':\n",
    "        df[signal_col] = df[signal_col] * -1\n",
    "\n",
    "    # Êî∂Áõä\n",
    "    df['raw_ret'] = df[price_col].shift(-1) / df[price_col] - 1\n",
    "    df = df.dropna(subset=['raw_ret'])\n",
    "\n",
    "    # ÊªöÂä®ÂàÜ‰ΩçÊï∞Ôºà‰ΩøÁî®ËøáÂéªÁöÑÊï∞ÊçÆÔºâ\n",
    "    rolling = df[signal_col].rolling(window=window, min_periods=window)\n",
    "    df['long_thresh'] = rolling.quantile(long_quantile)\n",
    "    df['short_thresh'] = rolling.quantile(short_quantile)\n",
    "    df = df.dropna(subset=['long_thresh', 'short_thresh'])\n",
    "\n",
    "    # ‰ªì‰ΩçÔºö‰ø°Âè∑ÂøÖÈ°ªÁøªËΩ¨ÊâçÊç¢‰ªìÔºåÈÅøÂÖçÈó™ÁÉÅ\n",
    "    df['position'] = 0\n",
    "    pos = 0\n",
    "    positions = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        signal = row[signal_col]\n",
    "        long_th = row['long_thresh']\n",
    "        short_th = row['short_thresh']\n",
    "\n",
    "        long_th = short_quantile\n",
    "        short_th = long_quantile\n",
    "\n",
    "        if signal < long_th and pos != 1:\n",
    "            pos = 1  # ÂÅöÂ§ö\n",
    "        elif signal > short_th and pos != -1:\n",
    "            pos = -1  # ÂÅöÁ©∫\n",
    "        # Âê¶Âàô‰øùÊåÅÂéüÊù•‰ªì‰Ωç\n",
    "        positions.append(pos)\n",
    "\n",
    "    df['position'] = positions\n",
    "\n",
    "    # Êç¢‰ªìÊâãÁª≠Ë¥π\n",
    "    df['position_shift'] = df['position'].shift(1).fillna(0)\n",
    "    df['turnover'] = (df['position'] != df['position_shift']).astype(int)\n",
    "    df['strategy_ret'] = df['position_shift'] * df['raw_ret'] - df['turnover'] * fee\n",
    "    df['cumulative'] = (1 + df['strategy_ret']).cumprod()\n",
    "    df['buy_hold'] = df[price_col] / df[price_col].iloc[0]\n",
    "\n",
    "    # ‚ûï Áª©ÊïàÊåáÊ†á\n",
    "    trade_id = (df['position'] != df['position_shift']).cumsum()\n",
    "    df['trade_id'] = trade_id\n",
    "    trades = df[df['turnover'] == 1]\n",
    "    trade_returns = []\n",
    "\n",
    "    for tid in trades['trade_id'].unique():\n",
    "        sub = df[df['trade_id'] == tid]\n",
    "        if len(sub) > 1:\n",
    "            trade_return = (1 + sub['strategy_ret']).prod() - 1\n",
    "            trade_returns.append(trade_return)\n",
    "\n",
    "    trade_returns = np.array(trade_returns)\n",
    "    trade_count = len(trade_returns)\n",
    "    win_rate = (trade_returns > 0).sum() / trade_count\n",
    "\n",
    "    total_return = df['cumulative'].iloc[-1] - 1\n",
    "    sharpe = df['strategy_ret'].mean() / df['strategy_ret'].std()\n",
    "    max_dd = ((df['cumulative'].cummax() - df['cumulative']) / df['cumulative'].cummax()).max()\n",
    "    total_fee = df['turnover'].sum() * fee\n",
    "\n",
    "    if plot:\n",
    "        print(\"\\nüìä Á≠ñÁï•Áª©ÊïàÊåáÊ†á:\")\n",
    "\n",
    "        print(f\"ÊÄª‰∫§ÊòìÊ¨°Êï∞: {trade_count}\")\n",
    "        print(f\"ÊÄªÊâãÁª≠Ë¥π: {total_fee:.4f}ÔºàË¥πÁéá: {fee:.4f}Ôºâ\")\n",
    "\n",
    "        print(f\"ËÉúÁéá: {win_rate:.2%}\")\n",
    "        print(f\"ÊÄªÊî∂Áõä: {total_return:.2%}\")\n",
    "        print(f\"Â§èÊôÆÊØîÁéá: {sharpe:.2f}\")\n",
    "        print(f\"ÊúÄÂ§ßÂõûÊí§: {max_dd:.2%}\")\n",
    "\n",
    "        # ÁîªÂáÄÂÄºÊõ≤Á∫øÂíåÊåÅ‰ªì‰ø°Âè∑\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(df[timestamp_col], df['cumulative'], label='Strategy')\n",
    "        plt.plot(df[timestamp_col], df['buy_hold'], label='Buy & Hold', linestyle='--')\n",
    "        plt.fill_between(df[timestamp_col], 0.98, 1.02, where=df['position'] == 1, color='green', alpha=0.1,\n",
    "                         label='Long')\n",
    "        plt.fill_between(df[timestamp_col], 0.98, 1.02, where=df['position'] == -1, color='red', alpha=0.1,\n",
    "                         label='Short')\n",
    "        plt.title('Rolling Quantile Backtest')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def backtest_all_signals(\n",
    "        df,\n",
    "        signal_cols,\n",
    "        price_col='price',\n",
    "        timestamp_col='timestamp',\n",
    "        long_quantile=0.8,\n",
    "        short_quantile=0.2,\n",
    "        window=500,\n",
    "        fee=0.001,\n",
    "        top_n=50\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for signal_col in tqdm(signal_cols, desc=\"Backtesting Signals\"):  # ‚¨ÖÔ∏è Âä†ËøõÂ∫¶Êù°\n",
    "        df_result = single_asset_rolling_quantile_backtest(\n",
    "            df=df,\n",
    "            signal_col=signal_col,\n",
    "            price_col=price_col,\n",
    "            timestamp_col=timestamp_col,\n",
    "            long_quantile=long_quantile,\n",
    "            short_quantile=short_quantile,\n",
    "            window=window,\n",
    "            fee=fee,\n",
    "            plot=False  # ÊîπÊàêÂèØÊéßÁöÑ\n",
    "        )\n",
    "\n",
    "        # ÂèñÊúÄÂêé‰∏ÄË°åÁöÑÁ¥ØËÆ°Êî∂ÁõäÁ≠â‰ø°ÊÅØ\n",
    "        total_return = df_result['cumulative'].iloc[-1] - 1\n",
    "        sharpe = df_result['strategy_ret'].mean() / df_result['strategy_ret'].std() if df_result[\n",
    "                                                                                           'strategy_ret'].std() > 0 else 0\n",
    "        max_dd = ((df_result['cumulative'].cummax() - df_result['cumulative']) / df_result['cumulative'].cummax()).max()\n",
    "        total_fee = df_result['turnover'].sum() * fee\n",
    "\n",
    "        results.append({\n",
    "            'signal': signal_col,\n",
    "            'total_return': total_return,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'total_fee': total_fee,\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df = result_df.sort_values(by='total_return', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # ÊâìÂç∞Ââç top_n ÁöÑÂõ†Â≠êÊî∂ÁõäÁéá\n",
    "    print(f\"\\nüìä Ââç {top_n} Âõ†Â≠êÔºàÊåâÊÄªÊî∂ÁõäÊéíÂ∫èÔºâ:\")\n",
    "    print(result_df.head(top_n).round(4))\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def cal_z_score(x: pl.Expr, mean: pl.Expr, std: pl.Expr) -> pl.Expr:\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def scaled_sigmoid_expr(x: pl.Expr, start: pl.Expr, end: pl.Expr) -> pl.Expr:\n",
    "    n = (start - end).abs()\n",
    "    score = pl.lit(2) / (\n",
    "            pl.lit(1) + (pl.lit(2.71828) ** (-pl.lit(4_000_000).log(10) * ((x - start - n) / n) + pl.lit(5e-3).log(10)))\n",
    "    )\n",
    "    return score / pl.lit(2)\n",
    "\n",
    "\n",
    "def rolling_scaled_sigmoid_expr(\n",
    "        x: str,\n",
    "        mean_col: str,\n",
    "        std_col: str,\n",
    "        max_col: str,\n",
    "        min_col: str,\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(pl.col(std_col) == 0)\n",
    "        .then(pl.lit(0.5))\n",
    "        .otherwise(\n",
    "            scaled_sigmoid_expr(\n",
    "                cal_z_score(pl.col(x), pl.col(mean_col), pl.col(std_col)),\n",
    "                pl.lit(-2.),\n",
    "                pl.lit(2.)\n",
    "            )\n",
    "        )\n",
    "        .alias(f\"{x}_scaled\")  # ÂàóÂëΩÂêç\n",
    "    )\n",
    "\n",
    "def rolling_sum_expr(col_name: str, window: int) -> pl.Expr:\n",
    "    return pl.col(col_name).rolling_sum(window).alias(f\"{col_name}_sum_{window}\")\n",
    "\n",
    "def rolling_normalize_data(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['price', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "        and not col.startswith(\"future_return_\")\n",
    "        and not col.endswith('_scaled')  # scaled ÊòØÊúÄÁªà‰∫ßÁâ©Ôºå‰øùÁïô\n",
    "        and not (\n",
    "            col.endswith('_rolling_mean') or\n",
    "            col.endswith('_rolling_std') or\n",
    "            col.endswith('_rolling_max') or\n",
    "            col.endswith('_rolling_min')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rolling_cols = []\n",
    "    for column in columns_to_normalize:\n",
    "        rolling_cols.extend([\n",
    "            pl.col(column).rolling_mean(window).alias(f\"{column}_rolling_mean\"),\n",
    "            pl.col(column).rolling_std(window).alias(f\"{column}_rolling_std\"),\n",
    "            pl.col(column).rolling_max(window).alias(f\"{column}_rolling_max\"),\n",
    "            pl.col(column).rolling_min(window).alias(f\"{column}_rolling_min\"),\n",
    "        ])\n",
    "        \n",
    "    intermediate_cols = [\n",
    "        f\"{column}_rolling_mean\" for column in columns_to_normalize\n",
    "    ] + [\n",
    "        f\"{column}_rolling_std\" for column in columns_to_normalize\n",
    "    ] + [\n",
    "        f\"{column}_rolling_max\" for column in columns_to_normalize\n",
    "    ] + [\n",
    "        f\"{column}_rolling_min\" for column in columns_to_normalize\n",
    "    ]\n",
    "    \n",
    "    normalized_df = (\n",
    "        rollin_df\n",
    "        .with_columns(rolling_cols)\n",
    "        .with_columns([\n",
    "            rolling_scaled_sigmoid_expr(\n",
    "                column,\n",
    "                f\"{column}_rolling_mean\",\n",
    "                f\"{column}_rolling_std\",\n",
    "                f\"{column}_rolling_max\",\n",
    "                f\"{column}_rolling_min\",\n",
    "            ) for column in columns_to_normalize\n",
    "        ])\n",
    "        .drop(intermediate_cols)\n",
    "    )\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "def rolling_minmax_scaled_expr(\n",
    "        col: str,\n",
    "        min_col: str,\n",
    "        max_col: str,\n",
    "        scaled_col: str\n",
    ") -> pl.Expr:\n",
    "    return (\n",
    "        ((pl.col(col) - pl.col(min_col)) / (pl.col(max_col) - pl.col(min_col) + 1e-9))\n",
    "        .clip(0.0, 1.0)\n",
    "        .alias(scaled_col)\n",
    "    )\n",
    "\n",
    "def rolling_minmax_normalize(rollin_df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    columns_to_normalize = [\n",
    "        col for col in rollin_df.columns\n",
    "        if col not in ['price', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and not col.endswith('_scaled')  # scaled ÊòØÊúÄÁªà‰∫ßÁâ©Ôºå‰øùÁïô\n",
    "           and not (\n",
    "                col.endswith('_rolling_mean') or\n",
    "                col.endswith('_rolling_std') or\n",
    "                col.endswith('_rolling_max') or\n",
    "                col.endswith('_rolling_min')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rolling_cols = []\n",
    "    for column in columns_to_normalize:\n",
    "        rolling_cols.extend([\n",
    "            pl.col(column).rolling_max(window).alias(f\"{column}_rolling_max\"),\n",
    "            pl.col(column).rolling_min(window).alias(f\"{column}_rolling_min\"),\n",
    "        ])\n",
    "\n",
    "    intermediate_cols = [\n",
    "                            f\"{column}_rolling_max\" for column in columns_to_normalize\n",
    "                        ] + [\n",
    "                            f\"{column}_rolling_min\" for column in columns_to_normalize\n",
    "                        ]\n",
    "\n",
    "    return (\n",
    "        rollin_df\n",
    "        .with_columns(rolling_cols)\n",
    "        .with_columns([\n",
    "            rolling_minmax_scaled_expr(\n",
    "                col=column,\n",
    "                min_col=f\"{column}_rolling_min\",\n",
    "                max_col=f\"{column}_rolling_max\",\n",
    "                scaled_col=f\"{column}_scaled\"\n",
    "            ) for column in columns_to_normalize\n",
    "        ])\n",
    "        .drop(intermediate_cols)\n",
    "    )\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "def features_automation(\n",
    "    input_df_path: str,\n",
    "):\n",
    "    origin_df = pl.read_csv(input_df_path)\n",
    "    monthly_dataframes = split_df_by_month(origin_df)\n",
    "    for mo_df in monthly_dataframes:\n",
    "        mo_df = batch_apply_transforms(mo_df, 200, 1)\n",
    "\n",
    "        # check ram\n",
    "        print(f\"Polars DataFrame size: {mo_df.estimated_size() / (1024 ** 2):.4f} MB\")\n",
    "\n",
    "        # check avg steps\n",
    "        prices_np = mo_df[\"price\"].to_numpy()\n",
    "        avg_steps_05pct = avg_steps_to_volatility(prices_np, 0.005)  # Ê≥¢Âä®1%\n",
    "        avg_steps_1pct = avg_steps_to_volatility(prices_np, 0.01)  # Ê≥¢Âä®1%\n",
    "        avg_steps_2pct = avg_steps_to_volatility(prices_np, 0.02)  # Ê≥¢Âä®1%\n",
    "        print(\"Ê≥¢Âä® ¬±0.5% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_05pct)\n",
    "        print(\"Ê≥¢Âä® ¬±1% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_1pct)\n",
    "        print(\"Ê≥¢Âä® ¬±2% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_2pct)\n",
    "\n",
    "        # cal future ret\n",
    "        mo_df = mo_df.with_columns([\n",
    "            future_return_expr(\"price\", avg_steps_05pct),\n",
    "            future_return_expr(\"price\", avg_steps_1pct),\n",
    "            future_return_expr(\"price\", avg_steps_2pct),\n",
    "        ])\n",
    "        print(mo_df)\n",
    "\n",
    "        mo_df = rolling_minmax_normalize(mo_df, 200)\n",
    "        print(mo_df)\n",
    "        # clean df via pandas drop cols and rows\n",
    "        mo_df = clean_df_drop_nulls(mo_df)\n",
    "        # print(mo_df)\n",
    "        # define exclude\n",
    "        exclude_prefixes = ['price', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "        exclude_cols = exclude_prefixes + ['hour_group']\n",
    "\n",
    "        # define target col\n",
    "        target_col = f\"future_return_{avg_steps_2pct}\"\n",
    "\n",
    "        # cal hourly ic\n",
    "        ic_hourly = calc_hourly_rankic(\n",
    "            mo_df,\n",
    "            timestamp_col=\"timestamp\",\n",
    "            target_col=target_col,\n",
    "            exclude_cols=exclude_cols\n",
    "        )\n",
    "\n",
    "        # print(ic_hourly)\n",
    "\n",
    "        # get factor list filter by ic\n",
    "        ic_summary = summarize_ic_df_wide(ic_hourly, exclude_cols)\n",
    "        # print(ic_summary)\n",
    "\n",
    "        ic_top_rank, ic_bot_rank, ir_top_rank = get_top_bottom_ic_ir(ic_summary, top_n=50)\n",
    "\n",
    "        # get factor list filter by qcut\n",
    "        res = calc_binned_return_and_stability(mo_df, future_return_col=target_col, n_bins=50)\n",
    "        top_bin_ret = [r['factor'] for r in res['by_spread'][:50]]\n",
    "        top_stability = [r['factor'] for r in res['by_stability'][:50]]\n",
    "        top_monotonicity = [r['factor'] for r in res['by_monotonicity'][:50]]\n",
    "\n",
    "        # print(ic_top_rank)\n",
    "        # print(ic_bot_rank)\n",
    "        # print(ir_top_rank)\n",
    "        # print(top_bin_ret)\n",
    "        # print(top_stability)\n",
    "        # print(top_monotonicity)\n",
    "\n",
    "        # filter factors\n",
    "        factors = [\n",
    "            col for col in mo_df.columns\n",
    "            if col.endswith(\"_scaled\")\n",
    "               and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "               and not col.startswith(\"future_return_\")\n",
    "               and col != \"price\"\n",
    "        ]\n",
    "        keep_set = set(ic_top_rank) | set(ic_bot_rank) | set(ir_top_rank) | set(top_bin_ret) | set(top_stability) | set(top_monotonicity)\n",
    "        filtered_factors = [f for f in factors if f in keep_set]\n",
    "\n",
    "        result_df = backtest_all_signals(mo_df, filtered_factors)\n",
    "\n",
    "        # ÂèØÈÄâÔºöÊâìÂç∞‰øùÁïô‰∫ÜÂ§öÂ∞ë\n",
    "        print(f\"‰øùÁïôÂõ†Â≠êÊï∞Ôºö{len(filtered_factors)} / ÂéüÂßãÂõ†Â≠êÊï∞Ôºö{len(factors)}\")\n",
    "\n",
    "        # ÂèØÈÄâÔºöÊèêÂèñÂá∫Êñ∞ÁöÑ Polars Â≠ê DataFrame\n",
    "        filtered_df = mo_df.select(filtered_factors + [target_col])  # Âä†‰∏äÁõÆÊ†áÂàóÊñπ‰æøÂêéÁª≠Âª∫Ê®°\n",
    "        low_corr_factors = filter_by_spearman_corr(filtered_df, factor_cols=filtered_factors, threshold=0.99)\n",
    "\n",
    "        # Á¨¨‰∫åÊ≠•ÔºöÂÜçÁî® MI ÈÄâÂá∫Ââç k ‰∏™‰ø°ÊÅØÈáèÊúÄÂ§ßÁöÑÂõ†Â≠ê\n",
    "        final_selected_factors = filter_by_mutual_info(filtered_df, low_corr_factors, target_col=target_col, top_k=30)\n",
    "        print(final_selected_factors)\n",
    "\n",
    "        top_k = 500\n",
    "        for i, row in (result_df[['signal', 'total_return', 'sharpe', 'max_drawdown', 'total_fee']].head(top_k).round(4).iterrows()):\n",
    "            print(f\"{i + 1:>2}. Signal: {row['signal']:<40} | Return: {row['total_return']:<7} | Sharpe: {row['sharpe']:<5} | MaxDD: {row['max_drawdown']:<5} | Fee: {row['total_fee']}\")\n",
    "\n",
    "        del mo_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def calculate_half_life(signal_series: pl.Series):\n",
    "    df = pl.DataFrame({\n",
    "        \"x\": signal_series\n",
    "    }).with_columns([\n",
    "        pl.col(\"x\").shift(1).alias(\"x_lag\")\n",
    "    ]).drop_nulls()\n",
    "\n",
    "    # ÂèñÂá∫‰∏∫ numpy Êñπ‰æøÂõûÂΩí\n",
    "    x = df[\"x\"].to_numpy()\n",
    "    x_lag = df[\"x_lag\"].to_numpy()\n",
    "\n",
    "    # Âä†‰∏äÂ∏∏Êï∞È°πÂÅö OLS ÂõûÂΩí\n",
    "    X = np.vstack([np.ones_like(x_lag), x_lag]).T\n",
    "    beta = np.linalg.lstsq(X, x, rcond=None)[0][1]\n",
    "\n",
    "    # Èò≤Ê≠¢ log(Ë¥üÊï∞) Êä•Èîô\n",
    "    if beta <= 0 or beta >= 1:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    half_life = -math.log(2) / math.log(beta)\n",
    "    return half_life\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce69dbb2-b268-4be0-81bc-867738a0fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_df = pl.read_csv(\"C:/quant/data/binance_resampled_data/BTCUSDT_factors_threshold0.0005_rolling200.csv\")\n",
    "monthly_dataframes = split_df_by_month(origin_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d8e70f-bb40-4f1b-8252-98f01efc0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars DataFrame size: 5.6834 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cal abs change 0.50% avg steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18169/18169 [00:00<00:00, 60707.07it/s]\n",
      "cal abs change 1.00% avg steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18169/18169 [00:01<00:00, 16106.46it/s]\n",
      "cal abs change 2.00% avg steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18169/18169 [00:04<00:00, 4391.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≥¢Âä® ¬±0.5% ÁöÑÂùáÂÄºÊ≠•Êï∞: 88\n",
      "Ê≥¢Âä® ¬±1% ÁöÑÂùáÂÄºÊ≠•Êï∞: 344\n",
      "Ê≥¢Âä® ¬±2% ÁöÑÂùáÂÄºÊ≠•Êï∞: 1255\n",
      "shape: (18_169, 44)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ timestamp        ‚îÜ price    ‚îÜ sum_buy_size ‚îÜ sum_sell_size ‚îÜ ‚Ä¶ ‚îÜ timestamp_dt ‚îÜ future_return_88 ‚îÜ future_return_344 ‚îÜ future_return_1255 ‚îÇ\n",
      "‚îÇ ---              ‚îÜ ---      ‚îÜ ---          ‚îÜ ---           ‚îÜ   ‚îÜ ---          ‚îÜ ---              ‚îÜ ---               ‚îÜ ---                ‚îÇ\n",
      "‚îÇ i64              ‚îÜ f64      ‚îÜ f64          ‚îÜ f64           ‚îÜ   ‚îÜ datetime[Œºs] ‚îÜ f64              ‚îÜ f64               ‚îÜ f64                ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1746057893116000 ‚îÜ 94172.7  ‚îÜ 209.926      ‚îÜ 220.482       ‚îÜ ‚Ä¶ ‚îÜ 2025-05-01   ‚îÜ 0.006153         ‚îÜ 0.023211          ‚îÜ 0.029411           ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 00:04:53.116 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1746057923706000 ‚îÜ 94222.2  ‚îÜ 56.985       ‚îÜ 15.072        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-01   ‚îÜ 0.0051           ‚îÜ 0.023203          ‚îÜ 0.029479           ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 00:05:23.706 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1746057934249000 ‚îÜ 94170.5  ‚îÜ 19.905       ‚îÜ 81.358        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-01   ‚îÜ 0.006165         ‚îÜ 0.024337          ‚îÜ 0.029519           ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 00:05:34.249 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1746058047701000 ‚îÜ 94119.6  ‚îÜ 72.447       ‚îÜ 113.92        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-01   ‚îÜ 0.007218         ‚îÜ 0.024371          ‚îÜ 0.030592           ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 00:07:27.701 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1746058059172000 ‚îÜ 94167.0  ‚îÜ 22.532       ‚îÜ 4.872         ‚îÜ ‚Ä¶ ‚îÜ 2025-05-01   ‚îÜ 0.006192         ‚îÜ 0.024401          ‚îÜ 0.030596           ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 00:07:39.172 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ ‚Ä¶                ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶            ‚îÜ ‚Ä¶             ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶            ‚îÜ ‚Ä¶                ‚îÜ ‚Ä¶                 ‚îÜ ‚Ä¶                  ‚îÇ\n",
      "‚îÇ 1747612726588000 ‚îÜ 106403.7 ‚îÜ 29.234       ‚îÜ 60.608        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-18   ‚îÜ null             ‚îÜ null              ‚îÜ null               ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 23:58:46.588 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1747612740210000 ‚îÜ 106350.1 ‚îÜ 22.489       ‚îÜ 45.729        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-18   ‚îÜ null             ‚îÜ null              ‚îÜ null               ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 23:59:00.210 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1747612773323000 ‚îÜ 106403.3 ‚îÜ 269.36       ‚îÜ 151.69        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-18   ‚îÜ null             ‚îÜ null              ‚îÜ null               ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 23:59:33.323 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1747612774191000 ‚îÜ 106456.8 ‚îÜ 13.667       ‚îÜ 0.598         ‚îÜ ‚Ä¶ ‚îÜ 2025-05-18   ‚îÜ null             ‚îÜ null              ‚îÜ null               ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 23:59:34.191 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îÇ 1747612797580000 ‚îÜ 106400.1 ‚îÜ 79.225       ‚îÜ 51.744        ‚îÜ ‚Ä¶ ‚îÜ 2025-05-18   ‚îÜ null             ‚îÜ null              ‚îÜ null               ‚îÇ\n",
      "‚îÇ                  ‚îÜ          ‚îÜ              ‚îÜ               ‚îÜ   ‚îÜ 23:59:57.580 ‚îÜ                  ‚îÜ                   ‚îÜ                    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rolling_minmax_normalize_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(mo_df)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# mo_df = rolling_normalize_data(mo_df, 4000)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m mo_df \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_minmax_normalize_data\u001b[49m(mo_df, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(mo_df)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# clean df via pandas drop cols and rows\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rolling_minmax_normalize_data' is not defined"
     ]
    }
   ],
   "source": [
    "# mo_df = batch_apply_transforms(monthly_dataframes[0], 200, 1)\n",
    "mo_df = monthly_dataframes[1]\n",
    "\n",
    "# check ram\n",
    "print(f\"Polars DataFrame size: {mo_df.estimated_size() / (1024 ** 2):.4f} MB\")\n",
    "\n",
    "# check avg steps\n",
    "prices_np = mo_df[\"price\"].to_numpy()\n",
    "avg_steps_05pct = avg_steps_to_volatility(prices_np, 0.005)  # Ê≥¢Âä®1%\n",
    "avg_steps_1pct = avg_steps_to_volatility(prices_np, 0.01)  # Ê≥¢Âä®1%\n",
    "avg_steps_2pct = avg_steps_to_volatility(prices_np, 0.02)  # Ê≥¢Âä®1%\n",
    "print(\"Ê≥¢Âä® ¬±0.5% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_05pct)\n",
    "print(\"Ê≥¢Âä® ¬±1% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_1pct)\n",
    "print(\"Ê≥¢Âä® ¬±2% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_2pct)\n",
    "\n",
    "# cal future ret\n",
    "mo_df = mo_df.with_columns([\n",
    "    future_return_expr(\"price\", avg_steps_05pct),\n",
    "    future_return_expr(\"price\", avg_steps_1pct),\n",
    "    future_return_expr(\"price\", avg_steps_2pct),\n",
    "])\n",
    "print(mo_df)\n",
    "\n",
    "# mo_df = rolling_normalize_data(mo_df, 4000)\n",
    "mo_df = rolling_minmax_normalize_data(mo_df, 200)\n",
    "print(mo_df)\n",
    "# clean df via pandas drop cols and rows\n",
    "mo_df = clean_df_drop_nulls(mo_df)\n",
    "print(mo_df)\n",
    "# define exclude\n",
    "exclude_prefixes = ['price', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "exclude_cols = exclude_prefixes + ['hour_group']\n",
    "\n",
    "# define target col\n",
    "target_col = f\"future_return_{avg_steps_2pct}\"\n",
    "\n",
    "# cal hourly ic\n",
    "ic_hourly = calc_hourly_rankic(\n",
    "    mo_df,\n",
    "    timestamp_col=\"timestamp\",\n",
    "    target_col=target_col,\n",
    "    exclude_cols=exclude_cols\n",
    ")\n",
    "\n",
    "# print(ic_hourly)\n",
    "\n",
    "# get factor list filter by ic\n",
    "ic_summary = summarize_ic_df_wide(ic_hourly, exclude_cols)\n",
    "# print(ic_summary)\n",
    "\n",
    "ic_top_rank, ic_bot_rank, ir_top_rank = get_top_bottom_ic_ir(ic_summary, top_n=50)\n",
    "\n",
    "# get factor list filter by qcut\n",
    "res = calc_binned_return_and_stability(mo_df, future_return_col=target_col, n_bins=50)\n",
    "top_bin_ret = [r['factor'] for r in res['by_spread'][:50]]\n",
    "top_stability = [r['factor'] for r in res['by_stability'][:50]]\n",
    "top_monotonicity = [r['factor'] for r in res['by_monotonicity'][:50]]\n",
    "\n",
    "# print(ic_top_rank)\n",
    "# print(ic_bot_rank)\n",
    "# print(ir_top_rank)\n",
    "# print(top_bin_ret)\n",
    "# print(top_stability)\n",
    "# print(top_monotonicity)\n",
    "\n",
    "# filter factors\n",
    "factors = [\n",
    "    col for col in mo_df.columns\n",
    "    if col.endswith(\"_scaled\")\n",
    "       and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "       and not col.startswith(\"future_return_\")\n",
    "       and col != \"price\"\n",
    "]\n",
    "keep_set = set(ic_top_rank) | set(ic_bot_rank) | set(ir_top_rank) | set(top_bin_ret) | set(top_stability) | set(top_monotonicity)\n",
    "filtered_factors = [f for f in factors if f in keep_set]\n",
    "\n",
    "result_df = backtest_all_signals(mo_df, factors)\n",
    "\n",
    "# ÂèØÈÄâÔºöÊâìÂç∞‰øùÁïô‰∫ÜÂ§öÂ∞ë\n",
    "print(f\"‰øùÁïôÂõ†Â≠êÊï∞Ôºö{len(filtered_factors)} / ÂéüÂßãÂõ†Â≠êÊï∞Ôºö{len(factors)}\")\n",
    "\n",
    "# # ÂèØÈÄâÔºöÊèêÂèñÂá∫Êñ∞ÁöÑ Polars Â≠ê DataFrame\n",
    "# filtered_df = mo_df.select(filtered_factors + [target_col])  # Âä†‰∏äÁõÆÊ†áÂàóÊñπ‰æøÂêéÁª≠Âª∫Ê®°\n",
    "# low_corr_factors = filter_by_spearman_corr(filtered_df, factor_cols=filtered_factors, threshold=0.99)\n",
    "\n",
    "# # Á¨¨‰∫åÊ≠•ÔºöÂÜçÁî® MI ÈÄâÂá∫Ââç k ‰∏™‰ø°ÊÅØÈáèÊúÄÂ§ßÁöÑÂõ†Â≠ê\n",
    "# final_selected_factors = filter_by_mutual_info(filtered_df, low_corr_factors, target_col=target_col, top_k=30)\n",
    "# print(final_selected_factors)\n",
    "\n",
    "top_k = 500\n",
    "for i, row in (result_df[['signal', 'total_return', 'sharpe', 'max_drawdown', 'total_fee']].head(top_k).round(4).iterrows()):\n",
    "    print(f\"{i + 1:>2}. Signal: {row['signal']:<40} | Return: {row['total_return']:<7} | Sharpe: {row['sharpe']:<5} | MaxDD: {row['max_drawdown']:<5} | Fee: {row['total_fee']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c595dd-22ba-436d-8d92-6fe33a146515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_df2 = batch_apply_transforms(monthly_dataframes[1], 200, 1)\n",
    "mo_df2 = monthly_dataframes[1]\n",
    "\n",
    "# check ram\n",
    "print(f\"Polars DataFrame size: {mo_df2.estimated_size() / (1024 ** 2):.4f} MB\")\n",
    "\n",
    "# check avg steps\n",
    "prices_np = mo_df2[\"price\"].to_numpy()\n",
    "avg_steps_05pct = avg_steps_to_volatility(prices_np, 0.005)  # Ê≥¢Âä®1%\n",
    "avg_steps_1pct = avg_steps_to_volatility(prices_np, 0.01)  # Ê≥¢Âä®1%\n",
    "avg_steps_2pct = avg_steps_to_volatility(prices_np, 0.02)  # Ê≥¢Âä®1%\n",
    "print(\"Ê≥¢Âä® ¬±0.5% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_05pct)\n",
    "print(\"Ê≥¢Âä® ¬±1% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_1pct)\n",
    "print(\"Ê≥¢Âä® ¬±2% ÁöÑÂùáÂÄºÊ≠•Êï∞:\", avg_steps_2pct)\n",
    "\n",
    "# cal future ret\n",
    "mo_df2 = mo_df2.with_columns([\n",
    "    future_return_expr(\"price\", avg_steps_05pct),\n",
    "    future_return_expr(\"price\", avg_steps_1pct),\n",
    "    future_return_expr(\"price\", avg_steps_2pct),\n",
    "])\n",
    "print(mo_df2)\n",
    "\n",
    "mo_df2 = rolling_minmax_normalize(mo_df2, 4000)\n",
    "# mo_df2 = rolling_minmax_normalize_data(mo_df2, 200)\n",
    "print(mo_df2)\n",
    "# clean df via pandas drop cols and rows\n",
    "mo_df2 = clean_df_drop_nulls(mo_df2)\n",
    "print(mo_df2)\n",
    "# define exclude\n",
    "exclude_prefixes = ['price', 'timestamp', 'timestamp_dt', 'symbol']\n",
    "exclude_cols = exclude_prefixes + ['hour_group']\n",
    "\n",
    "# define target col\n",
    "target_col = f\"future_return_{avg_steps_2pct}\"\n",
    "\n",
    "# cal hourly ic\n",
    "ic_hourly = calc_hourly_rankic(\n",
    "    mo_df2,\n",
    "    timestamp_col=\"timestamp\",\n",
    "    target_col=target_col,\n",
    "    exclude_cols=exclude_cols\n",
    ")\n",
    "\n",
    "# print(ic_hourly)\n",
    "\n",
    "# get factor list filter by ic\n",
    "ic_summary = summarize_ic_df_wide(ic_hourly, exclude_cols)\n",
    "# print(ic_summary)\n",
    "\n",
    "ic_top_rank, ic_bot_rank, ir_top_rank = get_top_bottom_ic_ir(ic_summary, top_n=50)\n",
    "\n",
    "# get factor list filter by qcut\n",
    "res = calc_binned_return_and_stability(mo_df2, future_return_col=target_col, n_bins=50)\n",
    "top_bin_ret = [r['factor'] for r in res['by_spread'][:50]]\n",
    "top_stability = [r['factor'] for r in res['by_stability'][:50]]\n",
    "top_monotonicity = [r['factor'] for r in res['by_monotonicity'][:50]]\n",
    "\n",
    "# print(ic_top_rank)\n",
    "# print(ic_bot_rank)\n",
    "# print(ir_top_rank)\n",
    "# print(top_bin_ret)\n",
    "# print(top_stability)\n",
    "# print(top_monotonicity)\n",
    "\n",
    "# filter factors\n",
    "factors = [\n",
    "    col for col in mo_df2.columns\n",
    "    if col.endswith(\"_scaled\")\n",
    "       and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "       and not col.startswith(\"future_return_\")\n",
    "       and col != \"price\"\n",
    "]\n",
    "keep_set = set(ic_top_rank) | set(ic_bot_rank) | set(ir_top_rank) | set(top_bin_ret) | set(top_stability) | set(top_monotonicity)\n",
    "filtered_factors = [f for f in factors if f in keep_set]\n",
    "\n",
    "result_df = backtest_all_signals(mo_df2, factors)\n",
    "\n",
    "# ÂèØÈÄâÔºöÊâìÂç∞‰øùÁïô‰∫ÜÂ§öÂ∞ë\n",
    "print(f\"‰øùÁïôÂõ†Â≠êÊï∞Ôºö{len(filtered_factors)} / ÂéüÂßãÂõ†Â≠êÊï∞Ôºö{len(factors)}\")\n",
    "\n",
    "# # ÂèØÈÄâÔºöÊèêÂèñÂá∫Êñ∞ÁöÑ Polars Â≠ê DataFrame\n",
    "# filtered_df = mo_df2.select(filtered_factors + [target_col])  # Âä†‰∏äÁõÆÊ†áÂàóÊñπ‰æøÂêéÁª≠Âª∫Ê®°\n",
    "# low_corr_factors = filter_by_spearman_corr(filtered_df, factor_cols=filtered_factors, threshold=0.99)\n",
    "\n",
    "# # Á¨¨‰∫åÊ≠•ÔºöÂÜçÁî® MI ÈÄâÂá∫Ââç k ‰∏™‰ø°ÊÅØÈáèÊúÄÂ§ßÁöÑÂõ†Â≠ê\n",
    "# final_selected_factors = filter_by_mutual_info(filtered_df, low_corr_factors, target_col=target_col, top_k=30)\n",
    "# print(final_selected_factors)\n",
    "\n",
    "top_k = 500\n",
    "for i, row in (result_df[['signal', 'total_return', 'sharpe', 'max_drawdown', 'total_fee']].head(top_k).round(4).iterrows()):\n",
    "    print(f\"{i + 1:>2}. Signal: {row['signal']:<40} | Return: {row['total_return']:<7} | Sharpe: {row['sharpe']:<5} | MaxDD: {row['max_drawdown']:<5} | Fee: {row['total_fee']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1db2f-d622-45f2-925b-192d7f9f5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 'best_bid_amount_X_ret_mean_100_cond_dev_rolling200_scaled'\n",
    "\n",
    "a = single_asset_rolling_quantile_backtest(\n",
    "    mo_df,\n",
    "    signal_col=factor,\n",
    "    price_col='price',\n",
    "    timestamp_col='timestamp',\n",
    "    long_quantile=0.8,\n",
    "    short_quantile=0.2,\n",
    "    window=500,\n",
    "    fee=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f82f7-b88f-447d-9792-cf53539ad3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = single_asset_rolling_quantile_backtest(\n",
    "    mo_df2,\n",
    "    signal_col=factor,\n",
    "    price_col='price',\n",
    "    timestamp_col='timestamp',\n",
    "    long_quantile=0.8,\n",
    "    short_quantile=0.2,\n",
    "    window=500,\n",
    "    fee=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4ea16-2b9b-44cd-8298-eab1730a163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mo_df2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_factor_histogram(df, factor_name, bins=50):\n",
    "    data = df.select(pl.col(factor_name)).to_numpy().flatten()\n",
    "    print(data.min(), data.max())  # ÁúãÁúãÊúÄÂ∞èÂÄºÂíåÊúÄÂ§ßÂÄº\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(data, bins=bins, kde=True)\n",
    "    plt.title(f\"Histogram of {factor_name}\")\n",
    "    plt.xlabel(factor_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plot_factor_histogram(df, factor)\n",
    "plot_factor_histogram(df, \"price\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4052eae-4958-4a33-9a12-26c98ab84684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_factor_price_target(\n",
    "    df: pl.DataFrame,\n",
    "    factor_name: str,\n",
    "    price_col: str,\n",
    "    target_col: str,\n",
    "    n: int = 300\n",
    "):\n",
    "    df_plot = df.select([price_col, factor_name, target_col]).head(n).to_pandas()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    ax1.set_xlabel(\"Index\")\n",
    "    ax1.set_ylabel(\"Price\", color=\"tab:blue\")\n",
    "    ax1.plot(df_plot[price_col], color=\"tab:blue\", label=\"Price\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(factor_name, color=\"tab:red\")\n",
    "    ax2.plot(df_plot[factor_name], color=\"tab:red\", alpha=0.6, label=factor_name)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))  # Á¨¨‰∏â‰∏™yËΩ¥ÂæÄÂè≥ÂÅèÁßª\n",
    "    ax3.set_ylabel(target_col, color=\"tab:green\")\n",
    "    ax3.plot(df_plot[target_col], color=\"tab:green\", alpha=0.5, linestyle=\"--\", label=target_col)\n",
    "    ax3.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "    plt.title(f\"{factor_name} vs {price_col} vs {target_col}\")\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_factor_price_target(\n",
    "    df=df,\n",
    "    factor_name=factor,\n",
    "    price_col=\"price\",\n",
    "    target_col=\"momentum_confirmed_by_orderflow\",\n",
    "    n=2990000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a977d-bb36-4df5-a606-867eee53980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_factor_price_target(\n",
    "    df: pl.DataFrame,\n",
    "    factor_name: str,\n",
    "    price_col: str,\n",
    "    target_col: str,\n",
    "    n: int = 300\n",
    "):\n",
    "    df_plot = df.select([price_col, factor_name, target_col]).head(n).to_pandas()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    ax1.set_xlabel(\"Index\")\n",
    "    ax1.set_ylabel(\"Price\", color=\"tab:blue\")\n",
    "    ax1.plot(df_plot[price_col], color=\"tab:blue\", label=\"Price\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(factor_name, color=\"tab:red\")\n",
    "    ax2.plot(df_plot[factor_name], color=\"tab:red\", alpha=0.6, label=factor_name)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))  # Á¨¨‰∏â‰∏™yËΩ¥ÂæÄÂè≥ÂÅèÁßª\n",
    "    ax3.set_ylabel(target_col, color=\"tab:green\")\n",
    "    ax3.plot(df_plot[target_col], color=\"tab:green\", alpha=0.5, linestyle=\"--\", label=target_col)\n",
    "    ax3.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "    plt.title(f\"{factor_name} vs {price_col} vs {target_col}\")\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_factor_price_target(\n",
    "    df=df,\n",
    "    factor_name=factor,\n",
    "    price_col=\"price\",\n",
    "    target_col=\"momentum_confirmed_by_orderflow\",\n",
    "    n=2990000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a3961-aa8d-48e6-9dff-953138a63e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def diagnose_time_series_factor(df: pl.DataFrame, factor_name: str, lags: int = 20):\n",
    "    series = df.select(pl.col(factor_name)).to_numpy().flatten()\n",
    "    series = pd.Series(series).dropna()  # drop nan\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print(f\"üìä Factor: {factor_name}\")\n",
    "    print(f\"‚úÖ Mean: {series.mean():.4f}\")\n",
    "    print(f\"‚úÖ Std: {series.std():.4f}\")\n",
    "    print(f\"‚úÖ Skewness: {stats.skew(series):.4f}\")\n",
    "    print(f\"‚úÖ Kurtosis: {stats.kurtosis(series):.4f}\")\n",
    "    print(f\"‚úÖ ACF (lag=1): {series.autocorr(lag=1):.4f}\")\n",
    "\n",
    "    # ADF test\n",
    "    adf_result = adfuller(series)\n",
    "    print(f\"‚úÖ ADF Test p-value: {adf_result[1]:.4f} {'(Stationary ‚úÖ)' if adf_result[1] < 0.05 else '(Non-stationary ‚ö†Ô∏è)'}\")\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    sns.histplot(series, bins=50, kde=True, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title(\"Histogram + KDE\")\n",
    "\n",
    "    stats.probplot(series, dist=\"norm\", plot=axs[0, 1])\n",
    "    axs[0, 1].set_title(\"QQ-Plot\")\n",
    "\n",
    "    plot_acf(series, ax=axs[0, 2], lags=lags, title=\"ACF\")\n",
    "\n",
    "    plot_pacf(series, ax=axs[1, 0], lags=lags, title=\"PACF\", method=\"ywm\")\n",
    "\n",
    "    axs[1, 1].plot(series.values)\n",
    "    axs[1, 1].set_title(\"Raw Time Series\")\n",
    "\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Factor Diagnostic: {factor_name}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ded71a-b0ea-417e-a171-31547e263b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose_time_series_factor(df, factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38310b2-588b-42e7-a6e7-7623c4bddea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rolling_ic_ir_icto_index(\n",
    "    df: pl.DataFrame,\n",
    "    target_col: str,\n",
    "    exclude_prefixes: list[str],\n",
    "    window_size: int,\n",
    "    step: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    exclude_cols = exclude_prefixes + [target_col]\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col.endswith(\"_scaled\")\n",
    "           and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and col != \"price\"\n",
    "    ]\n",
    "\n",
    "    n = df.height\n",
    "    results = []\n",
    "    prev_ranks = {}\n",
    "\n",
    "    for start in tqdm(range(0, n - window_size + 1, step), desc=\"Rolling IC & ICTO\"):\n",
    "        end = start + window_size\n",
    "        df_win = df.slice(start, window_size)\n",
    "\n",
    "        # rank ËΩ¨Êç¢\n",
    "        df_ranked = df_win.with_columns([\n",
    "            (pl.col(c).rank(method=\"average\") / window_size).alias(c + \"_rank\") for c in feature_cols + [target_col]\n",
    "        ])\n",
    "        target_rank_col = target_col + \"_rank\"\n",
    "\n",
    "        for feat in feature_cols:\n",
    "            feat_rank_col = feat + \"_rank\"\n",
    "            ic = df_ranked.select(\n",
    "                pl.corr(pl.col(feat_rank_col), pl.col(target_rank_col)).alias(\"ic\")\n",
    "            ).to_series()[0]\n",
    "    \n",
    "            turnover = None\n",
    "            if feat in prev_ranks:\n",
    "                cur_ranks = df_ranked[feat_rank_col].to_numpy()\n",
    "                prev = prev_ranks[feat]\n",
    "                if len(prev) == len(cur_ranks):\n",
    "                    turnover = np.mean(np.abs(cur_ranks - prev))\n",
    "    \n",
    "            # Êõ¥Êñ∞ prev_ranks\n",
    "            prev_ranks[feat] = df_ranked[feat_rank_col].to_numpy()\n",
    "    \n",
    "            results.append({\n",
    "                \"window_start\": start,\n",
    "                \"window_end\": end - 1,\n",
    "                \"factor\": feat,\n",
    "                \"ic\": ic,\n",
    "                \"turnover\": turnover\n",
    "            })\n",
    "\n",
    "    df_result = pl.DataFrame(results).drop_nulls(\"turnover\")\n",
    "\n",
    "    df_ir = (\n",
    "        df_result\n",
    "        .group_by(\"factor\")\n",
    "        .agg([\n",
    "            pl.mean(\"ic\").alias(\"mean_ic\"),\n",
    "            pl.std(\"ic\").alias(\"std_ic\"),\n",
    "            pl.mean(\"turnover\").alias(\"mean_turnover\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"mean_ic\") / pl.col(\"std_ic\")).alias(\"ir\"),\n",
    "            (pl.col(\"mean_ic\") / (pl.col(\"mean_turnover\") + 1e-8)).abs().alias(\"icto\")\n",
    "        ])\n",
    "        .sort(\"icto\", descending=True)\n",
    "    )\n",
    "    return df_ir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67799e8-b27e-461d-bebc-502ae02783da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = rolling_ic_ir_icto_index(\n",
    "    mo_df2,\n",
    "    target_col=target_col,\n",
    "    exclude_prefixes=exclude_prefixes,\n",
    "    window_size=10000,\n",
    "    step=100,  # ÊªëÁ™óÊ≠•Èïø\n",
    ")\n",
    "\n",
    "print(ic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741131f-22a9-4b84-93cc-a8a0f86f4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_factors = ic_df.sort(\"mean_turnover\")\n",
    "print(sorted_factors)\n",
    "print(sorted_factors[\"factor\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cd4cb-2fd2-4c18-8c9b-ed1bc6dc336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_ic_ir_icto_index_fast(\n",
    "    df: pl.DataFrame,\n",
    "    target_col: str,\n",
    "    exclude_prefixes: list[str],\n",
    "    window_size: int,\n",
    "    step: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col.endswith(\"_scaled\")\n",
    "           and all(not col.startswith(prefix) for prefix in exclude_prefixes)\n",
    "           and not col.startswith(\"future_return_\")\n",
    "           and col != \"price\"\n",
    "    ]\n",
    "\n",
    "    n = df.height\n",
    "    results = []\n",
    "    prev_ranks = {}\n",
    "\n",
    "    for start in tqdm(range(0, n - window_size + 1, step), desc=\"Rolling IC & ICTO\"):\n",
    "        end = start + window_size\n",
    "        df_win = df.slice(start, window_size)\n",
    "\n",
    "        # üîÅ ‰∏ÄÊ¨°ÊÄß rank ËΩ¨Êç¢ÔºåÊèêÈ´òÊÄßËÉΩ\n",
    "        rank_exprs = [\n",
    "            (pl.col(c).rank(method=\"average\") / window_size).alias(c + \"_rank\")\n",
    "            for c in feature_cols + [target_col]\n",
    "        ]\n",
    "        df_ranked = df_win.with_columns(rank_exprs)\n",
    "\n",
    "        # ÊäΩÂèñÊâÄÊúâ rank ÂàóÔºåËΩ¨Êàê numpy ÂÅöÂπ∂Ë°åËÆ°ÁÆó\n",
    "        df_ranked_np = df_ranked.select([c + \"_rank\" for c in feature_cols + [target_col]]).to_numpy()\n",
    "        target_idx = -1  # ÊúÄÂêé‰∏ÄÂàóÊòØ target_col_rank\n",
    "        target_vec = df_ranked_np[:, target_idx]\n",
    "\n",
    "        for i, feat in enumerate(feature_cols):\n",
    "            feat_vec = df_ranked_np[:, i]\n",
    "\n",
    "            # fast pearson corr\n",
    "            ic = np.corrcoef(feat_vec, target_vec)[0, 1]\n",
    "\n",
    "            turnover = None\n",
    "            if feat in prev_ranks:\n",
    "                prev = prev_ranks[feat]\n",
    "                if len(prev) == len(feat_vec):\n",
    "                    turnover = np.mean(np.abs(feat_vec - prev))\n",
    "\n",
    "            # Â≠òÂÇ®ÂΩìÂâç\n",
    "            prev_ranks[feat] = feat_vec\n",
    "\n",
    "            results.append({\n",
    "                \"window_start\": int(start),\n",
    "                \"window_end\": int(end - 1),\n",
    "                \"factor\": str(feat),\n",
    "                \"ic\": float(ic) if not np.isnan(ic) else None,\n",
    "                \"turnover\": float(turnover) if turnover is not None else None\n",
    "            })\n",
    "\n",
    "\n",
    "    return (\n",
    "        pl\n",
    "        .DataFrame(results)\n",
    "        .group_by(\"factor\")\n",
    "        .agg([\n",
    "            pl.mean(\"ic\").alias(\"mean_ic\"),\n",
    "            pl.std(\"ic\").alias(\"std_ic\"),\n",
    "            pl.mean(\"turnover\").alias(\"mean_turnover\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"mean_ic\") / pl.col(\"std_ic\")).alias(\"ir\"),\n",
    "            (pl.col(\"mean_ic\") / (pl.col(\"mean_turnover\") + 1e-8)).abs().alias(\"icto\")\n",
    "        ])\n",
    "        .sort(\"icto\", descending=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0c8b2-e4b3-42f1-9dba-6c382eae3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = rolling_ic_ir_icto_index_fast(\n",
    "    mo_df,\n",
    "    target_col=target_col,\n",
    "    exclude_prefixes=exclude_prefixes,\n",
    "    window_size=10000,\n",
    "    step=100,  # ÊªëÁ™óÊ≠•Èïø\n",
    ")\n",
    "\n",
    "print(ic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e01229-4c7f-4cbe-b249-333ef9fa1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_maximize_icir(df: pl.DataFrame, factor_cols: list[str], target_col: str, max_factors: int = 20):\n",
    "    selected = []\n",
    "    remaining = factor_cols.copy()\n",
    "\n",
    "    for _ in range(max_factors):\n",
    "        best_icir = -np.inf\n",
    "        best_factor = None\n",
    "\n",
    "        for f in remaining:\n",
    "            trial_factors = selected + [f]\n",
    "            X = df.select(trial_factors).to_numpy()\n",
    "            y = df[target_col].to_numpy()\n",
    "            icir = spearmanr(np.mean(X, axis=1), y)[0] / np.std(np.mean(X, axis=1))\n",
    "            if icir > best_icir:\n",
    "                best_icir = icir\n",
    "                best_factor = f\n",
    "        if best_factor:\n",
    "            selected.append(best_factor)\n",
    "            remaining.remove(best_factor)\n",
    "        else:\n",
    "            break\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "358f0920-bc5c-4bc1-adcd-7b7d92a54307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.64185389, -0.09531018,  0.        ,  0.09531018,  4.81947479])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([-0.9, -0.1, 0, 0.1, 122.9])\n",
    "np.log1p(np.abs(x)) * np.sign(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c3b94-c924-461b-8ab5-2d81afeba7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
