{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-01T04:21:58.659248Z",
     "start_time": "2025-09-01T04:21:58.644965Z"
    }
   },
   "source": [
    "print(chr(0))\n",
    "print(ord('ç‰›'))\n",
    "\n",
    "print(chr(29275))\n",
    "print(chr(0x1F60E))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "29275\n",
      "ç‰›\n",
      "ğŸ˜\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:21:12.211169Z",
     "start_time": "2025-09-01T04:21:12.192568Z"
    }
   },
   "cell_type": "code",
   "source": "print(chr(0))",
   "id": "d3fa03a696547309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:22:12.807024Z",
     "start_time": "2025-09-01T04:22:12.796894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "l = \"this is a test\" + chr(0) + \"string\"\n",
    "l"
   ],
   "id": "9b60edb1185cca12",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:22:21.132204Z",
     "start_time": "2025-09-01T04:22:21.119669Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"this is a test\" + chr(0) + \"string\")\n",
   "id": "97f70eb1e12e485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:25:18.224710Z",
     "start_time": "2025-09-01T04:25:18.213212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# åŠ¨æ‰‹è·‘ï¼\n",
    "test_string = \"hello! å¤©æµ·!\"  # è¯•è¯•ä½ çš„åå­—ï¼Ÿ\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)  # è¾“å‡º: b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\n",
    "print(type(utf8_encoded))\n",
    "\n",
    "# æ‹†è§£å­—èŠ‚å€¼ï¼ˆ0-255æ•´æ•°ï¼‰\n",
    "list(utf8_encoded)\n",
    "\n",
    "# éªŒè¯å¯é€†æ€§\n",
    "print(len(test_string), len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))  # å®Œç¾è¿˜åŸ: 'hello! å¤©æµ·!'"
   ],
   "id": "28602203da6e443",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\n",
      "<class 'bytes'>\n",
      "10 14\n",
      "hello! å¤©æµ·!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:26:03.502044Z",
     "start_time": "2025-09-01T04:26:03.490537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):  # æ³¨æ„å‚æ•°åä¿®æ­£ï¼\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "# decode_utf8_bytes_to_str_wrong(\"cafÃ©\".encode(\"utf-8\")) é”™è¯¯"
   ],
   "id": "95077a4c86276b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:29:52.617076Z",
     "start_time": "2025-09-01T04:29:52.603056Z"
    }
   },
   "cell_type": "code",
   "source": "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])\n",
   "id": "d0fcf253226d9688",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:30:06.961727Z",
     "start_time": "2025-09-01T04:30:06.949196Z"
    }
   },
   "cell_type": "code",
   "source": "max([(\"es\"),(\"st\")])\n",
   "id": "734f8138c2535f30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:45:43.046412Z",
     "start_time": "2025-09-01T04:45:42.355059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import heapq\n",
    "import regex\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, DefaultDict, Any, Union\n",
    "import mmap\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# GPT-2é¢„åˆ†è¯æ¨¡å¼\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "def load_and_sample_data(file_path: str, sample_size: int = 22000, special_token: str = \"<|endoftext|>\") -> str:\n",
    "    \"\"\"å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å¹¶é‡‡æ ·æ–‡æ¡£\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r+\", encoding='utf-8', errors='ignore') as f:\n",
    "            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "                documents = []\n",
    "                start = 0\n",
    "                while start < len(mm):\n",
    "                    end = mm.find(special_token.encode('utf-8'), start)\n",
    "                    if end == -1:\n",
    "                        doc = mm[start:].decode('utf-8', errors='replace').strip()\n",
    "                        if doc:\n",
    "                            documents.append(doc)\n",
    "                        break\n",
    "\n",
    "                    doc = mm[start:end].decode('utf-8', errors='replace').strip()\n",
    "                    if doc:\n",
    "                        documents.append(doc)\n",
    "                    start = end + len(special_token)\n",
    "\n",
    "                if len(documents) > sample_size:\n",
    "                    documents = random.sample(documents, sample_size)\n",
    "\n",
    "                return special_token.join(documents)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}\")\n",
    "\n",
    "def gpt2_bytes_to_unicode_local() -> Dict[int, str]:\n",
    "    \"\"\"å­—èŠ‚åˆ°Unicodeæ˜ å°„\"\"\"\n",
    "    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(256):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(256 + n)\n",
    "            n += 1\n",
    "    return {b: chr(c) for b, c in zip(bs, cs)}\n",
    "\n",
    "def pre_tokenize_document(doc: str, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n",
    "    \"\"\"é¢„åˆ†è¯å¤„ç†å•ä¸ªæ–‡æ¡£\"\"\"\n",
    "    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)\n",
    "    sequences = []\n",
    "    for token in tokens:\n",
    "        token_unicode = ''.join(bytes_to_unicode_map[b] for b in token.encode('utf-8'))\n",
    "        sequences.append(list(token_unicode))\n",
    "    return sequences\n",
    "\n",
    "def parallel_pre_tokenize(documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n",
    "    \"\"\"å¹¶è¡Œé¢„åˆ†è¯ä¼˜åŒ–\"\"\"\n",
    "    if num_processes <= 1:\n",
    "        return [seq for doc in documents for seq in pre_tokenize_document(doc, bytes_to_unicode_map)]\n",
    "\n",
    "    with multiprocessing.Pool(\n",
    "        num_processes,\n",
    "        initializer=init_worker,\n",
    "        initargs=(bytes_to_unicode_map,)\n",
    "    ) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(pre_tokenize_worker, documents, chunksize=50),\n",
    "            total=len(documents),\n",
    "            desc=\"é¢„åˆ†è¯\",\n",
    "            mininterval=1\n",
    "        ))\n",
    "    return [seq for doc_sequences in results for seq in doc_sequences]\n",
    "\n",
    "# å…¨å±€å˜é‡ç”¨äºå¤šè¿›ç¨‹\n",
    "global_worker_byte_map = None\n",
    "def init_worker(byte_map: Dict[int, str]):\n",
    "    global global_worker_byte_map\n",
    "    global_worker_byte_map = byte_map\n",
    "\n",
    "def pre_tokenize_worker(doc: str) -> List[List[str]]:\n",
    "    return pre_tokenize_document(doc, global_worker_byte_map)\n",
    "\n",
    "class BPEIndex:\n",
    "    \"\"\"é«˜æ•ˆç´¢å¼•ç»“æ„ç”¨äºBPEåˆå¹¶\"\"\"\n",
    "    def __init__(self, sequences: List[List[str]]):\n",
    "        self.sequences = sequences # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬åºåˆ—\n",
    "        self.pair_counts: DefaultDict[Tuple[str, str], int] = defaultdict(int) # ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡\n",
    "        self.pair_positions: DefaultDict[Tuple[str, str], List[Tuple[int, int]]] = defaultdict(list) # è®°å½•å­—èŠ‚å¯¹ä½ç½®\n",
    "        self.heap = []  # æœ€å¤§å †ï¼ˆå­˜æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼‰\n",
    "        self.heap_entries: Dict[Tuple[str, str], Any] = {} # å †æ¡ç›®å¿«é€Ÿè®¿é—®\n",
    "\n",
    "        # åˆå§‹åŒ–ç´¢å¼• ä¸€æ¬¡æ€§ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°ä½ç½®å’Œé¢‘ç‡â€”â€”å°†ä¸å¯è¡Œçš„O(NÂ²)é—®é¢˜è½¬åŒ–ä¸ºå¯å¤„ç†çš„O(N log N)\n",
    "        for seq_idx, seq in enumerate(sequences):\n",
    "            for pos in range(len(seq) - 1):\n",
    "                pair = (seq[pos], seq[pos + 1])\n",
    "                self.pair_counts[pair] += 1\n",
    "                self.pair_positions[pair].append((seq_idx, pos))\n",
    "\n",
    "        # æ„å»ºå † å°†é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆ>1æ¬¡ï¼‰åŠ å…¥æœ€å¤§å †ï¼Œè®© get_most_frequent() èƒ½ O(1) è·å–æœ€é«˜é¢‘å¯¹ã€‚\n",
    "        for pair, count in self.pair_counts.items():\n",
    "            if count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair\n",
    "                entry = [-count, pair]\n",
    "                heapq.heappush(self.heap, entry)\n",
    "                self.heap_entries[pair] = entry\n",
    "\n",
    "    def get_most_frequent(self) -> Tuple[str, str]:\n",
    "        \"\"\"å¿«é€Ÿè¿”å›å½“å‰æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆè·³è¿‡å·²è¢«åˆå¹¶çš„æ— æ•ˆæ¡ç›®ï¼‰\"\"\"\n",
    "        while self.heap:\n",
    "            neg_count, pair = self.heap[0]\n",
    "            # æ£€æŸ¥pairæ˜¯å¦ä»ç„¶æœ‰æ•ˆ\n",
    "            if pair not in self.heap_entries:\n",
    "                heapq.heappop(self.heap)\n",
    "                continue\n",
    "\n",
    "            current_count = self.pair_counts.get(pair, 0)\n",
    "\n",
    "            # æ£€æŸ¥è®¡æ•°æ˜¯å¦åŒ¹é…ä¸”å¤§äº1\n",
    "            if -neg_count == current_count and current_count > 1:\n",
    "                return pair\n",
    "            # å¦åˆ™ç§»é™¤æ— æ•ˆæ¡ç›®\n",
    "            heapq.heappop(self.heap)\n",
    "            if pair in self.heap_entries:  # ç¡®ä¿æ¡ç›®å­˜åœ¨\n",
    "                del self.heap_entries[pair]\n",
    "        return None\n",
    "\n",
    "    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:\n",
    "        \"\"\"åˆå¹¶å­—ç¬¦å¯¹å¹¶æ›´æ–°ç´¢å¼•\"\"\"\n",
    "        if pair not in self.pair_positions or not self.pair_positions[pair]:\n",
    "            return 0\n",
    "\n",
    "        # æŒ‰åºåˆ—å’Œä½ç½®åˆ†ç»„\n",
    "        positions_by_seq = defaultdict(list)\n",
    "        for seq_idx, pos in self.pair_positions[pair]:\n",
    "            positions_by_seq[seq_idx].append(pos)\n",
    "\n",
    "        merge_count = 0\n",
    "        for seq_idx, positions in positions_by_seq.items():\n",
    "            seq = self.sequences[seq_idx]\n",
    "            # æŒ‰ä½ç½®å€’åºæ’åº\n",
    "            positions.sort(reverse=True)\n",
    "            last_merged_pos = -2\n",
    "\n",
    "            for pos in positions:\n",
    "                # æ£€æŸ¥æ˜¯å¦å·²è¢«å‰é¢çš„åˆå¹¶å½±å“\n",
    "                if pos >= len(seq) - 1 or pos <= last_merged_pos:\n",
    "                    continue\n",
    "                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:\n",
    "                    continue\n",
    "\n",
    "                # æ‰§è¡Œåˆå¹¶\n",
    "                seq[pos] = new_token\n",
    "                del seq[pos + 1]\n",
    "                merge_count += 1\n",
    "                last_merged_pos = pos\n",
    "\n",
    "                # æ›´æ–°å·¦ä¾§pair\n",
    "                if pos > 0:\n",
    "                    left_pair = (seq[pos - 1], pair[0])\n",
    "                    self._update_pair_count(left_pair, -1)\n",
    "\n",
    "                    new_left_pair = (seq[pos - 1], new_token)\n",
    "                    self._update_pair_count(new_left_pair, 1)\n",
    "                    self._add_position(new_left_pair, seq_idx, pos - 1)\n",
    "\n",
    "                # æ›´æ–°å³ä¾§pair\n",
    "                if pos < len(seq) - 1:\n",
    "                    right_pair = (pair[1], seq[pos + 1])\n",
    "                    self._update_pair_count(right_pair, -1)\n",
    "\n",
    "                    new_right_pair = (new_token, seq[pos + 1])\n",
    "                    self._update_pair_count(new_right_pair, 1)\n",
    "                    self._add_position(new_right_pair, seq_idx, pos)\n",
    "\n",
    "        # æ¸…ç†å·²åˆå¹¶çš„pair\n",
    "        if pair in self.pair_counts:\n",
    "            del self.pair_counts[pair]\n",
    "        if pair in self.pair_positions:\n",
    "            del self.pair_positions[pair]\n",
    "        if pair in self.heap_entries:\n",
    "            # æ ‡è®°ä¸ºæ— æ•ˆï¼Œç¨åæ¸…ç†\n",
    "            self.heap_entries[pair] = None\n",
    "\n",
    "        return merge_count\n",
    "\n",
    "    def _update_pair_count(self, pair: Tuple[str, str], delta: int):\n",
    "        \"\"\"æ›´æ–°å­—ç¬¦å¯¹è®¡æ•°\"\"\"\n",
    "        if delta == 0:\n",
    "            return\n",
    "\n",
    "        # ç¡®ä¿pairå­˜åœ¨äºå­—å…¸ä¸­\n",
    "        if pair not in self.pair_counts:\n",
    "            self.pair_counts[pair] = 0\n",
    "\n",
    "        new_count = self.pair_counts[pair] + delta\n",
    "        self.pair_counts[pair] = new_count\n",
    "\n",
    "        # ç¡®ä¿è®¡æ•°ä¸ä¸ºè´Ÿ\n",
    "        if new_count < 0:\n",
    "            new_count = 0\n",
    "            self.pair_counts[pair] = 0\n",
    "\n",
    "        if pair in self.heap_entries and self.heap_entries[pair] is not None:\n",
    "            # æ›´æ–°å †æ¡ç›®\n",
    "            self.heap_entries[pair][0] = -new_count\n",
    "            heapq.heapify(self.heap)\n",
    "        elif new_count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair\n",
    "            # æ–°å»ºå †æ¡ç›®\n",
    "            entry = [-new_count, pair]\n",
    "            heapq.heappush(self.heap, entry)\n",
    "            self.heap_entries[pair] = entry\n",
    "\n",
    "    def _add_position(self, pair: Tuple[str, str], seq_idx: int, pos: int):\n",
    "        \"\"\"æ·»åŠ æ–°ä½ç½®åˆ°ç´¢å¼•\"\"\"\n",
    "        self.pair_positions[pair].append((seq_idx, pos))\n",
    "\n",
    "def run_train_bpe(\n",
    "    input_path: Union[str, os.PathLike],\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str] = [\"<|endoftext|>\"],\n",
    "    num_processes: int = 8,\n",
    "    sample_size: int = 22000,\n",
    "    **kwargs,\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    # å‚æ•°éªŒè¯\n",
    "    base_vocab_size = 256 + len(special_tokens)\n",
    "    if vocab_size < base_vocab_size:\n",
    "        raise ValueError(f\"vocab_sizeè‡³å°‘éœ€{base_vocab_size}\")\n",
    "\n",
    "    # 1. å­—èŠ‚åˆ°Unicodeæ˜ å°„\n",
    "    bytes_to_unicode_map = gpt2_bytes_to_unicode_local()\n",
    "    unicode_to_bytes_map = {v: bytes([k]) for k, v in bytes_to_unicode_map.items()}\n",
    "\n",
    "    # 2. åˆå§‹åŒ–è¯æ±‡è¡¨\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    next_token_id = 256\n",
    "    existing_bytes = set(vocab.values())\n",
    "\n",
    "    # 3. æ·»åŠ ç‰¹æ®Štoken\n",
    "    for st in special_tokens:\n",
    "        st_bytes = st.encode(\"utf-8\")\n",
    "        if st_bytes not in existing_bytes and len(vocab) < vocab_size:\n",
    "            vocab[next_token_id] = st_bytes\n",
    "            existing_bytes.add(st_bytes)\n",
    "            next_token_id += 1\n",
    "\n",
    "    # 4. åŠ è½½å¹¶é‡‡æ ·æ•°æ®\n",
    "    print(f\"ğŸ“– ä» {input_path} åŠ è½½å¹¶é‡‡æ · {sample_size} ä¸ªæ–‡æ¡£...\")\n",
    "    text = load_and_sample_data(input_path, sample_size, special_tokens[0])\n",
    "\n",
    "    # 5. åˆ†å‰²æ–‡æ¡£\n",
    "    escaped_tokens = [re.escape(st) for st in special_tokens]   ## è¿”å› \"<\\|endoftext\\|>\"\n",
    "    split_pattern = \"|\".join(escaped_tokens)\n",
    "    documents = [part for part in re.split(split_pattern, text) if part]\n",
    "\n",
    "    # 6. å¹¶è¡Œé¢„åˆ†è¯\n",
    "    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)\n",
    "    print(f\"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(sequences):,} ä¸ªtokenåºåˆ—\")\n",
    "\n",
    "    # 7. åˆå§‹åŒ–ç´¢å¼•ç»“æ„\n",
    "    print(\"ğŸ”§ æ„å»ºBPEç´¢å¼•...\")\n",
    "    bpe_index = BPEIndex(sequences)\n",
    "    merges = []\n",
    "    vocab_progress = len(vocab)\n",
    "    total_merges = vocab_size - vocab_progress\n",
    "\n",
    "    # 8. BPEè®­ç»ƒä¸»å¾ªç¯\n",
    "    print(f\"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: {total_merges:,}\")\n",
    "    progress_bar = tqdm(total=total_merges, desc=\"è®­ç»ƒBPE\", unit=\"åˆå¹¶\", mininterval=0.5)\n",
    "\n",
    "    while vocab_progress < vocab_size:\n",
    "        best_pair = bpe_index.get_most_frequent()\n",
    "        if best_pair is None:\n",
    "            print(\"\\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ\")\n",
    "            break\n",
    "\n",
    "        # åˆ›å»ºæ–°token\n",
    "        new_token_str = best_pair[0] + best_pair[1]\n",
    "        p1_bytes = unicode_to_bytes_map[best_pair[0]]\n",
    "        p2_bytes = unicode_to_bytes_map[best_pair[1]]\n",
    "        new_token_bytes = p1_bytes + p2_bytes\n",
    "\n",
    "        # æ‰§è¡Œåˆå¹¶\n",
    "        merge_count = bpe_index.merge_pair(best_pair, new_token_str)\n",
    "        if merge_count == 0:\n",
    "            continue\n",
    "\n",
    "        # æ›´æ–°è¯æ±‡è¡¨\n",
    "        if new_token_bytes not in existing_bytes:\n",
    "            vocab[next_token_id] = new_token_bytes\n",
    "            existing_bytes.add(new_token_bytes)\n",
    "            merges.append((p1_bytes, p2_bytes))\n",
    "            next_token_id += 1\n",
    "            vocab_progress += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # æ›´æ–°æ˜ å°„è¡¨\n",
    "        unicode_to_bytes_map[new_token_str] = new_token_bytes\n",
    "\n",
    "    progress_bar.close()\n",
    "    return vocab, merges\n",
    "\n",
    "def evaluate_tokenizer(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_text: str):\n",
    "    \"\"\"ç®€å•è¯„ä¼°åˆ†è¯å™¨æ•ˆæœ\"\"\"\n",
    "    print(\"\\nğŸ” åˆ†è¯å™¨è¯„ä¼°\")\n",
    "    sample_text = test_text[:200] + \"...\" if len(test_text) > 200 else test_text\n",
    "    print(f\"æ ·ä¾‹æ–‡æœ¬: {sample_text}\")\n",
    "\n",
    "    # ç®€å•ç»Ÿè®¡\n",
    "    unique_tokens = set(vocab.values())\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {len(vocab):,}\")\n",
    "    print(f\"å”¯ä¸€tokenæ•°: {len(unique_tokens):,}\")\n",
    "    print(f\"åˆå¹¶æ“ä½œæ•°: {len(merges):,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®å‚æ•°\n",
    "    config = {\n",
    "        \"vocab_size\": 10000,\n",
    "        \"special_tokens\": [\"<|endoftext|>\", \"<pad>\", \"<unk>\"],\n",
    "        \"num_processes\": 8,\n",
    "        \"sample_size\": 22000,  # åˆå§‹é‡‡æ ·22,000æ–‡æ¡£\n",
    "    }\n",
    "\n",
    "    # æ•°æ®é›†è·¯å¾„\n",
    "    train_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt\"\n",
    "    valid_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-valid.txt\"\n",
    "\n",
    "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not Path(train_path).exists():\n",
    "        raise FileNotFoundError(f\"è®­ç»ƒé›†æ–‡ä»¶ {train_path} ä¸å­˜åœ¨\")\n",
    "    if not Path(valid_path).exists():\n",
    "        raise FileNotFoundError(f\"éªŒè¯é›†æ–‡ä»¶ {valid_path} ä¸å­˜åœ¨\")\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒ\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_vocab, train_merges = run_train_bpe(train_path, **config)\n",
    "\n",
    "    print(f\"\\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}ç§’\")\n",
    "\n",
    "    # å°è§„æ¨¡éªŒè¯ (ä½¿ç”¨éªŒè¯é›†çš„10%)\n",
    "    print(\"\\nğŸ”¬ å°è§„æ¨¡éªŒè¯\")\n",
    "    valid_config = config.copy()\n",
    "    valid_config[\"sample_size\"] = int(2)  # éªŒè¯é›†ä½¿ç”¨500æ–‡æ¡£ (10%)\n",
    "\n",
    "    valid_vocab, valid_merges = run_train_bpe(valid_path, **valid_config)\n",
    "\n",
    "    # åˆ†æç»“æœ\n",
    "    print(\"\\nğŸ“Š è®­ç»ƒç»“æœ\")\n",
    "    print(f\"è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(train_vocab):,}\")\n",
    "    print(f\"è®­ç»ƒåˆå¹¶æ“ä½œæ•°: {len(train_merges):,}\")\n",
    "    print(f\"éªŒè¯è¯æ±‡è¡¨å¤§å°: {len(valid_vocab):,}\")\n",
    "    print(f\"éªŒè¯åˆå¹¶æ“ä½œæ•°: {len(valid_merges):,}\")\n",
    "\n",
    "    # æ¯”è¾ƒè¯æ±‡è¡¨é‡å ç‡\n",
    "    train_tokens = set(train_vocab.values())\n",
    "    valid_tokens = set(valid_vocab.values())\n",
    "    overlap = train_tokens & valid_tokens\n",
    "    print(f\"\\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: {len(overlap)/len(train_tokens):.1%}\")\n",
    "\n",
    "    # åŠ è½½éªŒè¯é›†æ ·ä¾‹è¿›è¡Œè¯„ä¼°\n",
    "    with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        valid_text = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦ç”¨äºè¯„ä¼°\n",
    "    evaluate_tokenizer(train_vocab, train_merges, valid_text)\n",
    "\n",
    "    import json  # éœ€è¦å¯¼å…¥jsonæ¨¡å—\n",
    "\n",
    "    # åœ¨mainå‡½æ•°æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨å†…å­˜åˆ†æä¹‹å‰ï¼‰\n",
    "    def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_path: str, merges_path: str):\n",
    "        \"\"\"ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶\"\"\"\n",
    "        # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)\n",
    "        vocab_str = {idx: token.decode('utf-8', errors='replace') for idx, token in vocab.items()}\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_str, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (æ–‡æœ¬æ ¼å¼)\n",
    "        with open(merges_path, 'w', encoding='utf-8') as f:\n",
    "            for merge in merges:\n",
    "                part1 = merge[0].decode('utf-8', errors='replace')\n",
    "                part2 = merge[1].decode('utf-8', errors='replace')\n",
    "                f.write(f\"{part1} {part2}\\n\")\n",
    "\n",
    "    # åœ¨mainå‡½æ•°ä¸­è°ƒç”¨ä¿å­˜åŠŸèƒ½ï¼ˆåœ¨è®­ç»ƒå®Œæˆåï¼‰\n",
    "    output_dir = \"/home/mw/project\"  # ä¿®æ”¹ä¸ºæ‚¨çš„è¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vocab_path = os.path.join(output_dir, \"gpt2_vocab.json\")\n",
    "    merges_path = os.path.join(output_dir, \"gpt2_merges.txt\")\n",
    "\n",
    "    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)\n",
    "    print(f\"âœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {vocab_path}\")\n",
    "    print(f\"âœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: {merges_path}\")\n",
    "\n",
    "    # å†…å­˜åˆ†æ\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    mem_usage = process.memory_info().rss / (1024 ** 3)  # GB\n",
    "    print(f\"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB\")"
   ],
   "id": "5d1bec6ffdf21f9c",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "è®­ç»ƒé›†æ–‡ä»¶ /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt ä¸å­˜åœ¨",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 339\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;66;03m# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\u001B[39;00m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Path(train_path)\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m--> 339\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mè®­ç»ƒé›†æ–‡ä»¶ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ä¸å­˜åœ¨\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Path(valid_path)\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124méªŒè¯é›†æ–‡ä»¶ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalid_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ä¸å­˜åœ¨\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: è®­ç»ƒé›†æ–‡ä»¶ /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt ä¸å­˜åœ¨"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-01T05:02:44.159455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config, get_cosine_schedule_with_warmup\n",
    "\n",
    "# 1) åŠ è½½åˆ†è¯å™¨ï¼ˆç”¨ä½ è®­ç»ƒå¥½çš„BPEï¼›æ¼”ç¤ºç”¨ç°æˆçš„ï¼‰\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")  # æ¢æˆä½ çš„BPE tokenizer ç›®å½•\n",
    "tok.pad_token = tok.eos_token  # ä»…ä¸ºdataloaderå¯¹é½\n",
    "\n",
    "# 2) æ•°æ®ï¼šæŠŠé¢„ç¼–ç å¥½çš„é•¿tokensæ–‡ä»¶åˆ‡ç‰‡ï¼ˆæ¼”ç¤ºï¼šç°åœºç¼–ç ï¼‰\n",
    "texts = [\"ä»Šå¤©å¤©æ°”ä¸é”™ã€‚\\n\", \"I love RL for trading.\\n\"] * 10000\n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, ctx_len=1024):\n",
    "        self.ctx_len = ctx_len\n",
    "        ids = []\n",
    "        for t in texts:\n",
    "            ids.extend(tokenizer.encode(t))\n",
    "        self.tokens = torch.tensor(ids, dtype=torch.long)\n",
    "        # å¯æ›¿æ¢ä¸º mmap çš„ np.memmap('tokens.int32', 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.ctx_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = i * self.ctx_len\n",
    "        x = self.tokens[s:s+self.ctx_len]\n",
    "        y = self.tokens[s+1:s+self.ctx_len+1]\n",
    "        return x, y\n",
    "\n",
    "ctx_len = 512\n",
    "ds = CausalDataset(texts, tok, ctx_len)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# 3) æ¨¡å‹\n",
    "cfg = GPT2Config(\n",
    "    vocab_size=len(tok),\n",
    "    n_positions=ctx_len,\n",
    "    n_ctx=ctx_len,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(cfg).train().cuda()\n",
    "\n",
    "# 4) ä¼˜åŒ–å™¨ & è°ƒåº¦\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "num_steps = 2000\n",
    "warmup = 100\n",
    "sched = get_cosine_schedule_with_warmup(optim, warmup, num_steps)\n",
    "\n",
    "# 5) è®­ç»ƒå¾ªç¯ï¼ˆAMP + å› æœæŸå¤±å·²ç”±æ¨¡å‹å†…éƒ¨å¤„ç†ï¼‰\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, (x, y) in enumerate(loader):\n",
    "    if step >= num_steps: break\n",
    "    x = x.cuda(non_blocking=True)\n",
    "    y = y.cuda(non_blocking=True)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out.loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    sched.step()\n",
    "    if step % 50 == 0:\n",
    "        ppl = math.exp(loss.item())\n",
    "        print(f\"step {step} | loss {loss.item():.3f} | ppl {ppl:.2f}\")\n"
   ],
   "id": "7cb78e475c809405",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train_lora.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = \"decapoda-research/llama-7b-hf\"  # æ¢æˆä½ åŸºåº§\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½æ•°æ®ï¼ˆå‡è®¾ jsonlï¼Œå­—æ®µ: instruction,input,outputï¼‰\n",
    "ds = load_dataset(\"json\", data_files=\"sft_data.jsonl\")[\"train\"]\n",
    "\n",
    "# æ‹¼æ¥ prompt\n",
    "def make_prompt(example):\n",
    "    instruction = example.get(\"instruction\",\"\")\n",
    "    inp = example.get(\"input\",\"\")\n",
    "    output = example.get(\"output\",\"\")\n",
    "    prompt = f\"### æŒ‡ä»¤:\\n{instruction}\\n\\n### è¾“å…¥:\\n{inp}\\n\\n### å›ç­”:\\n\"\n",
    "    full = prompt + output\n",
    "    return {\"input_ids\": tokenizer(prompt, truncation=True, max_length=1024).input_ids,\n",
    "            \"labels\": tokenizer(full, truncation=True, max_length=1024).input_ids}\n",
    "\n",
    "ds = ds.map(make_prompt, remove_columns=ds.column_names)\n",
    "\n",
    "# k-bit å‡†å¤‡ (å¦‚æœç”¨ bitsandbytes 4-bit é‡åŒ–)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb.nn.modules.Linear4bit(linear_act=False))\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA é…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # ä¸åŒæ¨¡å‹æ¨¡å—åä¸åŒï¼ŒLlama-like å¸¸ç”¨ q_proj/v_proj\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# training args via HF Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_sft_ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "    # ç®€å• pad\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, return_tensors=\"pt\", padding=True).input_ids\n",
    "    labels = tokenizer.pad({\"input_ids\": labels}, return_tensors=\"pt\", padding=True).input_ids\n",
    "    # è®¾ç½® pad token ä¸º -100 é¿å…è®¡ç®— loss\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=ds.select(range(1000)),\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"lora_sft_final\")\n"
   ],
   "id": "675bdd8a4b42aeca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
