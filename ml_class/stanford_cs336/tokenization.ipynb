{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-01T04:21:58.659248Z",
     "start_time": "2025-09-01T04:21:58.644965Z"
    }
   },
   "source": [
    "print(chr(0))\n",
    "print(ord('牛'))\n",
    "\n",
    "print(chr(29275))\n",
    "print(chr(0x1F60E))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "29275\n",
      "牛\n",
      "😎\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:21:12.211169Z",
     "start_time": "2025-09-01T04:21:12.192568Z"
    }
   },
   "cell_type": "code",
   "source": "print(chr(0))",
   "id": "d3fa03a696547309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:22:12.807024Z",
     "start_time": "2025-09-01T04:22:12.796894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "l = \"this is a test\" + chr(0) + \"string\"\n",
    "l"
   ],
   "id": "9b60edb1185cca12",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:22:21.132204Z",
     "start_time": "2025-09-01T04:22:21.119669Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"this is a test\" + chr(0) + \"string\")\n",
   "id": "97f70eb1e12e485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:25:18.224710Z",
     "start_time": "2025-09-01T04:25:18.213212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 动手跑！\n",
    "test_string = \"hello! 天海!\"  # 试试你的名字？\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)  # 输出: b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\n",
    "print(type(utf8_encoded))\n",
    "\n",
    "# 拆解字节值（0-255整数）\n",
    "list(utf8_encoded)\n",
    "\n",
    "# 验证可逆性\n",
    "print(len(test_string), len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))  # 完美还原: 'hello! 天海!'"
   ],
   "id": "28602203da6e443",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\n",
      "<class 'bytes'>\n",
      "10 14\n",
      "hello! 天海!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:26:03.502044Z",
     "start_time": "2025-09-01T04:26:03.490537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):  # 注意参数名修正！\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "# decode_utf8_bytes_to_str_wrong(\"café\".encode(\"utf-8\")) 错误"
   ],
   "id": "95077a4c86276b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:29:52.617076Z",
     "start_time": "2025-09-01T04:29:52.603056Z"
    }
   },
   "cell_type": "code",
   "source": "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])\n",
   "id": "d0fcf253226d9688",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:30:06.961727Z",
     "start_time": "2025-09-01T04:30:06.949196Z"
    }
   },
   "cell_type": "code",
   "source": "max([(\"es\"),(\"st\")])\n",
   "id": "734f8138c2535f30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:45:43.046412Z",
     "start_time": "2025-09-01T04:45:42.355059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import heapq\n",
    "import regex\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, DefaultDict, Any, Union\n",
    "import mmap\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# GPT-2预分词模式\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "def load_and_sample_data(file_path: str, sample_size: int = 22000, special_token: str = \"<|endoftext|>\") -> str:\n",
    "    \"\"\"内存映射方式加载并采样文档\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r+\", encoding='utf-8', errors='ignore') as f:\n",
    "            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "                documents = []\n",
    "                start = 0\n",
    "                while start < len(mm):\n",
    "                    end = mm.find(special_token.encode('utf-8'), start)\n",
    "                    if end == -1:\n",
    "                        doc = mm[start:].decode('utf-8', errors='replace').strip()\n",
    "                        if doc:\n",
    "                            documents.append(doc)\n",
    "                        break\n",
    "\n",
    "                    doc = mm[start:end].decode('utf-8', errors='replace').strip()\n",
    "                    if doc:\n",
    "                        documents.append(doc)\n",
    "                    start = end + len(special_token)\n",
    "\n",
    "                if len(documents) > sample_size:\n",
    "                    documents = random.sample(documents, sample_size)\n",
    "\n",
    "                return special_token.join(documents)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"加载数据集失败: {e}\")\n",
    "\n",
    "def gpt2_bytes_to_unicode_local() -> Dict[int, str]:\n",
    "    \"\"\"字节到Unicode映射\"\"\"\n",
    "    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(256):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(256 + n)\n",
    "            n += 1\n",
    "    return {b: chr(c) for b, c in zip(bs, cs)}\n",
    "\n",
    "def pre_tokenize_document(doc: str, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n",
    "    \"\"\"预分词处理单个文档\"\"\"\n",
    "    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)\n",
    "    sequences = []\n",
    "    for token in tokens:\n",
    "        token_unicode = ''.join(bytes_to_unicode_map[b] for b in token.encode('utf-8'))\n",
    "        sequences.append(list(token_unicode))\n",
    "    return sequences\n",
    "\n",
    "def parallel_pre_tokenize(documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n",
    "    \"\"\"并行预分词优化\"\"\"\n",
    "    if num_processes <= 1:\n",
    "        return [seq for doc in documents for seq in pre_tokenize_document(doc, bytes_to_unicode_map)]\n",
    "\n",
    "    with multiprocessing.Pool(\n",
    "        num_processes,\n",
    "        initializer=init_worker,\n",
    "        initargs=(bytes_to_unicode_map,)\n",
    "    ) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(pre_tokenize_worker, documents, chunksize=50),\n",
    "            total=len(documents),\n",
    "            desc=\"预分词\",\n",
    "            mininterval=1\n",
    "        ))\n",
    "    return [seq for doc_sequences in results for seq in doc_sequences]\n",
    "\n",
    "# 全局变量用于多进程\n",
    "global_worker_byte_map = None\n",
    "def init_worker(byte_map: Dict[int, str]):\n",
    "    global global_worker_byte_map\n",
    "    global_worker_byte_map = byte_map\n",
    "\n",
    "def pre_tokenize_worker(doc: str) -> List[List[str]]:\n",
    "    return pre_tokenize_document(doc, global_worker_byte_map)\n",
    "\n",
    "class BPEIndex:\n",
    "    \"\"\"高效索引结构用于BPE合并\"\"\"\n",
    "    def __init__(self, sequences: List[List[str]]):\n",
    "        self.sequences = sequences # 存储所有文本序列\n",
    "        self.pair_counts: DefaultDict[Tuple[str, str], int] = defaultdict(int) # 统计字节对频率\n",
    "        self.pair_positions: DefaultDict[Tuple[str, str], List[Tuple[int, int]]] = defaultdict(list) # 记录字节对位置\n",
    "        self.heap = []  # 最大堆（存最高频字节对）\n",
    "        self.heap_entries: Dict[Tuple[str, str], Any] = {} # 堆条目快速访问\n",
    "\n",
    "        # 初始化索引 一次性统计所有相邻字节对的出现位置和频率——将不可行的O(N²)问题转化为可处理的O(N log N)\n",
    "        for seq_idx, seq in enumerate(sequences):\n",
    "            for pos in range(len(seq) - 1):\n",
    "                pair = (seq[pos], seq[pos + 1])\n",
    "                self.pair_counts[pair] += 1\n",
    "                self.pair_positions[pair].append((seq_idx, pos))\n",
    "\n",
    "        # 构建堆 将高频字节对（>1次）加入最大堆，让 get_most_frequent() 能 O(1) 获取最高频对。\n",
    "        for pair, count in self.pair_counts.items():\n",
    "            if count > 1:  # 只添加计数大于1的pair\n",
    "                entry = [-count, pair]\n",
    "                heapq.heappush(self.heap, entry)\n",
    "                self.heap_entries[pair] = entry\n",
    "\n",
    "    def get_most_frequent(self) -> Tuple[str, str]:\n",
    "        \"\"\"快速返回当前最高频字节对（跳过已被合并的无效条目）\"\"\"\n",
    "        while self.heap:\n",
    "            neg_count, pair = self.heap[0]\n",
    "            # 检查pair是否仍然有效\n",
    "            if pair not in self.heap_entries:\n",
    "                heapq.heappop(self.heap)\n",
    "                continue\n",
    "\n",
    "            current_count = self.pair_counts.get(pair, 0)\n",
    "\n",
    "            # 检查计数是否匹配且大于1\n",
    "            if -neg_count == current_count and current_count > 1:\n",
    "                return pair\n",
    "            # 否则移除无效条目\n",
    "            heapq.heappop(self.heap)\n",
    "            if pair in self.heap_entries:  # 确保条目存在\n",
    "                del self.heap_entries[pair]\n",
    "        return None\n",
    "\n",
    "    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:\n",
    "        \"\"\"合并字符对并更新索引\"\"\"\n",
    "        if pair not in self.pair_positions or not self.pair_positions[pair]:\n",
    "            return 0\n",
    "\n",
    "        # 按序列和位置分组\n",
    "        positions_by_seq = defaultdict(list)\n",
    "        for seq_idx, pos in self.pair_positions[pair]:\n",
    "            positions_by_seq[seq_idx].append(pos)\n",
    "\n",
    "        merge_count = 0\n",
    "        for seq_idx, positions in positions_by_seq.items():\n",
    "            seq = self.sequences[seq_idx]\n",
    "            # 按位置倒序排序\n",
    "            positions.sort(reverse=True)\n",
    "            last_merged_pos = -2\n",
    "\n",
    "            for pos in positions:\n",
    "                # 检查是否已被前面的合并影响\n",
    "                if pos >= len(seq) - 1 or pos <= last_merged_pos:\n",
    "                    continue\n",
    "                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:\n",
    "                    continue\n",
    "\n",
    "                # 执行合并\n",
    "                seq[pos] = new_token\n",
    "                del seq[pos + 1]\n",
    "                merge_count += 1\n",
    "                last_merged_pos = pos\n",
    "\n",
    "                # 更新左侧pair\n",
    "                if pos > 0:\n",
    "                    left_pair = (seq[pos - 1], pair[0])\n",
    "                    self._update_pair_count(left_pair, -1)\n",
    "\n",
    "                    new_left_pair = (seq[pos - 1], new_token)\n",
    "                    self._update_pair_count(new_left_pair, 1)\n",
    "                    self._add_position(new_left_pair, seq_idx, pos - 1)\n",
    "\n",
    "                # 更新右侧pair\n",
    "                if pos < len(seq) - 1:\n",
    "                    right_pair = (pair[1], seq[pos + 1])\n",
    "                    self._update_pair_count(right_pair, -1)\n",
    "\n",
    "                    new_right_pair = (new_token, seq[pos + 1])\n",
    "                    self._update_pair_count(new_right_pair, 1)\n",
    "                    self._add_position(new_right_pair, seq_idx, pos)\n",
    "\n",
    "        # 清理已合并的pair\n",
    "        if pair in self.pair_counts:\n",
    "            del self.pair_counts[pair]\n",
    "        if pair in self.pair_positions:\n",
    "            del self.pair_positions[pair]\n",
    "        if pair in self.heap_entries:\n",
    "            # 标记为无效，稍后清理\n",
    "            self.heap_entries[pair] = None\n",
    "\n",
    "        return merge_count\n",
    "\n",
    "    def _update_pair_count(self, pair: Tuple[str, str], delta: int):\n",
    "        \"\"\"更新字符对计数\"\"\"\n",
    "        if delta == 0:\n",
    "            return\n",
    "\n",
    "        # 确保pair存在于字典中\n",
    "        if pair not in self.pair_counts:\n",
    "            self.pair_counts[pair] = 0\n",
    "\n",
    "        new_count = self.pair_counts[pair] + delta\n",
    "        self.pair_counts[pair] = new_count\n",
    "\n",
    "        # 确保计数不为负\n",
    "        if new_count < 0:\n",
    "            new_count = 0\n",
    "            self.pair_counts[pair] = 0\n",
    "\n",
    "        if pair in self.heap_entries and self.heap_entries[pair] is not None:\n",
    "            # 更新堆条目\n",
    "            self.heap_entries[pair][0] = -new_count\n",
    "            heapq.heapify(self.heap)\n",
    "        elif new_count > 1:  # 只添加计数大于1的pair\n",
    "            # 新建堆条目\n",
    "            entry = [-new_count, pair]\n",
    "            heapq.heappush(self.heap, entry)\n",
    "            self.heap_entries[pair] = entry\n",
    "\n",
    "    def _add_position(self, pair: Tuple[str, str], seq_idx: int, pos: int):\n",
    "        \"\"\"添加新位置到索引\"\"\"\n",
    "        self.pair_positions[pair].append((seq_idx, pos))\n",
    "\n",
    "def run_train_bpe(\n",
    "    input_path: Union[str, os.PathLike],\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str] = [\"<|endoftext|>\"],\n",
    "    num_processes: int = 8,\n",
    "    sample_size: int = 22000,\n",
    "    **kwargs,\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    # 参数验证\n",
    "    base_vocab_size = 256 + len(special_tokens)\n",
    "    if vocab_size < base_vocab_size:\n",
    "        raise ValueError(f\"vocab_size至少需{base_vocab_size}\")\n",
    "\n",
    "    # 1. 字节到Unicode映射\n",
    "    bytes_to_unicode_map = gpt2_bytes_to_unicode_local()\n",
    "    unicode_to_bytes_map = {v: bytes([k]) for k, v in bytes_to_unicode_map.items()}\n",
    "\n",
    "    # 2. 初始化词汇表\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    next_token_id = 256\n",
    "    existing_bytes = set(vocab.values())\n",
    "\n",
    "    # 3. 添加特殊token\n",
    "    for st in special_tokens:\n",
    "        st_bytes = st.encode(\"utf-8\")\n",
    "        if st_bytes not in existing_bytes and len(vocab) < vocab_size:\n",
    "            vocab[next_token_id] = st_bytes\n",
    "            existing_bytes.add(st_bytes)\n",
    "            next_token_id += 1\n",
    "\n",
    "    # 4. 加载并采样数据\n",
    "    print(f\"📖 从 {input_path} 加载并采样 {sample_size} 个文档...\")\n",
    "    text = load_and_sample_data(input_path, sample_size, special_tokens[0])\n",
    "\n",
    "    # 5. 分割文档\n",
    "    escaped_tokens = [re.escape(st) for st in special_tokens]   ## 返回 \"<\\|endoftext\\|>\"\n",
    "    split_pattern = \"|\".join(escaped_tokens)\n",
    "    documents = [part for part in re.split(split_pattern, text) if part]\n",
    "\n",
    "    # 6. 并行预分词\n",
    "    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)\n",
    "    print(f\"✅ 预分词完成，得到 {len(sequences):,} 个token序列\")\n",
    "\n",
    "    # 7. 初始化索引结构\n",
    "    print(\"🔧 构建BPE索引...\")\n",
    "    bpe_index = BPEIndex(sequences)\n",
    "    merges = []\n",
    "    vocab_progress = len(vocab)\n",
    "    total_merges = vocab_size - vocab_progress\n",
    "\n",
    "    # 8. BPE训练主循环\n",
    "    print(f\"🔄 开始BPE训练，目标合并数: {total_merges:,}\")\n",
    "    progress_bar = tqdm(total=total_merges, desc=\"训练BPE\", unit=\"合并\", mininterval=0.5)\n",
    "\n",
    "    while vocab_progress < vocab_size:\n",
    "        best_pair = bpe_index.get_most_frequent()\n",
    "        if best_pair is None:\n",
    "            print(\"\\n⚠️ 没有更多有效的字符对可供合并，提前结束训练\")\n",
    "            break\n",
    "\n",
    "        # 创建新token\n",
    "        new_token_str = best_pair[0] + best_pair[1]\n",
    "        p1_bytes = unicode_to_bytes_map[best_pair[0]]\n",
    "        p2_bytes = unicode_to_bytes_map[best_pair[1]]\n",
    "        new_token_bytes = p1_bytes + p2_bytes\n",
    "\n",
    "        # 执行合并\n",
    "        merge_count = bpe_index.merge_pair(best_pair, new_token_str)\n",
    "        if merge_count == 0:\n",
    "            continue\n",
    "\n",
    "        # 更新词汇表\n",
    "        if new_token_bytes not in existing_bytes:\n",
    "            vocab[next_token_id] = new_token_bytes\n",
    "            existing_bytes.add(new_token_bytes)\n",
    "            merges.append((p1_bytes, p2_bytes))\n",
    "            next_token_id += 1\n",
    "            vocab_progress += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # 更新映射表\n",
    "        unicode_to_bytes_map[new_token_str] = new_token_bytes\n",
    "\n",
    "    progress_bar.close()\n",
    "    return vocab, merges\n",
    "\n",
    "def evaluate_tokenizer(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_text: str):\n",
    "    \"\"\"简单评估分词器效果\"\"\"\n",
    "    print(\"\\n🔍 分词器评估\")\n",
    "    sample_text = test_text[:200] + \"...\" if len(test_text) > 200 else test_text\n",
    "    print(f\"样例文本: {sample_text}\")\n",
    "\n",
    "    # 简单统计\n",
    "    unique_tokens = set(vocab.values())\n",
    "    print(f\"词汇表大小: {len(vocab):,}\")\n",
    "    print(f\"唯一token数: {len(unique_tokens):,}\")\n",
    "    print(f\"合并操作数: {len(merges):,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    config = {\n",
    "        \"vocab_size\": 10000,\n",
    "        \"special_tokens\": [\"<|endoftext|>\", \"<pad>\", \"<unk>\"],\n",
    "        \"num_processes\": 8,\n",
    "        \"sample_size\": 22000,  # 初始采样22,000文档\n",
    "    }\n",
    "\n",
    "    # 数据集路径\n",
    "    train_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt\"\n",
    "    valid_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-valid.txt\"\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not Path(train_path).exists():\n",
    "        raise FileNotFoundError(f\"训练集文件 {train_path} 不存在\")\n",
    "    if not Path(valid_path).exists():\n",
    "        raise FileNotFoundError(f\"验证集文件 {valid_path} 不存在\")\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"🚀 开始训练\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_vocab, train_merges = run_train_bpe(train_path, **config)\n",
    "\n",
    "    print(f\"\\n✅ 训练完成! 耗时: {time.time() - start_time:.2f}秒\")\n",
    "\n",
    "    # 小规模验证 (使用验证集的10%)\n",
    "    print(\"\\n🔬 小规模验证\")\n",
    "    valid_config = config.copy()\n",
    "    valid_config[\"sample_size\"] = int(2)  # 验证集使用500文档 (10%)\n",
    "\n",
    "    valid_vocab, valid_merges = run_train_bpe(valid_path, **valid_config)\n",
    "\n",
    "    # 分析结果\n",
    "    print(\"\\n📊 训练结果\")\n",
    "    print(f\"训练词汇表大小: {len(train_vocab):,}\")\n",
    "    print(f\"训练合并操作数: {len(train_merges):,}\")\n",
    "    print(f\"验证词汇表大小: {len(valid_vocab):,}\")\n",
    "    print(f\"验证合并操作数: {len(valid_merges):,}\")\n",
    "\n",
    "    # 比较词汇表重叠率\n",
    "    train_tokens = set(train_vocab.values())\n",
    "    valid_tokens = set(valid_vocab.values())\n",
    "    overlap = train_tokens & valid_tokens\n",
    "    print(f\"\\n📈 词汇表重叠率: {len(overlap)/len(train_tokens):.1%}\")\n",
    "\n",
    "    # 加载验证集样例进行评估\n",
    "    with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        valid_text = f.read(1000)  # 读取前1000字符用于评估\n",
    "    evaluate_tokenizer(train_vocab, train_merges, valid_text)\n",
    "\n",
    "    import json  # 需要导入json模块\n",
    "\n",
    "    # 在main函数末尾添加以下代码（在内存分析之前）\n",
    "    def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_path: str, merges_path: str):\n",
    "        \"\"\"保存词汇表和合并列表到文件\"\"\"\n",
    "        # 1. 保存词汇表 (JSON格式)\n",
    "        vocab_str = {idx: token.decode('utf-8', errors='replace') for idx, token in vocab.items()}\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_str, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 2. 保存合并列表 (文本格式)\n",
    "        with open(merges_path, 'w', encoding='utf-8') as f:\n",
    "            for merge in merges:\n",
    "                part1 = merge[0].decode('utf-8', errors='replace')\n",
    "                part2 = merge[1].decode('utf-8', errors='replace')\n",
    "                f.write(f\"{part1} {part2}\\n\")\n",
    "\n",
    "    # 在main函数中调用保存功能（在训练完成后）\n",
    "    output_dir = \"/home/mw/project\"  # 修改为您的输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vocab_path = os.path.join(output_dir, \"gpt2_vocab.json\")\n",
    "    merges_path = os.path.join(output_dir, \"gpt2_merges.txt\")\n",
    "\n",
    "    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)\n",
    "    print(f\"✅ 词汇表已保存至: {vocab_path}\")\n",
    "    print(f\"✅ 合并列表已保存至: {merges_path}\")\n",
    "\n",
    "    # 内存分析\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    mem_usage = process.memory_info().rss / (1024 ** 3)  # GB\n",
    "    print(f\"💾 峰值内存使用: {mem_usage:.2f} GB\")"
   ],
   "id": "5d1bec6ffdf21f9c",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "训练集文件 /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt 不存在",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 339\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;66;03m# 检查文件是否存在\u001B[39;00m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Path(train_path)\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m--> 339\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m训练集文件 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 不存在\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Path(valid_path)\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m验证集文件 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalid_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 不存在\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: 训练集文件 /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt 不存在"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-01T05:02:44.159455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config, get_cosine_schedule_with_warmup\n",
    "\n",
    "# 1) 加载分词器（用你训练好的BPE；演示用现成的）\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")  # 换成你的BPE tokenizer 目录\n",
    "tok.pad_token = tok.eos_token  # 仅为dataloader对齐\n",
    "\n",
    "# 2) 数据：把预编码好的长tokens文件切片（演示：现场编码）\n",
    "texts = [\"今天天气不错。\\n\", \"I love RL for trading.\\n\"] * 10000\n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, ctx_len=1024):\n",
    "        self.ctx_len = ctx_len\n",
    "        ids = []\n",
    "        for t in texts:\n",
    "            ids.extend(tokenizer.encode(t))\n",
    "        self.tokens = torch.tensor(ids, dtype=torch.long)\n",
    "        # 可替换为 mmap 的 np.memmap('tokens.int32', 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.ctx_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = i * self.ctx_len\n",
    "        x = self.tokens[s:s+self.ctx_len]\n",
    "        y = self.tokens[s+1:s+self.ctx_len+1]\n",
    "        return x, y\n",
    "\n",
    "ctx_len = 512\n",
    "ds = CausalDataset(texts, tok, ctx_len)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# 3) 模型\n",
    "cfg = GPT2Config(\n",
    "    vocab_size=len(tok),\n",
    "    n_positions=ctx_len,\n",
    "    n_ctx=ctx_len,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(cfg).train().cuda()\n",
    "\n",
    "# 4) 优化器 & 调度\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "num_steps = 2000\n",
    "warmup = 100\n",
    "sched = get_cosine_schedule_with_warmup(optim, warmup, num_steps)\n",
    "\n",
    "# 5) 训练循环（AMP + 因果损失已由模型内部处理）\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, (x, y) in enumerate(loader):\n",
    "    if step >= num_steps: break\n",
    "    x = x.cuda(non_blocking=True)\n",
    "    y = y.cuda(non_blocking=True)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out.loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    sched.step()\n",
    "    if step % 50 == 0:\n",
    "        ppl = math.exp(loss.item())\n",
    "        print(f\"step {step} | loss {loss.item():.3f} | ppl {ppl:.2f}\")\n"
   ],
   "id": "7cb78e475c809405",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train_lora.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = \"decapoda-research/llama-7b-hf\"  # 换成你基座\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载数据（假设 jsonl，字段: instruction,input,output）\n",
    "ds = load_dataset(\"json\", data_files=\"sft_data.jsonl\")[\"train\"]\n",
    "\n",
    "# 拼接 prompt\n",
    "def make_prompt(example):\n",
    "    instruction = example.get(\"instruction\",\"\")\n",
    "    inp = example.get(\"input\",\"\")\n",
    "    output = example.get(\"output\",\"\")\n",
    "    prompt = f\"### 指令:\\n{instruction}\\n\\n### 输入:\\n{inp}\\n\\n### 回答:\\n\"\n",
    "    full = prompt + output\n",
    "    return {\"input_ids\": tokenizer(prompt, truncation=True, max_length=1024).input_ids,\n",
    "            \"labels\": tokenizer(full, truncation=True, max_length=1024).input_ids}\n",
    "\n",
    "ds = ds.map(make_prompt, remove_columns=ds.column_names)\n",
    "\n",
    "# k-bit 准备 (如果用 bitsandbytes 4-bit 量化)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb.nn.modules.Linear4bit(linear_act=False))\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # 不同模型模块名不同，Llama-like 常用 q_proj/v_proj\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# training args via HF Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_sft_ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "    # 简单 pad\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, return_tensors=\"pt\", padding=True).input_ids\n",
    "    labels = tokenizer.pad({\"input_ids\": labels}, return_tensors=\"pt\", padding=True).input_ids\n",
    "    # 设置 pad token 为 -100 避免计算 loss\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=ds.select(range(1000)),\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"lora_sft_final\")\n"
   ],
   "id": "675bdd8a4b42aeca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
